#!/bin/bash
# –ó–∞–ø—É—Å–∫ –§–ê–ó–ò 1 - Language Pretraining
# –ù–∞–≤—á–∞–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ—ó Transformer –º–æ–¥–µ–ª—ñ –∑ –Ω—É–ª—è –Ω–∞ plain text

set -euo pipefail  # –ó—É–ø–∏–Ω–∏—Ç–∏ –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ, –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö, –ø–æ–º–∏–ª–∫–∞—Ö —É pipe

echo "üöÄ –§–ê–ó–ê 1 - Language Pretraining"
echo "=================================="

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
if [[ "$VIRTUAL_ENV" == "" ]]; then
    echo "‚ö†Ô∏è  –ê–∫—Ç–∏–≤—É–π—Ç–µ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ:"
    echo "   source venv-linux/bin/activate"
    exit 1
fi

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å plain text corpus
if [[ ! -f "datasets/pretrain_text.txt" ]]; then
    echo "‚ùå Plain text corpus –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!"
    echo "   –ó–∞–ø—É—Å—Ç—ñ—Ç—å: python scripts/prepare_phase1_dataset.py"
    exit 1
fi

# –ü–æ–∫–∞–∑–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ dataset
DATASET_FILE="datasets/pretrain_text.txt"
FILE_SIZE=$(stat -c%s "$DATASET_FILE")

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Ä–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É
if [ "$FILE_SIZE" -lt 1000000 ]; then
    echo "‚ùå Dataset —Ñ–∞–π–ª –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–∏–π –∞–±–æ –ø–æ—Ä–æ–∂–Ω—ñ–π"
    echo "   –†–æ–∑–º—ñ—Ä: $FILE_SIZE –±–∞–π—Ç (–ø–æ—Ç—Ä—ñ–±–Ω–æ –º—ñ–Ω—ñ–º—É–º 1MB)"
    exit 1
fi

# –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —á–∏ —î –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç
if ! head -n 5 "$DATASET_FILE" | grep -qi "[a-z]"; then
    echo "‚ùå Dataset –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç—É"
    echo "   –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Ñ–∞–π–ª: $DATASET_FILE"
    exit 1
fi

# –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ —Å–ª–æ–≤–∞ —Ç–∞ —Å–∏–º–≤–æ–ª–∏ (–¥–ª—è plain text —Ü–µ –±—ñ–ª—å—à —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
WORDS=$(wc -w < "$DATASET_FILE")
CHARS=$(wc -c < "$DATASET_FILE")

echo "üìä –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ dataset:"
echo "   –§–∞–π–ª: $DATASET_FILE"
echo "   –†–æ–∑–º—ñ—Ä: $((FILE_SIZE / 1024 / 1024))MB ($FILE_SIZE –±–∞–π—Ç)"
echo "   –°–∏–º–≤–æ–ª—ñ–≤: $CHARS"
echo "   –°–ª—ñ–≤: $WORDS"
echo "   –§–æ—Ä–º–∞—Ç: Plain text (–æ–¥–∏–Ω –¥–æ–≤–≥–∏–π —Ä—è–¥–æ–∫ –¥–ª—è language modeling)"

# –ü–æ–∫–∞–∑–∞—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
echo ""
echo "üìã –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –§–ê–ó–ò 1:"
echo "   Config: config/phase1_pretraining.yaml"
echo "   Model: Small Transformer (~15-25M params)"
echo "   Objective: Causal Language Modeling"
echo "   Initialization: Random (–ë–ï–ó pretrained weights)"
echo "   Max epochs: 3"

# –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è
echo ""
read -p "ü§î –ü–æ—á–∞—Ç–∏ –§–ê–ó–£ 1? (y/N): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "‚ùå –°–∫–∞—Å–æ–≤–∞–Ω–æ"
    exit 1
fi

# –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –ø–∞–ø–∫–∏
mkdir -p checkpoints/phase1
mkdir -p logs/phase1

echo ""
echo "üéØ –ó–∞–ø—É—Å–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –§–ê–ó–ò 1..."
echo "   –¶–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ –∫—ñ–ª—å–∫–∞ –≥–æ–¥–∏–Ω –Ω–∞ CPU!"
echo "   –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥: tail -f logs/phase1/phase1_pretraining.log"

# –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
# –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
python scripts/train_phase1_pretraining.py \
    --config config/phase1_pretraining.yaml || {
    ERR_CODE=$?
    echo ""
    echo "‚ö†Ô∏è  Training interrupted/failed with code $ERR_CODE"
    exit $ERR_CODE
}

echo ""
echo "‚úÖ –§–ê–ó–ê 1 –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
echo "   –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: checkpoints/phase1/best_model.pt"
echo ""
echo "üîÑ –ù–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫: –§–ê–ó–ê 2 - Instruction Tuning"
echo "   –ó–∞–ø—É—Å—Ç—ñ—Ç—å: ./start_phase2_instruction_tuning.sh"
