"""
FastAPI Inference Server –¥–ª—è TRM –º–æ–¥–µ–ª—ñ
OpenAI-compatible REST API
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
import torch
from pathlib import Path
import json

app = FastAPI(title="TRM Inference Server")

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª—ñ —Ç–∞ tokenizer
model = None
tokenizer = None
device = "cpu"


class CompletionRequest(BaseModel):
    """Request –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É"""
    prompt: str
    max_tokens: int = 128
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None


class CompletionResponse(BaseModel):
    """Response –∑ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º"""
    text: str
    tokens_generated: int


@app.on_event("startup")
def load():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å —Ç–∞ tokenizer –Ω–∞ startup"""
    global model, tokenizer, device
    
    # –®—É–∫–∞—Ç–∏ –º–æ–¥–µ–ª—å
    model_path = Path("models/trained")
    if not model_path.exists():
        model_path = Path("checkpoints")
    
    # –ó–Ω–∞–π—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—é –º–æ–¥–µ–ª—å
    model_files = list(model_path.glob("*.pt"))
    if not model_files:
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, inference –±—É–¥–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π")
        return
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—é –º–æ–¥–µ–ª—å
    latest_model = max(model_files, key=lambda p: p.stat().st_mtime)
    print(f"üì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {latest_model}")
    
    try:
        checkpoint = torch.load(latest_model, map_location=device)
        if 'model_state_dict' in checkpoint:
            # –ü–æ—Ç—Ä—ñ–±–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            # –¢–∏–º—á–∞—Å–æ–≤–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–µ—Ñ–æ–ª—Ç–∏
            from train.model_factory import create_model
            model = create_model(
                dim=256,
                vocab_size=50257,
                depth=4,
                seq_len=256
            )
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            # –ú–æ–¥–µ–ª—å –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞
            model = checkpoint
        
        model.eval()
        model.to(device)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        model = None
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ tokenizer
    try:
        from tiny_recursive_model.utils import load_tokenizer
        tokenizer, _, _ = load_tokenizer("gpt2")
        print("‚úÖ Tokenizer –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è tokenizer: {e}")
        tokenizer = None


@app.get("/")
def root():
    """Root endpoint"""
    return {
        "service": "TRM Inference Server",
        "status": "running",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None
    }


@app.post("/v1/completions", response_model=CompletionResponse)
def complete(req: CompletionRequest):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç—É (OpenAI-compatible)
    
    Args:
        req: Request –∑ prompt —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    
    Returns:
        Response –∑ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º
    """
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="–ú–æ–¥–µ–ª—å –∞–±–æ tokenizer –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ")
    
    try:
        # Encode prompt
        tokens = tokenizer.encode(req.prompt)
        tokens_tensor = torch.tensor([tokens], device=device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è (—Å–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è)
        model.eval()
        with torch.no_grad():
            # –¢–∏–º—á–∞—Å–æ–≤–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Å—Ç—É –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é
            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—ñ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –ø–æ–≤–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑ sampling
            output = model(tokens_tensor)
            generated_tokens = output.argmax(dim=-1)[0].cpu().tolist()
        
        # Decode
        generated_text = tokenizer.decode(generated_tokens[:req.max_tokens])
        tokens_generated = len(generated_tokens[:req.max_tokens])
        
        return CompletionResponse(
            text=generated_text,
            tokens_generated=tokens_generated
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {str(e)}")


@app.get("/health")
def health():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

