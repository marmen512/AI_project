"""
GGUF Export Script
–ï–∫—Å–ø–æ—Ä—Ç TRM –º–æ–¥–µ–ª—ñ –≤ GGUF —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ llama.cpp —Ç–∞ —Å—É–º—ñ—Å–Ω–∏–º–∏ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
"""
import torch
import json
from pathlib import Path
from typing import Optional, Dict, Any
import sys

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from export.gguf_converter import export_trm_to_gguf
from export.quantization import quantize_model
from tiny_recursive_model.utils import load_tokenizer
from inference.model_inference import load_trained_model
from train.constants import DEFAULT_TOKENIZER_NAME


def main():
    """CLI –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É –º–æ–¥–µ–ª—ñ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Export TRM model to GGUF format",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
  # –ë–∞–∑–æ–≤–∏–π –µ–∫—Å–ø–æ—Ä—Ç:
  python scripts/export_gguf.py --model checkpoints/model.pt --output models/exported/model.gguf
  
  # –ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—î—é:
  python scripts/export_gguf.py --model checkpoints/model.pt --output models/exported/model.gguf --quantization q4
        """
    )
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="Path to PyTorch model checkpoint (.pt file)"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Output path for GGUF file"
    )
    parser.add_argument(
        "--name",
        type=str,
        default="trm",
        help="Model name"
    )
    parser.add_argument(
        "--quantization",
        type=str,
        choices=["q4", "q5", "q8"],
        default=None,
        help="Quantization type (q4, q5, q8)"
    )
    parser.add_argument(
        "--no-tokenizer",
        action="store_true",
        help="Don't include tokenizer"
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to model config JSON file"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        choices=["cpu", "cuda"],
        help="Device for model loading"
    )
    
    args = parser.parse_args()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—É –º–æ–¥–µ–ª—ñ
    model_path = project_root / args.model
    if not model_path.exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {model_path}")
        sys.exit(1)
    
    output_path = project_root / args.output
    
    print("=" * 80)
    print("üì¶ –ï–ö–°–ü–û–†–¢ –ú–û–î–ï–õ–Ü –í GGUF")
    print("=" * 80)
    print(f"üì• –ú–æ–¥–µ–ª—å: {model_path}")
    print(f"üì§ –í–∏—Ö—ñ–¥: {output_path}")
    if args.quantization:
        print(f"üîß –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è: {args.quantization}")
    print()
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
    print("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    try:
        tokenizer, _, _ = load_tokenizer(DEFAULT_TOKENIZER_NAME)
        
        inference = load_trained_model(
            model_path=str(model_path),
            device=args.device,
            tokenizer_name=DEFAULT_TOKENIZER_NAME
        )
        
        model = inference.model
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ config —è–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ
    cfg = {}
    if args.config:
        config_path = project_root / args.config
        try:
            with open(config_path, 'r') as f:
                cfg = json.load(f)
            print(f"‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: {config_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è config: {e}")
            print("   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º")
    else:
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –±–∞–∑–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑ –º–æ–¥–µ–ª—ñ
        cfg = {
            'model': {
                'dim': getattr(model, 'dim', 256),
                'depth': getattr(model, 'depth', 4),
                'vocab_size': getattr(model, 'vocab_size', 50257),
            },
            'training': {},
            'curriculum': {}
        }
    
    # –ï–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –≤ GGUF
    print(f"\nüöÄ –ï–∫—Å–ø–æ—Ä—Ç –≤ GGUF —Ñ–æ—Ä–º–∞—Ç...")
    try:
        gguf_path = export_trm_to_gguf(
            model=model,
            tokenizer=tokenizer,
            cfg=cfg,
            path=output_path
        )
        print(f"‚úÖ GGUF —Ñ–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {gguf_path}")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–∞
    if args.quantization:
        print(f"\nüîß –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è ({args.quantization})...")
        try:
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ç–∏–º—á–∞—Å–æ–≤–∏–π PyTorch checkpoint –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó
            temp_checkpoint = output_path.with_suffix('.temp.pt')
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': cfg
            }, temp_checkpoint)
            
            quantized_path = output_path.with_suffix(f'.{args.quantization}.pt')
            quantize_model(
                model_path=str(temp_checkpoint),
                output_path=str(quantized_path),
                quantization_type=args.quantization
            )
            
            print(f"‚úÖ –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {quantized_path}")
            print(f"‚ö†Ô∏è  –ü—Ä–∏–º—ñ—Ç–∫–∞: –î–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ –≤ GGUF –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏")
            
            # –í–∏–¥–∞–ª–∏—Ç–∏ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
            temp_checkpoint.unlink()
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—ó: {e}")
            print("   –ë–∞–∑–æ–≤–∏–π GGUF —Ñ–∞–π–ª –≤—Å–µ –æ–¥–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
            import traceback
            traceback.print_exc()
    
    # –ï–∫—Å–ø–æ—Ä—Ç tokenizer —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    if not args.no_tokenizer:
        print(f"\nüî§ –ï–∫—Å–ø–æ—Ä—Ç tokenizer...")
        try:
            from export.tokenizer_export import export_tokenizer
            tokenizer_path = output_path.with_suffix('.tokenizer.json')
            export_tokenizer(tokenizer, tokenizer_path)
            print(f"‚úÖ Tokenizer –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {tokenizer_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É tokenizer: {e}")
    
    print("\n" + "=" * 80)
    print("‚úÖ –ï–ö–°–ü–û–†–¢ –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û!")
    print("=" * 80)
    print(f"üìÅ GGUF —Ñ–∞–π–ª: {gguf_path}")
    if not args.no_tokenizer:
        print(f"üìÅ Tokenizer: {output_path.with_suffix('.tokenizer.json')}")
    if args.quantization:
        print(f"üìÅ –ö–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: {output_path.with_suffix(f'.{args.quantization}.pt')}")
    print()


if __name__ == "__main__":
    main()

