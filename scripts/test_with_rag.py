"""
–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ –∑ RAG (Retrieval-Augmented Generation)
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
"""

import torch
from tiny_recursive_model import TinyRecursiveModel, MLPMixer1D
from tiny_recursive_model.utils import load_tokenizer, tokenize_and_pad, prepare_code_input
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional


class RAGSystem:
    """–ü—Ä–æ—Å—Ç–∏–π RAG (Retrieval-Augmented Generation) –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫—É –ø—Ä–∏–∫–ª–∞–¥—ñ–≤"""
    
    def __init__(self, max_examples: int = 50):
        self.examples = []
        self.max_examples = max_examples
    
    def add_example(self, context: str, query: str, completion: str):
        """–î–æ–¥–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥ –¥–æ RAG"""
        self.examples.append({
            'context': context,
            'query': query,
            'completion': completion
        })
        # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ N –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        if len(self.examples) > self.max_examples:
            self.examples.pop(0)
    
    def retrieve_similar(self, query: str, top_k: int = 3) -> List[Dict]:
        """–ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ (–ø—Ä–æ—Å—Ç–∏–π –ø–æ—à—É–∫ –ø–æ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª–æ–≤–∞—Ö)"""
        if not self.examples:
            return []
        
        query_lower = query.lower()
        scored = []
        
        for ex in self.examples:
            score = 0
            # –ü—Ä–æ—Å—Ç–∏–π –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Å–ø—ñ–ª—å–Ω–∏—Ö —Å–ª—ñ–≤
            ex_text = (ex['context'] + ' ' + ex['query']).lower()
            for word in query_lower.split():
                if word in ex_text:
                    score += 1
            scored.append((score, ex))
        
        # –°–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é
        scored.sort(reverse=True, key=lambda x: x[0])
        return [ex for _, ex in scored[:top_k]]


def load_rag_examples(dataset_path: Path, max_examples: int = 50) -> RAGSystem:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è RAG
    
    Args:
        dataset_path: –®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É (JSON —Ñ–∞–π–ª)
        max_examples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        
    Returns:
        RAGSystem –∑ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–º–∏ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏
    """
    rag = RAGSystem(max_examples=max_examples)
    
    if not dataset_path.exists():
        print(f"‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {dataset_path}")
        return rag
    
    print(f"üìö –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è RAG –∑ {dataset_path.name}...")
    
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —Ä—ñ–∑–Ω–∏—Ö —Ñ–æ—Ä–º–∞—Ç—ñ–≤ –¥–∞—Ç–∞—Å–µ—Ç—É
        if isinstance(data, dict) and 'data' in data:
            examples = data['data']
        elif isinstance(data, list):
            examples = data
        else:
            examples = []
        
        # –û–±–º–µ–∂–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        examples = examples[:max_examples]
        
        # –î–æ–¥–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–æ RAG
        for ex in examples:
            context = ex.get('context', '')
            query = ex.get('query', '')
            completion = ex.get('completion', '')
            
            if context or query or completion:
                rag.add_example(context, query, completion)
        
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(rag.examples)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è RAG")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É: {e}")
    
    return rag


def format_rag_context(similar_examples: List[Dict]) -> str:
    """
    –§–æ—Ä–º–∞—Ç—É–≤–∞—Ç–∏ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑—ñ —Å—Ö–æ–∂–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
    
    Args:
        similar_examples: –°–ø–∏—Å–æ–∫ —Å—Ö–æ–∂–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        
    Returns:
        –í—ñ–¥—Ñ–æ—Ä–º–∞—Ç–æ–≤–∞–Ω–∏–π RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç
    """
    if not similar_examples:
        return ""
    
    rag_context = "\n\n–ü—Ä–∏–∫–ª–∞–¥–∏ —Å—Ö–æ–∂–∏—Ö –∑–∞–≤–¥–∞–Ω—å:\n"
    for idx, ex in enumerate(similar_examples, 1):
        context_preview = ex['context'][:150] + "..." if len(ex['context']) > 150 else ex['context']
        completion_preview = ex['completion'][:150] + "..." if len(ex['completion']) > 150 else ex['completion']
        
        rag_context += f"{idx}. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_preview}\n"
        rag_context += f"   –ó–∞–ø–∏—Ç: {ex['query']}\n"
        rag_context += f"   –í—ñ–¥–ø–æ–≤—ñ–¥—å: {completion_preview}\n\n"
    
    return rag_context


def load_model(model_path="trm_optimized.pt", dim=512, depth=4, seq_len=2048, vocab_size=50257):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–≤—á–µ–Ω—É –º–æ–¥–µ–ª—å"""
    print("üì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    model = TinyRecursiveModel(
        dim=dim,
        num_tokens=vocab_size,
        network=MLPMixer1D(dim=dim, depth=depth, seq_len=seq_len)
    )
    model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.eval()
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞: {model_path}")
    return model

def test_comprehensive(model, tokenizer, test_cases, rag_system: Optional[RAGSystem] = None, 
                      rag_top_k: int = 3, seq_len=2048):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ RAG –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é
    
    Args:
        model: –ù–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
        test_cases: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤
        rag_system: RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ—à—É–∫—É —Å—Ö–æ–∂–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        rag_top_k: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ö–æ–∂–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
        seq_len: –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
    """
    results = []
    
    print("\n" + "="*70)
    print("üß™ –ö–û–ú–ü–õ–ï–ö–°–ù–ï –¢–ï–°–¢–£–í–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü" + (" –ó RAG" if rag_system else ""))
    print("="*70)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'‚îÄ'*70}")
        print(f"–¢–ï–°–¢ {i}/{len(test_cases)}: {test_case['name']}")
        print(f"{'‚îÄ'*70}")
        print(f"üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{test_case['context']}")
        print(f"\n‚ùì –ó–∞–ø–∏—Ç: {test_case['query']}")
        
        try:
            # RAG: –ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
            rag_context = ""
            used_rag = False
            similar_examples = []
            if rag_system:
                similar_examples = rag_system.retrieve_similar(test_case['query'], top_k=rag_top_k)
                if similar_examples:
                    rag_context = format_rag_context(similar_examples)
                    used_rag = True
                    print(f"\nüîç RAG: –ó–Ω–∞–π–¥–µ–Ω–æ {len(similar_examples)} —Å—Ö–æ–∂–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
            
            # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥ (–∑ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —è–∫—â–æ —î)
            base_input = prepare_code_input(test_case['context'], test_case['query'])
            input_text = rag_context + base_input if rag_context else base_input
            
            # –û—Ç—Ä–∏–º–∞—Ç–∏ pad_token_id
            pad_token_id = 0
            if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:
                pad_token_id = tokenizer.pad_token_id
            
            # –¢–æ–∫–µ–Ω—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∏ padding
            input_ids = tokenize_and_pad(
                tokenizer,
                input_text,
                seq_len,
                pad_token_id=pad_token_id,
                truncation=True
            ).unsqueeze(0)
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            print("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ...")
            start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
            end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None
            
            if start_time:
                start_time.record()
            else:
                import time
                start_cpu = time.time()
            
            with torch.no_grad():
                pred_tokens, exit_steps = model.predict(
                    input_ids,
                    max_deep_refinement_steps=12,
                    halt_prob_thres=0.5
                )
            
            if end_time:
                end_time.record()
                torch.cuda.synchronize()
                gen_time = start_time.elapsed_time(end_time) / 1000
            else:
                gen_time = time.time() - start_cpu
            
            # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏
            output = tokenizer.decode(pred_tokens[0][:300], skip_special_tokens=True)
            steps = exit_steps[0].item() if len(exit_steps) > 0 else 0
            
            print(f"\nüì§ –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ:")
            print(f"{output[:500]}{'...' if len(output) > 500 else ''}")
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   - –ö—Ä–æ–∫–∏ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: {steps}")
            print(f"   - –ß–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {gen_time:.2f}—Å")
            print(f"   - –î–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ: {len(output)} —Å–∏–º–≤–æ–ª—ñ–≤")
            
            # –ê–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ
            has_code = any(c in output for c in ['def ', 'class ', 'import ', 'return ', '='])
            has_structure = any(c in output for c in ['\n', '    ', '(', ')'])
            
            print(f"   - –ú—ñ—Å—Ç–∏—Ç—å –∫–æ–¥: {'‚úÖ' if has_code else '‚ùå'}")
            print(f"   - –°—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∞: {'‚úÖ' if has_structure else '‚ùå'}")
            
            results.append({
                'test': test_case['name'],
                'input': test_case['query'],
                'output': output,
                'steps': steps,
                'time': gen_time,
                'has_code': has_code,
                'has_structure': has_structure,
                'used_rag': used_rag,
                'rag_examples_count': len(similar_examples),
                'success': True
            })
            
        except Exception as e:
            print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            results.append({
                'test': test_case['name'],
                'input': test_case['query'],
                'error': str(e),
                'success': False
            })
    
    return results

def generate_report(results, model_path, rag_enabled: bool = False):
    """–ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∑–≤—ñ—Ç"""
    print("\n" + "="*70)
    print("üìä –ó–í–Ü–¢ –ü–†–û –¢–ï–°–¢–£–í–ê–ù–ù–Ø")
    print("="*70)
    
    successful = sum(1 for r in results if r.get('success', False))
    total = len(results)
    
    print(f"\nüìà –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –£—Å–ø—ñ—à–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤: {successful}/{total} ({successful/total*100:.1f}%)")
    print(f"   –ü–æ–º–∏–ª–æ–∫: {total - successful}")
    
    if successful > 0:
        avg_steps = sum(r.get('steps', 0) for r in results if r.get('success')) / successful
        avg_time = sum(r.get('time', 0) for r in results if r.get('success')) / successful
        code_count = sum(1 for r in results if r.get('has_code', False))
        
        print(f"\nüìä –°–µ—Ä–µ–¥–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏:")
        print(f"   –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤: {avg_steps:.1f}")
        print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {avg_time:.2f}—Å")
        print(f"   –¢–µ—Å—Ç—ñ–≤ –∑ –∫–æ–¥–æ–º: {code_count}/{successful} ({code_count/successful*100:.1f}%)")
        
        # RAG —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if rag_enabled:
            rag_used_count = sum(1 for r in results if r.get('used_rag', False))
            avg_rag_examples = sum(r.get('rag_examples_count', 0) for r in results if r.get('success')) / successful
            print(f"\nüîç RAG —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   –¢–µ—Å—Ç—ñ–≤ –∑ RAG: {rag_used_count}/{successful} ({rag_used_count/successful*100:.1f}%)")
            print(f"   –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å RAG –ø—Ä–∏–∫–ª–∞–¥—ñ–≤: {avg_rag_examples:.1f}")
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–≤—ñ—Ç
    report = {
        'timestamp': datetime.now().isoformat(),
        'model': model_path,
        'total_tests': total,
        'successful': successful,
        'rag_enabled': rag_enabled,
        'results': results
    }
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É temp —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
    project_root = Path(__file__).parent.parent
    temp_dir = project_root / "temp"
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    report_path = temp_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
    
    return report

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ –∑ RAG (Retrieval-Augmented Generation)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
  # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ RAG (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–Ω–∞–π–¥–µ eval –¥–∞—Ç–∞—Å–µ—Ç):
  python scripts/test_with_rag.py --model models/trained/openassistant_train.pt
  
  # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º RAG –¥–∞—Ç–∞—Å–µ—Ç–æ–º:
  python scripts/test_with_rag.py \\
      --model models/trained/openassistant_train.pt \\
      --rag-dataset datasets/eval/openassistant_eval.json
  
  # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –±–µ–∑ RAG:
  python scripts/test_with_rag.py \\
      --model models/trained/openassistant_train.pt \\
      --disable-rag
        """
    )
    parser.add_argument("--model", type=str, default="trm_optimized.pt",
                       help="–®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--dim", type=int, default=512,
                       help="–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--depth", type=int, default=4,
                       help="–ì–ª–∏–±–∏–Ω–∞")
    parser.add_argument("--seq-len", type=int, default=2048,
                       help="–î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ")
    parser.add_argument("--rag-dataset", type=str, default=None,
                       help="–®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è RAG (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–Ω–∞–π—Ç–∏ –≤ datasets/eval/)")
    parser.add_argument("--rag-top-k", type=int, default=3,
                       help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ö–æ–∂–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ RAG (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 3)")
    parser.add_argument("--rag-max-examples", type=int, default=50,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤ RAG (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 50)")
    parser.add_argument("--disable-rag", action="store_true",
                       help="–í–∏–º–∫–Ω—É—Ç–∏ RAG (–¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤)")
    
    args = parser.parse_args()
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ project_root
    project_root = Path(__file__).parent.parent
    
    # –¢–µ—Å—Ç–æ–≤—ñ –≤–∏–ø–∞–¥–∫–∏
    test_cases = [
        {
            'name': '–ê–Ω–∞–ª—ñ–∑ —Ñ—É–Ω–∫—Ü—ñ—ó',
            'context': 'def add(a, b):\n    return a + b',
            'query': '–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —Ü—é —Ñ—É–Ω–∫—Ü—ñ—é —Ç–∞ –¥–æ–¥–∞–π docstring'
        },
        {
            'name': '–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–¥—É',
            'context': 'def sum_list(numbers):\n    total = 0\n    for num in numbers:\n        total += num\n    return total',
            'query': '–û–ø—Ç–∏–º—ñ–∑—É–π —Ü—é —Ñ—É–Ω–∫—Ü—ñ—é'
        },
        {
            'name': '–î–æ–¥–∞–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ',
            'context': 'class User:\n    def __init__(self, name):\n        self.name = name',
            'query': '–î–æ–¥–∞–π –º–µ—Ç–æ–¥ get_email'
        },
        {
            'name': '–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫',
            'context': 'def divide(a, b):\n    return a / b',
            'query': '–î–æ–¥–∞–π –æ–±—Ä–æ–±–∫—É –ø–æ–º–∏–ª–æ–∫ –¥–ª—è –¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω—É–ª—å'
        },
        {
            'name': '–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥',
            'context': 'data = [1, 2, 3, 4, 5]\nresult = []\nfor x in data:\n    result.append(x ** 2)',
            'query': '–ü–µ—Ä–µ–ø–∏—à–∏ —á–µ—Ä–µ–∑ list comprehension'
        }
    ]
    
    print("="*70)
    print("üß™ –¢–ï–°–¢–£–í–ê–ù–ù–Ø –ù–ê–í–ß–ï–ù–û–á –ú–û–î–ï–õ–Ü")
    print("="*70)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    print("\n1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer, vocab_size, pad_token_id = load_tokenizer("gpt2")
        print(f"‚úÖ GPT-2 tokenizer –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ (vocab_size={vocab_size})")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        exit(1)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
    print("\n2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    try:
        model_path = project_root / args.model if not Path(args.model).is_absolute() else Path(args.model)
        model = load_model(str(model_path), args.dim, args.depth, args.seq_len)
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        exit(1)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ RAG —Å–∏—Å—Ç–µ–º—É (—è–∫—â–æ –Ω–µ –≤–∏–º–∫–Ω–µ–Ω–æ)
    rag_system = None
    if not args.disable_rag:
        print("\n3. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RAG —Å–∏—Å—Ç–µ–º–∏...")
        rag_dataset_path = None
        
        if args.rag_dataset:
            rag_dataset_path = project_root / args.rag_dataset if not Path(args.rag_dataset).is_absolute() else Path(args.rag_dataset)
        else:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–Ω–∞–π—Ç–∏ eval –¥–∞—Ç–∞—Å–µ—Ç
            eval_dir = project_root / "datasets" / "eval"
            eval_datasets = list(eval_dir.glob("*.json"))
            if eval_datasets:
                rag_dataset_path = eval_datasets[0]
                print(f"üìö –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–±—Ä–∞–Ω–æ eval –¥–∞—Ç–∞—Å–µ—Ç: {rag_dataset_path.name}")
            else:
                # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ train –¥–∞—Ç–∞—Å–µ—Ç
                train_dir = project_root / "datasets" / "train"
                train_datasets = list(train_dir.glob("*.json"))
                if train_datasets:
                    rag_dataset_path = train_datasets[0]
                    print(f"üìö –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è train –¥–∞—Ç–∞—Å–µ—Ç: {rag_dataset_path.name}")
        
        if rag_dataset_path and rag_dataset_path.exists():
            rag_system = load_rag_examples(rag_dataset_path, max_examples=args.rag_max_examples)
            if len(rag_system.examples) == 0:
                print("‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è RAG, RAG –≤–∏–º–∫–Ω–µ–Ω–æ")
                rag_system = None
        else:
            print("‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è RAG –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, RAG –≤–∏–º–∫–Ω–µ–Ω–æ")
            rag_system = None
    else:
        print("\n3. RAG –≤–∏–º–∫–Ω–µ–Ω–æ (--disable-rag)")
    
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    print("\n4. –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç—ñ–≤...")
    results = test_comprehensive(
        model, 
        tokenizer, 
        test_cases, 
        rag_system=rag_system,
        rag_top_k=args.rag_top_k,
        seq_len=args.seq_len
    )
    
    # –ó–≤—ñ—Ç
    report = generate_report(results, args.model, rag_enabled=rag_system is not None)
    
    print("\n" + "="*70)
    print("‚úÖ –¢–ï–°–¢–£–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*70)

