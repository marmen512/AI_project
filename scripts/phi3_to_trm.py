"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Phi-3.5-mini –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è Tiny Recursive Model
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Phi-3.5 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞ knowledge distillation
"""

import json
import os
import time
from typing import List, Dict
from pathlib import Path
from datetime import datetime, timedelta

try:
    from llama_cpp import Llama
except ImportError:
    print("‚ö†Ô∏è llama-cpp-python –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª—é—é...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "llama-cpp-python", "--quiet"])
    from llama_cpp import Llama

try:
    from tqdm import tqdm
except ImportError:
    print("‚ö†Ô∏è tqdm –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª—é—é...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "tqdm", "--quiet"])
    from tqdm import tqdm


class RAGSystem:
    """–ü—Ä–æ—Å—Ç–∏–π RAG (Retrieval-Augmented Generation) –¥–ª—è –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫—É –ø—Ä–∏–∫–ª–∞–¥—ñ–≤"""
    
    def __init__(self):
        self.examples = []
        self.max_examples = 50  # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ 50 –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
    
    def add_example(self, context: str, query: str, completion: str):
        """–î–æ–¥–∞—Ç–∏ –ø—Ä–∏–∫–ª–∞–¥ –¥–æ RAG"""
        self.examples.append({
            'context': context,
            'query': query,
            'completion': completion
        })
        # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ N –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        if len(self.examples) > self.max_examples:
            self.examples.pop(0)
    
    def retrieve_similar(self, query: str, top_k: int = 3) -> List[Dict]:
        """–ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ (–ø—Ä–æ—Å—Ç–∏–π –ø–æ—à—É–∫ –ø–æ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª–æ–≤–∞—Ö)"""
        if not self.examples:
            return []
        
        query_lower = query.lower()
        scored = []
        
        for ex in self.examples:
            score = 0
            # –ü—Ä–æ—Å—Ç–∏–π –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Å–ø—ñ–ª—å–Ω–∏—Ö —Å–ª—ñ–≤
            ex_text = (ex['context'] + ' ' + ex['query']).lower()
            for word in query_lower.split():
                if word in ex_text:
                    score += 1
            scored.append((score, ex))
        
        # –°–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é
        scored.sort(reverse=True, key=lambda x: x[0])
        return [ex for _, ex in scored[:top_k]]


class Phi3ToTRM:
    """–ö–ª–∞—Å –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Phi-3.5 –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è TRM"""
    
    def __init__(self, phi3_model_path: str, n_ctx: int = 2048, n_threads: int = None, n_gpu_layers: int = 0):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Phi-3.5 –º–æ–¥–µ–ª—ñ
        
        Args:
            phi3_model_path: –®–ª—è—Ö –¥–æ GGUF —Ñ–∞–π–ª—É
            n_ctx: –†–æ–∑–º—ñ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            n_threads: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
            n_gpu_layers: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤ –Ω–∞ GPU (0 = —Ç—ñ–ª—å–∫–∏ CPU)
        """
        import multiprocessing
        if n_threads is None:
            n_threads = max(multiprocessing.cpu_count() - 2, 1)  # –ó–∞–ª–∏—à–∏—Ç–∏ 2 —è–¥—Ä–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º–∏
        
        print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Phi-3.5 –∑ {phi3_model_path}...")
        print(f"   –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è {n_threads} CPU –ø–æ—Ç–æ–∫—ñ–≤")
        if n_gpu_layers > 0:
            print(f"   {n_gpu_layers} —à–∞—Ä—ñ–≤ –Ω–∞ GPU")
        
        self.llm = Llama(
            model_path=phi3_model_path,
            n_ctx=n_ctx,
            n_threads=n_threads,
            n_gpu_layers=n_gpu_layers,
            verbose=False
        )
        print("‚úÖ Phi-3.5 –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ RAG —Å–∏—Å—Ç–µ–º—É
        self.rag = RAGSystem()
    
    def generate_response(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥ Phi-3.5
        
        Args:
            prompt: –ó–∞–ø–∏—Ç
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
            
        Returns:
            –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
        """
        response = self.llm(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            repeat_penalty=1.1,
            stop=["[QUERY]", "\n\n\n"],
            echo=False
        )
        return response['choices'][0]['text'].strip()
    
    def generate_dataset_from_seeds(self, seed_examples: List[Dict], num_generations: int = 100) -> List[Dict]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤–µ–ª–∏–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ seed –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        
        Args:
            seed_examples: –ü–æ—á–∞—Ç–∫–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
            num_generations: –°–∫—ñ–ª—å–∫–∏ –Ω–æ–≤–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏
            
        Returns:
            –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        """
        print(f"\n–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è {num_generations} –Ω–æ–≤–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤...")
        dataset = seed_examples.copy()
        
        # –î–æ–¥–∞—Ç–∏ seed –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–æ RAG
        for ex in seed_examples:
            self.rag.add_example(ex.get('context', ''), ex.get('query', ''), ex.get('completion', ''))
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
        start_time = time.time()
        pbar = tqdm(total=num_generations, desc="–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è", unit="–ø—Ä–∏–∫–ª–∞–¥", 
                   bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
        
        # –®–∞–±–ª–æ–Ω–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∑–∞–≤–¥–∞–Ω—å
        task_templates = [
            "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü—ñ—é —è–∫–∞ {task}",
            "–°—Ç–≤–æ—Ä–∏ –∫–ª–∞—Å –¥–ª—è {task}",
            "–û–ø—Ç–∏–º—ñ–∑—É–π –∫–æ–¥ —è–∫–∏–π {task}",
            "–î–æ–¥–∞–π –æ–±—Ä–æ–±–∫—É –ø–æ–º–∏–ª–æ–∫ –¥–ª—è {task}",
            "–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏ –∫–æ–¥ —è–∫–∏–π {task}",
            "–î–æ–¥–∞–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é –¥–ª—è {task}",
            "–ü–æ–∫—Ä–∞—â –±–µ–∑–ø–µ–∫—É –∫–æ–¥—É —è–∫–∏–π {task}",
        ]
        
        task_descriptions = [
            "–æ–±—á–∏—Å–ª—é—î —Å—É–º—É —á–∏—Å–µ–ª",
            "—á–∏—Ç–∞—î —Ñ–∞–π–ª",
            "–≤—ñ–¥–ø—Ä–∞–≤–ª—è—î HTTP –∑–∞–ø–∏—Ç",
            "–æ–±—Ä–æ–±–ª—è—î JSON –¥–∞–Ω—ñ",
            "–ø—Ä–∞—Ü—é—î –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö",
            "–≥–µ–Ω–µ—Ä—É—î –≤–∏–ø–∞–¥–∫–æ–≤—ñ —á–∏—Å–ª–∞",
            "—à–∏—Ñ—Ä—É—î –¥–∞–Ω—ñ",
            "–≤–∞–ª—ñ–¥—É—î email",
            "–ø–∞—Ä—Å–∏—Ç—å XML",
            "–æ–±—á–∏—Å–ª—é—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É",
            "—Ñ—ñ–ª—å—Ç—Ä—É—î —Å–ø–∏—Å–æ–∫",
            "—Å–æ—Ä—Ç—É—î –¥–∞–Ω—ñ",
            "–∫–µ—à—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏",
            "–ª–æ–≥—É—î –ø–æ–¥—ñ—ó",
            "–≤—ñ–¥–ø—Ä–∞–≤–ª—è—î email",
        ]
        
        # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è: –≤–∏–Ω–µ—Å—Ç–∏ import –∑–∞ —Ü–∏–∫–ª
        import random
        
        for i in range(num_generations):
            # –í–∏–ø–∞–¥–∫–æ–≤–∏–π —à–∞–±–ª–æ–Ω —Ç–∞ –æ–ø–∏—Å
            template = random.choice(task_templates)
            description = random.choice(task_descriptions)
            task = template.format(task=description)
            
            # RAG: –ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
            similar_examples = self.rag.retrieve_similar(task, top_k=2)
            rag_context = ""
            if similar_examples:
                rag_context = "\n\n–ü—Ä–∏–∫–ª–∞–¥–∏ —Å—Ö–æ–∂–∏—Ö –∑–∞–≤–¥–∞–Ω—å:\n"
                for idx, ex in enumerate(similar_examples, 1):
                    rag_context += f"{idx}. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {ex['context'][:100]}...\n"
                    rag_context += f"   –ó–∞–ø–∏—Ç: {ex['query']}\n"
                    rag_context += f"   –í—ñ–¥–ø–æ–≤—ñ–¥—å: {ex['completion'][:100]}...\n\n"
            
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –∑–∞–ø–∏—Ç –¥–ª—è Phi-3.5 –∑ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            prompt = f"""–¢–∏ - –µ–∫—Å–ø–µ—Ä—Ç Python —Ä–æ–∑—Ä–æ–±–Ω–∏–∫. –°—Ç–≤–æ—Ä–∏ –ø—Ä–∏–∫–ª–∞–¥ –∫–æ–¥—É —Ç–∞ –∑–∞–ø–∏—Ç—É.
{rag_context}

–ó–∞–≤–¥–∞–Ω–Ω—è: {task}

–°—Ç–≤–æ—Ä–∏ JSON –æ–±'—î–∫—Ç –∑ —Ç–∞–∫–∏–º–∏ –ø–æ–ª—è–º–∏:
- "context": –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –∫–æ–¥ Python (–ø—Ä–æ—Å—Ç–∏–π, 5-10 —Ä—è–¥–∫—ñ–≤)
- "query": –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ —â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑—Ä–æ–±–∏—Ç–∏ –∑ –∫–æ–¥–æ–º
- "completion": –æ—á—ñ–∫—É–≤–∞–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å (–ø–æ–∫—Ä–∞—â–µ–Ω–∏–π/–¥–æ–ø–æ–≤–Ω–µ–Ω–∏–π –∫–æ–¥)

–§–æ—Ä–º–∞—Ç:
{{
  "context": "def example():\n    pass",
  "query": "–î–æ–¥–∞–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å",
  "completion": "def example():\n    # –î–æ–¥–∞–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å\n    return result"
}}

–°—Ç–≤–æ—Ä–∏ –ø—Ä–∏–∫–ª–∞–¥:"""
            
            try:
                response = self.generate_response(prompt, max_tokens=800, temperature=0.8)
                
                # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –≤–∏—Ç—è–≥—Ç–∏ JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    example = json.loads(json_str)
                    
                    # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è
                    if all(k in example for k in ['context', 'query', 'completion']):
                        # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è: –æ–±—Ä—ñ–∑–∞—Ç–∏ –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥—ñ —Ç–µ–∫—Å—Ç–∏
                        max_len = 2000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É
                        for key in ['context', 'query', 'completion']:
                            if len(example[key]) > max_len:
                                example[key] = example[key][:max_len]
                        
                        dataset.append(example)
                        # –î–æ–¥–∞—Ç–∏ –¥–æ RAG –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –≥–µ–Ω–µ—Ä–∞—Ü—ñ–π
                        self.rag.add_example(
                            example['context'],
                            example['query'],
                            example['completion']
                        )
                
            except (json.JSONDecodeError, KeyError) as e:
                # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON, –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏
                if i % 10 == 0:  # –õ–æ–≥—É–≤–∞—Ç–∏ –ø–æ–º–∏–ª–∫–∏ –∫–æ–∂–Ω—ñ 10 —ñ—Ç–µ—Ä–∞—Ü—ñ–π
                    pass  # –¢–∏—Ö–∞ –æ–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫
            except Exception as e:
                # –Ü–Ω—à—ñ –ø–æ–º–∏–ª–∫–∏ —Ç–∞–∫–æ–∂ —Ç–∏—Ö–æ –æ–±—Ä–æ–±–ª—è—Ç–∏
                if i % 10 == 0:
                    pass
            
            # –û–Ω–æ–≤–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä
            pbar.update(1)
            
            # –û–Ω–æ–≤–∏—Ç–∏ –æ–ø–∏—Å –∑ –æ—Ü—ñ–Ω–∫–æ—é —á–∞—Å—É
            elapsed = time.time() - start_time
            if i > 0:
                avg_time = elapsed / (i + 1)
                remaining = avg_time * (num_generations - i - 1)
                pbar.set_postfix({
                    '–£—Å–ø—ñ—à–Ω–æ': len(dataset) - len(seed_examples),
                    '–ó–∞–ª–∏—à–∏–ª–æ—Å—å': f"{timedelta(seconds=int(remaining))}"
                })
        
        pbar.close()
        elapsed_total = time.time() - start_time
        
        print(f"\n‚úÖ –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ {len(dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ (–±—É–ª–æ {len(seed_examples)})")
        print(f"‚è±Ô∏è –ß–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {timedelta(seconds=int(elapsed_total))}")
        print(f"üìä –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {((len(dataset) - len(seed_examples)) / num_generations * 100):.1f}%")
        return dataset
    
    def enhance_existing_dataset(self, dataset_path: str, output_path: str, num_enhancements: int = 50):
        """
        –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É —á–µ—Ä–µ–∑ Phi-3.5
        
        Args:
            dataset_path: –®–ª—è—Ö –¥–æ —ñ—Å–Ω—É—é—á–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
            output_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
            num_enhancements: –°–∫—ñ–ª—å–∫–∏ –Ω–æ–≤–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ —Å—Ç–≤–æ—Ä–∏—Ç–∏
        """
        print(f"\n–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É {dataset_path}...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        print(f"–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Ä–æ–∑–º—ñ—Ä: {len(dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
        
        # –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ—Å–Ω—É—é—á–∏—Ö
        enhanced_dataset = self.generate_dataset_from_seeds(dataset, num_enhancements)
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enhanced_dataset, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")
        print(f"   –†–æ–∑–º—ñ—Ä: {len(enhanced_dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
        
        return enhanced_dataset
    
    def create_training_dataset(self, output_path: str = "phi3_enhanced_dataset.json", num_examples: int = 500, teacher_model_path: str = None):
        """
        –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–æ–≤–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è TRM
        
        Args:
            output_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
            num_examples: –°–∫—ñ–ª—å–∫–∏ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —Å—Ç–≤–æ—Ä–∏—Ç–∏
            teacher_model_path: –®–ª—è—Ö –¥–æ teacher –º–æ–¥–µ–ª—ñ (–¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö)
        """
        print(f"\n–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É –∑ {num_examples} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤...")
        
        # –ü–æ—á–∞—Ç–∫–æ–≤—ñ seed –ø—Ä–∏–∫–ª–∞–¥–∏
        seed_examples = [
            {
                "context": "def add(a, b):\n    return a + b",
                "query": "–î–æ–¥–∞–π docstring",
                "completion": "def add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b"
            },
            {
                "context": "def process_data(data):\n    result = []\n    for item in data:\n        result.append(item * 2)\n    return result",
                "query": "–ü–µ—Ä–µ–ø–∏—à–∏ —á–µ—Ä–µ–∑ list comprehension",
                "completion": "def process_data(data):\n    return [item * 2 for item in data]"
            }
        ]
        
        # –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
        dataset = self.generate_dataset_from_seeds(seed_examples, num_examples)
        
        # –î–æ–¥–∞—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ –ø—Ä–æ teacher –º–æ–¥–µ–ª—å
        import hashlib
        from pathlib import Path
        
        dataset_metadata = {
            'metadata': {
                'teacher_model_path': teacher_model_path or str(self.llm.model_path) if hasattr(self.llm, 'model_path') else None,
                'teacher_model_name': Path(teacher_model_path).stem if teacher_model_path else None,
                'num_examples': len(dataset),
                'generated_at': str(Path(output_path).stat().st_mtime) if Path(output_path).exists() else None,
            },
            'data': dataset
        }
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_metadata, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ: {output_path}")
        print(f"   Teacher –º–æ–¥–µ–ª—å: {dataset_metadata['metadata'].get('teacher_model_name', '–Ω–µ–≤—ñ–¥–æ–º–æ')}")
        return dataset


def get_dataset_name_from_model(model_path: str) -> str:
    """
    –í–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–∞–∑–≤—É –¥–∞—Ç–∞—Å–µ—Ç—É –∑ –Ω–∞–∑–≤–∏ GGUF –º–æ–¥–µ–ª—ñ
    
    Args:
        model_path: –®–ª—è—Ö –¥–æ GGUF –º–æ–¥–µ–ª—ñ
        
    Returns:
        –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É —É —Ñ–æ—Ä–º–∞—Ç—ñ {–Ω–∞–∑–≤–∞_–º–æ–¥–µ–ª—ñ}_training_dataset.json
    """
    from pathlib import Path
    model_name = Path(model_path).stem  # –ë–µ–∑ .gguf
    return f"{model_name}_training_dataset.json"


def main():
    import argparse
    import sys
    from pathlib import Path
    
    # –î–æ–¥–∞—Ç–∏ config –¥–æ —à–ª—è—Ö—É –¥–ª—è —ñ–º–ø–æ—Ä—Ç—É
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root))
    
    try:
        from config import GGUFModelManager
        model_manager = GGUFModelManager()
        default_model = model_manager.get_default_model()
        if default_model:
            default_model_path = default_model['path']
        else:
            default_model_path = str(project_root / "models" / "gguf" / "phi-3.5-mini-instruct-q4_k_m.gguf")
    except:
        default_model_path = str(project_root / "models" / "gguf" / "phi-3.5-mini-instruct-q4_k_m.gguf")
    
    parser = argparse.ArgumentParser(description="–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è Phi-3.5 –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è TRM")
    parser.add_argument("--phi3-model", type=str, default=None,
                       help=f"–®–ª—è—Ö –¥–æ Phi-3.5 GGUF –º–æ–¥–µ–ª—ñ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–Ω–∞–π—Ç–∏ –≤ models/gguf/)")
    parser.add_argument("--enhance", type=str, default=None,
                       help="–ü–æ–∫—Ä–∞—â–∏—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π –¥–∞—Ç–∞—Å–µ—Ç (—à–ª—è—Ö –¥–æ JSON)")
    parser.add_argument("--create", action="store_true",
                       help="–°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç")
    parser.add_argument("--output", type=str, default=None,
                       help="–®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É. –Ø–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ, –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –Ω–∞–∑–≤—É –∑ GGUF –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--num-examples", type=int, default=200,
                       help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó")
    parser.add_argument("--n-ctx", type=int, default=2048,
                       help="–†–æ–∑–º—ñ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É")
    parser.add_argument("--n-threads", type=int, default=None,
                       help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)")
    parser.add_argument("--n-gpu-layers", type=int, default=0,
                       help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤ –Ω–∞ GPU (0 = —Ç—ñ–ª—å–∫–∏ CPU)")
    
    args = parser.parse_args()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –º–æ–¥–µ–ª—å —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–∞
    if args.phi3_model is None:
        try:
            from config import GGUFModelManager
            model_manager = GGUFModelManager()
            default_model = model_manager.get_default_model()
            if default_model:
                args.phi3_model = default_model['path']
                print(f"üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –º–æ–¥–µ–ª—å: {default_model['name']}")
            else:
                print("‚ùå GGUF –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ models/gguf/")
                print("   –î–æ–¥–∞–π—Ç–µ .gguf —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É models/gguf/")
                return
        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            print("   –í–∫–∞–∂—ñ—Ç—å --phi3-model –≤—Ä—É—á–Ω—É")
            return
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ
    if not os.path.exists(args.phi3_model):
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {args.phi3_model}")
        print(f"   –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –∞–±–æ –¥–æ–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤ models/gguf/")
        return
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–∞–∑–≤—É –¥–∞—Ç–∞—Å–µ—Ç—É —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ
    if args.output is None:
        dataset_name = get_dataset_name_from_model(args.phi3_model)
        # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤ datasets/train/ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        datasets_train_dir = project_root / "datasets" / "train"
        datasets_train_dir.mkdir(parents=True, exist_ok=True)
        args.output = str(datasets_train_dir / dataset_name)
        print(f"üìù –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É –≤–∏–∑–Ω–∞—á–µ–Ω–∞ –∑ –º–æ–¥–µ–ª—ñ: {dataset_name}")
        print(f"üìÅ –®–ª—è—Ö –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {args.output}")
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ Phi-3.5
    try:
        phi3 = Phi3ToTRM(
            args.phi3_model,
            n_ctx=args.n_ctx,
            n_threads=args.n_threads,
            n_gpu_layers=args.n_gpu_layers
        )
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Phi-3.5: {e}")
        return
    
    # –û–±—Ä–æ–±–∏—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ä–µ–∂–∏–º—É
    if args.enhance:
        # –ü–æ–∫—Ä–∞—â–∏—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        phi3.enhance_existing_dataset(
            args.enhance,
            args.output,
            args.num_examples
        )
    elif args.create:
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        phi3.create_training_dataset(
            args.output,
            args.num_examples,
            teacher_model_path=args.phi3_model
        )
    else:
        print("–í–∫–∞–∂—ñ—Ç—å --enhance –∞–±–æ --create")
        print("\n–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:")
        print("  # –°—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç:")
        print("  python phi3_to_trm.py --create --num-examples 500")
        print("\n  # –ü–æ–∫—Ä–∞—â–∏—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π:")
        print("  python phi3_to_trm.py --enhance ai_assistant_dataset.json --num-examples 200")


if __name__ == "__main__":
    main()

