"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ —Ä–æ–±–æ—Ç–∏ TRM –º–æ–¥–µ–ª—ñ
"""
import torch
import json
import os
from pathlib import Path

from tiny_recursive_model import TinyRecursiveModel, MLPMixer1D, Trainer
from tiny_recursive_model.utils import tokenize_and_pad, prepare_code_input, load_tokenizer
from torch.utils.data import Dataset


class SimpleCodeDataset(Dataset):
    """–ü—Ä–æ—Å—Ç–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó"""
    def __init__(self, tokenizer, max_seq_len=256, pad_token_id=0):
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.pad_token_id = pad_token_id
        
        # –ü—Ä–æ—Å—Ç—ñ –ø—Ä–∏–∫–ª–∞–¥–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
        self.data = [
            {
                "context": "def hello():\n    return 'world'",
                "query": "–î–æ–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä name",
                "completion": "def hello(name='world'):\n    return name"
            },
            {
                "context": "x = 5\ny = 10",
                "query": "–î–æ–¥–∞–π –æ–±—á–∏—Å–ª–µ–Ω–Ω—è —Å—É–º–∏",
                "completion": "x = 5\ny = 10\nresult = x + y"
            },
            {
                "context": "def add(a, b):\n    pass",
                "query": "–†–µ–∞–ª—ñ–∑—É–π —Ñ—É–Ω–∫—Ü—ñ—é",
                "completion": "def add(a, b):\n    return a + b"
            },
            {
                "context": "items = [1, 2, 3]",
                "query": "–ó–Ω–∞–π–¥–∏ –º–∞–∫—Å–∏–º—É–º",
                "completion": "items = [1, 2, 3]\nmax_item = max(items)"
            },
            {
                "context": "name = 'John'",
                "query": "–î–æ–¥–∞–π –ø—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è",
                "completion": "name = 'John'\ngreeting = f'Hello, {name}!'"
            }
        ]
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        context = item.get('context', '')
        query = item.get('query', '')
        completion = item.get('completion', '')
        
        input_text = prepare_code_input(context, query)
        
        input_ids = tokenize_and_pad(
            self.tokenizer,
            input_text,
            self.max_seq_len,
            pad_token_id=self.pad_token_id
        )
        
        output_ids = tokenize_and_pad(
            self.tokenizer,
            completion,
            self.max_seq_len,
            pad_token_id=self.pad_token_id
        )
        
        return input_ids, output_ids


def test_training():
    """–¢–µ—Å—Ç –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
    print("=" * 70)
    print("üß™ –¢–ï–°–¢ 1: –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ")
    print("=" * 70)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Ç–µ—Å—Ç—É
    dim = 128
    depth = 2
    seq_len = 256
    batch_size = 2
    epochs = 2
    
    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ:")
    print(f"   - dim: {dim}")
    print(f"   - depth: {depth}")
    print(f"   - seq_len: {seq_len}")
    print(f"   - batch_size: {batch_size}")
    print(f"   - epochs: {epochs}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    print(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞...")
    tokenizer, vocab_size, pad_token_id = load_tokenizer("gpt2")
    print(f"   ‚úÖ Vocab size: {vocab_size}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
    print(f"\nüìö –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É...")
    dataset = SimpleCodeDataset(tokenizer, max_seq_len=seq_len, pad_token_id=pad_token_id)
    print(f"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç: {len(dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
    print(f"\nüèóÔ∏è  –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    network = MLPMixer1D(dim=dim, depth=depth, seq_len=seq_len)
    model = TinyRecursiveModel(
        dim=dim,
        num_tokens=vocab_size,
        network=network,
        num_refinement_blocks=3,
        num_latent_refinements=4,
        halt_loss_weight=1.0
    )
    
    # –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {total_params:,} (–Ω–∞–≤—á–∞—é—Ç—å—Å—è: {trainable_params:,})")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ trainer
    print(f"\nüéì –°—Ç–≤–æ—Ä–µ–Ω–Ω—è trainer...")
    trainer = Trainer(
        model=model,
        dataset=dataset,
        learning_rate=1e-3,
        batch_size=batch_size,
        epochs=epochs,
        max_recurrent_steps=6,
        halt_prob_thres=0.5,
        warmup_steps=10,
        cpu=True,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ CPU –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
        checkpoint_dir=None,  # –ë–µ–∑ checkpoint –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Ç–µ—Å—Ç—É
        gradient_accumulation_steps=1,
        dataloader_num_workers=0,
        mixed_precision=None
    )
    print(f"   ‚úÖ Trainer –≥–æ—Ç–æ–≤–∏–π")
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    print(f"\nüöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è...")
    print("-" * 70)
    trainer()
    print("-" * 70)
    print("‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n")
    
    return model, tokenizer, seq_len


def test_inference(model, tokenizer, seq_len):
    """–¢–µ—Å—Ç —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –º–æ–¥–µ–ª—ñ"""
    print("=" * 70)
    print("üß™ –¢–ï–°–¢ 2: –†–æ–±–æ—Ç–∞ –º–æ–¥–µ–ª—ñ (—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å)")
    print("=" * 70)
    
    # –¢–µ—Å—Ç–æ–≤—ñ –ø—Ä–∏–∫–ª–∞–¥–∏
    test_cases = [
        {
            "context": "def multiply(a, b):\n    pass",
            "query": "–†–µ–∞–ª—ñ–∑—É–π —Ñ—É–Ω–∫—Ü—ñ—é",
            "expected": "return a * b"
        },
        {
            "context": "items = [1, 2, 3, 4, 5]",
            "query": "–ó–Ω–∞–π–¥–∏ —Å—É–º—É",
            "expected": "sum"
        }
    ]
    
    print(f"\nüîÆ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å...\n")
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"üìù –¢–µ—Å—Ç {i}:")
        print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {test_case['context']}")
        print(f"   –ó–∞–ø–∏—Ç: {test_case['query']}")
        print(f"   –û—á—ñ–∫—É—î—Ç—å—Å—è: {test_case['expected']}")
        
        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥
        input_text = prepare_code_input(test_case['context'], test_case['query'])
        input_ids = tokenize_and_pad(
            tokenizer,
            input_text,
            seq_len,
            pad_token_id=tokenizer.pad_token_id if hasattr(tokenizer, 'pad_token_id') else 0
        ).unsqueeze(0)  # –î–æ–¥–∞—Ç–∏ batch dimension
        
        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
        model.eval()
        with torch.no_grad():
            try:
                pred_tokens, exit_steps = model.predict(
                    input_ids,
                    max_deep_refinement_steps=8,
                    halt_prob_thres=0.3
                )
                
                # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if hasattr(tokenizer, 'decode'):
                    # –ó–Ω–∞–π—Ç–∏ –ø–µ—Ä—à–∏–π non-padding token –¥–ª—è –≤–∏–≤–æ–¥—É
                    pred_tokens_clean = pred_tokens[0].cpu().numpy()
                    # –û–±—Ä—ñ–∑–∞—Ç–∏ –¥–æ –ø–µ—Ä—à–∏—Ö 50 —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è —á–∏—Ç–∞–Ω–Ω—è
                    output = tokenizer.decode(pred_tokens_clean[:min(50, len(pred_tokens_clean))], skip_special_tokens=True)
                else:
                    output = ''.join([tokenizer.inv_vocab.get(int(t), '?') for t in pred_tokens[0][:50]])
                
                print(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {output[:100]}...")
                print(f"   üìä –ö—Ä–æ–∫—ñ–≤ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: {exit_steps[0].item()}")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: {e}")
        
        print()
    
    print("‚úÖ –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n")


def test_model_structure():
    """–¢–µ—Å—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ"""
    print("=" * 70)
    print("üß™ –¢–ï–°–¢ 3: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ")
    print("=" * 70)
    
    dim = 64
    vocab_size = 256
    seq_len = 128
    
    network = MLPMixer1D(dim=dim, depth=2, seq_len=seq_len)
    model = TinyRecursiveModel(
        dim=dim,
        num_tokens=vocab_size,
        network=network,
        num_refinement_blocks=3,
        num_latent_refinements=4
    )
    
    print(f"\nüìê –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ:")
    print(f"   - –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å: {dim}")
    print(f"   - –°–ª–æ–≤–Ω–∏–∫: {vocab_size} —Ç–æ–∫–µ–Ω—ñ–≤")
    print(f"   - –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ: {seq_len}")
    print(f"   - –ë–ª–æ–∫–∏ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: 3")
    print(f"   - –ü—Ä–∏—Ö–æ–≤–∞–Ω—ñ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: 4")
    
    # –¢–µ—Å—Ç forward pass
    print(f"\nüîÑ –¢–µ—Å—Ç forward pass...")
    batch_size = 2
    seq = torch.randint(0, vocab_size, (batch_size, seq_len))
    outputs, latents = model.get_initial()
    outputs = outputs.unsqueeze(0).repeat(batch_size, seq_len, 1)
    latents = latents.unsqueeze(0).repeat(batch_size, seq_len, 1)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    try:
        loss, (main_loss, halt_loss), outputs, latents, pred, halt = model(seq, outputs, latents, labels)
        print(f"   ‚úÖ Forward pass —É—Å–ø—ñ—à–Ω–∏–π")
        print(f"   üìä Loss: {loss.mean().item():.4f}")
        print(f"   üìä Main loss: {main_loss.mean().item():.4f}")
        print(f"   üìä Halt loss: {halt_loss.mean().item():.4f}")
        print(f"   üìä Halt prob: {halt.mean().item():.4f}")
    except Exception as e:
        print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        return
    
    # –¢–µ—Å—Ç predict
    print(f"\nüîÆ –¢–µ—Å—Ç predict...")
    test_seq = torch.randint(0, vocab_size, (1, seq_len))
    try:
        with torch.no_grad():
            pred_tokens, exit_steps = model.predict(test_seq, max_deep_refinement_steps=6)
        print(f"   ‚úÖ Predict —É—Å–ø—ñ—à–Ω–∏–π")
        print(f"   üìä –í–∏—Ö—ñ–¥–Ω—ñ —Ç–æ–∫–µ–Ω–∏: {pred_tokens.shape}")
        print(f"   üìä –ö—Ä–æ–∫–∏ –≤–∏—Ö–æ–¥—É: {exit_steps}")
    except Exception as e:
        print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
    
    print("\n‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–∞!\n")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    print("\n" + "=" * 70)
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–Ü–Ø TRM –ú–û–î–ï–õ–Ü")
    print("=" * 70)
    print("\n–¶–µ–π —Å–∫—Ä–∏–ø—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î:")
    print("1. –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –ø—Ä–æ—Å—Ç–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö")
    print("2. –†–æ–±–æ—Ç—É –º–æ–¥–µ–ª—ñ –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è (—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å)")
    print("3. –°—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞ –±–∞–∑–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ")
    print()
    
    try:
        # –¢–µ—Å—Ç 1: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ
        test_model_structure()
        
        # –¢–µ—Å—Ç 2: –ù–∞–≤—á–∞–Ω–Ω—è
        model, tokenizer, seq_len = test_training()
        
        # –¢–µ—Å—Ç 3: –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å
        test_inference(model, tokenizer, seq_len)
        
        print("=" * 70)
        print("‚úÖ –í–°–Ü –¢–ï–°–¢–ò –ü–†–û–ô–î–ï–ù–û –£–°–ü–Ü–®–ù–û!")
        print("=" * 70)
        
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

