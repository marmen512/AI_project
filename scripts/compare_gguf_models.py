"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö GGUF –º–æ–¥–µ–ª–µ–π —Ç–∞ –Ω–∞–≤—á–µ–Ω–∏—Ö TRM –º–æ–¥–µ–ª–µ–π
–ü–æ—Ä—ñ–≤–Ω—é—î —à–≤–∏–¥–∫—ñ—Å—Ç—å, —è–∫—ñ—Å—Ç—å —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
"""
import sys
import argparse
import json
import time
import torch
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
from datetime import datetime

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("‚ö†Ô∏è  llama-cpp-python –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ (GGUF –º–æ–¥–µ–ª—ñ –Ω–µ –±—É–¥—É—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ñ)")
    print("   –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install llama-cpp-python")

from config.model_loader import GGUFModelManager
from inference.model_inference import load_trained_model, find_trained_models, TRMInference


class UnifiedModelBenchmark:
    """–£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è GGUF —Ç–∞ TRM –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, test_cases: List[Dict[str, str]]):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –±–µ–Ω—á–º–∞—Ä–∫—É
        
        Args:
            test_cases: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤ –∑ 'prompt' —Ç–∞ 'expected_keywords'
        """
        self.test_cases = test_cases
        self.loaded_models: Dict[str, Any] = {}  # –ö–µ—à –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
    
    def load_gguf_model(self, model_path: str, n_ctx: int = 2048, n_threads: int = None, n_gpu_layers: int = 0) -> Llama:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ GGUF –º–æ–¥–µ–ª—å"""
        if not LLAMA_CPP_AVAILABLE:
            raise RuntimeError("llama-cpp-python –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        try:
            model = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_threads=n_threads,
                n_gpu_layers=n_gpu_layers,
                verbose=False
            )
            return model
        except Exception as e:
            raise RuntimeError(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GGUF –º–æ–¥–µ–ª—ñ {model_path}: {e}")
    
    def load_trm_model(self, model_path: str, config_path: Optional[str] = None, device: str = 'cpu') -> TRMInference:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ TRM –º–æ–¥–µ–ª—å"""
        try:
            inference = load_trained_model(
                model_path=model_path,
                config_path=config_path,
                device=device
            )
            return inference
        except Exception as e:
            raise RuntimeError(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è TRM –º–æ–¥–µ–ª—ñ {model_path}: {e}")
    
    def is_gguf_model(self, model_path: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —Ü–µ GGUF –º–æ–¥–µ–ª—å"""
        return Path(model_path).suffix.lower() == '.gguf'
    
    def is_trm_model(self, model_path: str) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —Ü–µ TRM –º–æ–¥–µ–ª—å"""
        return Path(model_path).suffix.lower() in ['.pt', '.pth']
    
    def generate_response_gguf(self, model: Llama, prompt: str, max_tokens: int = 256) -> Dict:
        """
        –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ GGUF –º–æ–¥–µ–ª—ñ —Ç–∞ –≤–∏–º—ñ—Ä—è—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é —Ç–∞ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        start_time = time.time()
        
        try:
            response = model(
                prompt,
                max_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                stop=["<|end|>", "<|endoftext|>", "\n\n\n"],
                echo=False
            )
            
            generation_time = time.time() - start_time
            
            # –û—Ç—Ä–∏–º–∞—Ç–∏ —Ç–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            if isinstance(response, dict):
                text = response.get('choices', [{}])[0].get('text', '')
                tokens_generated = response.get('usage', {}).get('completion_tokens', 0)
            else:
                text = str(response)
                tokens_generated = len(text.split())
            
            tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
            
            return {
                'text': text,
                'generation_time': generation_time,
                'tokens_generated': tokens_generated,
                'tokens_per_second': tokens_per_second,
                'success': True
            }
        except Exception as e:
            return {
                'text': '',
                'error': str(e),
                'generation_time': time.time() - start_time,
                'success': False
            }
    
    def generate_response_trm(self, inference: TRMInference, prompt: str, max_tokens: int = 256) -> Dict:
        """
        –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ TRM –º–æ–¥–µ–ª—ñ —Ç–∞ –≤–∏–º—ñ—Ä—è—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
        
        Args:
            inference: TRMInference –æ–±'—î–∫—Ç
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø–∏—Ç—É (–±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —è–∫ query, context –±—É–¥–µ –ø–æ—Ä–æ–∂–Ω—ñ–º)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è TRM)
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é —Ç–∞ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        start_time = time.time()
        
        try:
            # TRM –º–æ–¥–µ–ª—ñ –æ—á—ñ–∫—É—é—Ç—å context —Ç–∞ query
            # –Ø–∫—â–æ prompt –º—ñ—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Å–ø—Ä–æ–±—É—î–º–æ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏
            context = ""
            query = prompt
            
            # –ü—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–¥—ñ–ª—é–≤–∞—á –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ç–∞ –∑–∞–ø–∏—Ç—É
            if "|CONTEXT|" in prompt:
                parts = prompt.split("|CONTEXT|", 1)
                context = parts[0].strip()
                query = parts[1].strip() if len(parts) > 1 else prompt
            elif "\n\nQuery:" in prompt or "\n\n–ó–∞–ø–∏—Ç:" in prompt:
                # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–Ω–∞–π—Ç–∏ —Ä–æ–∑–¥—ñ–ª—é–≤–∞—á
                for sep in ["\n\nQuery:", "\n\n–ó–∞–ø–∏—Ç:", "\n\n---\n\n"]:
                    if sep in prompt:
                        parts = prompt.split(sep, 1)
                        context = parts[0].strip()
                        query = parts[1].strip()
                        break
            
            result = inference.predict(
                context=context,
                query=query,
                max_deep_refinement_steps=12,
                halt_prob_thres=0.5
            )
            
            generation_time = time.time() - start_time
            text = result.get('completion', '')
            
            # –û—Ü—ñ–Ω–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ (–ø—Ä–∏–±–ª–∏–∑–Ω–æ)
            tokens_generated = len(text.split()) if text else 0
            tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
            
            return {
                'text': text,
                'generation_time': generation_time,
                'tokens_generated': tokens_generated,
                'tokens_per_second': tokens_per_second,
                'exit_steps': result.get('exit_steps', 0),
                'success': True
            }
        except Exception as e:
            return {
                'text': '',
                'error': str(e),
                'generation_time': time.time() - start_time,
                'success': False
            }
    
    def evaluate_quality(self, response_text: str, expected_keywords: List[str]) -> Dict:
        """
        –û—Ü—ñ–Ω–∏—Ç–∏ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        
        Args:
            response_text: –¢–µ–∫—Å—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
            expected_keywords: –û—á—ñ–∫—É–≤–∞–Ω—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —è–∫–æ—Å—Ç—ñ
        """
        if not response_text:
            return {
                'keyword_match': 0.0,
                'length': 0,
                'has_structure': False,
                'quality_score': 0.0
            }
        
        response_lower = response_text.lower()
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        matched_keywords = sum(1 for kw in expected_keywords if kw.lower() in response_lower)
        keyword_match = matched_keywords / len(expected_keywords) if expected_keywords else 0.0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
        has_structure = any(marker in response_text for marker in ['\n', '. ', '! ', '? '])
        
        # –ë–∞–∑–æ–≤–∞ –æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ
        quality_score = (
            keyword_match * 0.5 +  # 50% –∑–∞ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞
            (1.0 if len(response_text) > 20 else len(response_text) / 20) * 0.3 +  # 30% –∑–∞ –¥–æ–≤–∂–∏–Ω—É
            (1.0 if has_structure else 0.0) * 0.2  # 20% –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        )
        
        return {
            'keyword_match': keyword_match,
            'matched_keywords': matched_keywords,
            'total_keywords': len(expected_keywords),
            'length': len(response_text),
            'has_structure': has_structure,
            'quality_score': quality_score
        }
    
    def benchmark_model(self, model_path: str, model_name: str = None, model_type: str = 'auto', config_path: Optional[str] = None, device: str = 'cpu', **model_kwargs) -> Dict:
        """
        –ó–∞–±–µ–Ω—á–º–∞—Ä–∫—É–≤–∞—Ç–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å (GGUF –∞–±–æ TRM)
        
        Args:
            model_path: –®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ
            model_name: –Ü–º'—è –º–æ–¥–µ–ª—ñ (–¥–ª—è –∑–≤—ñ—Ç—É)
            model_type: –¢–∏–ø –º–æ–¥–µ–ª—ñ ('gguf', 'trm', –∞–±–æ 'auto' –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è)
            config_path: –®–ª—è—Ö –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó TRM –º–æ–¥–µ–ª—ñ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            device: –ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è TRM –º–æ–¥–µ–ª—ñ ('cpu' –∞–±–æ 'cuda')
            **model_kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è GGUF –º–æ–¥–µ–ª—ñ (n_ctx, n_threads, n_gpu_layers)
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫—É
        """
        if model_name is None:
            model_name = Path(model_path).stem
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —Ç–∏–ø –º–æ–¥–µ–ª—ñ
        if model_type == 'auto':
            if self.is_gguf_model(model_path):
                model_type = 'gguf'
            elif self.is_trm_model(model_path):
                model_type = 'trm'
            else:
                return {
                    'model_name': model_name,
                    'model_path': model_path,
                    'error': f'–ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –º–æ–¥–µ–ª—ñ: {model_path}',
                    'success': False
                }
        
        print(f"\n{'='*70}")
        print(f"üìä –ë–ï–ù–ß–ú–ê–†–ö: {model_name} ({model_type.upper()})")
        print(f"{'='*70}")
        print(f"üìÅ –®–ª—è—Ö: {model_path}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
        print("‚è≥ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        try:
            if model_type == 'gguf':
                if not LLAMA_CPP_AVAILABLE:
                    raise RuntimeError("llama-cpp-python –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                model = self.load_gguf_model(model_path, **model_kwargs)
                print("‚úÖ GGUF –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
            elif model_type == 'trm':
                model = self.load_trm_model(model_path, config_path, device)
                print("‚úÖ TRM –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
            else:
                raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –º–æ–¥–µ–ª—ñ: {model_type}")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            return {
                'model_name': model_name,
                'model_path': model_path,
                'model_type': model_type,
                'error': str(e),
                'success': False
            }
        
        # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ —Ç–µ—Å—Ç–∏
        results = []
        total_time = 0
        total_tokens = 0
        
        print(f"\nüß™ –ó–∞–ø—É—Å–∫ {len(self.test_cases)} —Ç–µ—Å—Ç—ñ–≤...")
        
        for i, test_case in enumerate(self.test_cases, 1):
            prompt = test_case.get('prompt', '')
            expected_keywords = test_case.get('expected_keywords', [])
            test_name = test_case.get('name', f'Test {i}')
            
            print(f"\n  –¢–µ—Å—Ç {i}/{len(self.test_cases)}: {test_name}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —Ç–∏–ø—É –º–æ–¥–µ–ª—ñ
            if model_type == 'gguf':
                gen_result = self.generate_response_gguf(model, prompt, max_tokens=test_case.get('max_tokens', 256))
            else:  # TRM
                gen_result = self.generate_response_trm(model, prompt, max_tokens=test_case.get('max_tokens', 256))
            
            if not gen_result['success']:
                print(f"    ‚ùå –ü–æ–º–∏–ª–∫–∞: {gen_result.get('error', 'Unknown')}")
                results.append({
                    'test_name': test_name,
                    'success': False,
                    'error': gen_result.get('error')
                })
                continue
            
            # –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ
            quality = self.evaluate_quality(gen_result['text'], expected_keywords)
            
            total_time += gen_result['generation_time']
            total_tokens += gen_result['tokens_generated']
            
            print(f"    ‚è±Ô∏è  –ß–∞—Å: {gen_result['generation_time']:.2f}—Å")
            print(f"    üìù –¢–æ–∫–µ–Ω—ñ–≤: {gen_result['tokens_generated']}")
            print(f"    ‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å: {gen_result['tokens_per_second']:.1f} —Ç–æ–∫–µ–Ω—ñ–≤/—Å")
            print(f"    üìä –Ø–∫—ñ—Å—Ç—å: {quality['quality_score']:.2%}")
            print(f"    üîë –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {quality['matched_keywords']}/{quality['total_keywords']}")
            if 'exit_steps' in gen_result:
                print(f"    üîÑ –ö—Ä–æ–∫—ñ–≤ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: {gen_result['exit_steps']}")
            
            result_item = {
                'test_name': test_name,
                'success': True,
                'generation_time': gen_result['generation_time'],
                'tokens_generated': gen_result['tokens_generated'],
                'tokens_per_second': gen_result['tokens_per_second'],
                'response_length': len(gen_result['text']),
                'quality': quality
            }
            if 'exit_steps' in gen_result:
                result_item['exit_steps'] = gen_result['exit_steps']
            
            results.append(result_item)
        
        # –ü—ñ–¥—Å—É–º–æ–∫
        successful_tests = sum(1 for r in results if r.get('success', False))
        avg_time = total_time / successful_tests if successful_tests > 0 else 0
        avg_tokens_per_sec = sum(r.get('tokens_per_second', 0) for r in results if r.get('success')) / successful_tests if successful_tests > 0 else 0
        avg_quality = sum(r.get('quality', {}).get('quality_score', 0) for r in results if r.get('success')) / successful_tests if successful_tests > 0 else 0
        
        print(f"\nüìà –ü–Ü–î–°–£–ú–û–ö:")
        print(f"   –£—Å–ø—ñ—à–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤: {successful_tests}/{len(self.test_cases)}")
        print(f"   –°–µ—Ä–µ–¥–Ω—ñ–π —á–∞—Å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {avg_time:.2f}—Å")
        print(f"   –°–µ—Ä–µ–¥–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å: {avg_tokens_per_sec:.1f} —Ç–æ–∫–µ–Ω—ñ–≤/—Å")
        print(f"   –°–µ—Ä–µ–¥–Ω—è —è–∫—ñ—Å—Ç—å: {avg_quality:.2%}")
        
        return {
            'model_name': model_name,
            'model_path': model_path,
            'model_type': model_type,
            'success': True,
            'total_tests': len(self.test_cases),
            'successful_tests': successful_tests,
            'total_time': total_time,
            'total_tokens': total_tokens,
            'avg_generation_time': avg_time,
            'avg_tokens_per_second': avg_tokens_per_sec,
            'avg_quality_score': avg_quality,
            'test_results': results
        }
    
    def compare_models(self, model_paths: List[str], model_names: List[str] = None, model_types: List[str] = None, config_paths: List[Optional[str]] = None, device: str = 'cpu', **model_kwargs) -> Dict:
        """
        –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∫—ñ–ª—å–∫–∞ –º–æ–¥–µ–ª–µ–π (GGUF —Ç–∞/–∞–±–æ TRM)
        
        Args:
            model_paths: –°–ø–∏—Å–æ–∫ —à–ª—è—Ö—ñ–≤ –¥–æ –º–æ–¥–µ–ª–µ–π
            model_names: –°–ø–∏—Å–æ–∫ —ñ–º–µ–Ω –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            model_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø—ñ–≤ –º–æ–¥–µ–ª–µ–π ('gguf', 'trm', –∞–±–æ 'auto')
            config_paths: –°–ø–∏—Å–æ–∫ —à–ª—è—Ö—ñ–≤ –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π TRM –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            device: –ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è TRM –º–æ–¥–µ–ª–µ–π ('cpu' –∞–±–æ 'cuda')
            **model_kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è GGUF –º–æ–¥–µ–ª–µ–π (n_ctx, n_threads, n_gpu_layers)
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        """
        if model_names is None:
            model_names = [Path(p).stem for p in model_paths]
        
        if model_types is None:
            model_types = ['auto'] * len(model_paths)
        elif len(model_types) < len(model_paths):
            model_types.extend(['auto'] * (len(model_paths) - len(model_types)))
        
        if config_paths is None:
            config_paths = [None] * len(model_paths)
        elif len(config_paths) < len(model_paths):
            config_paths.extend([None] * (len(model_paths) - len(config_paths)))
        
        print("="*70)
        print("üî¨ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô (GGUF —Ç–∞ TRM)")
        print("="*70)
        print(f"\nüìã –¢–µ—Å—Ç–æ–≤–∏—Ö –≤–∏–ø–∞–¥–∫—ñ–≤: {len(self.test_cases)}")
        print(f"üì¶ –ú–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è: {len(model_paths)}")
        
        # –ü–æ–∫–∞–∑–∞—Ç–∏ —Ç–∏–ø–∏ –º–æ–¥–µ–ª–µ–π
        gguf_count = sum(1 for mt in model_types if mt == 'gguf' or (mt == 'auto' and any(self.is_gguf_model(p) for p in model_paths)))
        trm_count = sum(1 for mt in model_types if mt == 'trm' or (mt == 'auto' and any(self.is_trm_model(p) for p in model_paths)))
        if gguf_count > 0:
            print(f"   - GGUF –º–æ–¥–µ–ª–µ–π: {gguf_count}")
        if trm_count > 0:
            print(f"   - TRM –º–æ–¥–µ–ª–µ–π: {trm_count}")
        
        benchmark_results = []
        
        for i, (model_path, model_name) in enumerate(zip(model_paths, model_names)):
            model_type = model_types[i] if i < len(model_types) else 'auto'
            config_path = config_paths[i] if i < len(config_paths) else None
            result = self.benchmark_model(
                model_path, 
                model_name, 
                model_type=model_type,
                config_path=config_path,
                device=device,
                **model_kwargs
            )
            benchmark_results.append(result)
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –∑–≤—ñ—Ç –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        comparison = self.create_comparison_report(benchmark_results)
        
        return comparison
    
    def create_comparison_report(self, benchmark_results: List[Dict]) -> Dict:
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∑–≤—ñ—Ç –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è"""
        successful_results = [r for r in benchmark_results if r.get('success', False)]
        
        if not successful_results:
            print("\n‚ùå –ñ–æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–æ–π—à–ª–∞ —Ç–µ—Å—Ç–∏")
            return {'error': 'No successful benchmarks'}
        
        print("\n" + "="*70)
        print("üìä –ó–í–Ü–¢ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø")
        print("="*70)
        
        # –¢–∞–±–ª–∏—Ü—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        print("\nüìà –ü–û–†–Ü–í–ù–Ø–õ–¨–ù–ê –¢–ê–ë–õ–ò–¶–Ø:")
        print("-" * 70)
        print(f"{'–ú–æ–¥–µ–ª—å':<30} {'–®–≤–∏–¥–∫—ñ—Å—Ç—å':<15} {'–Ø–∫—ñ—Å—Ç—å':<10} {'–£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å':<10}")
        print("-" * 70)
        
        for result in successful_results:
            name = result['model_name'][:28]
            speed = f"{result['avg_tokens_per_second']:.1f} —Ç–æ–∫/—Å"
            quality = f"{result['avg_quality_score']:.1%}"
            success_rate = f"{result['successful_tests']}/{result['total_tests']}"
            print(f"{name:<30} {speed:<15} {quality:<10} {success_rate:<10}")
        
        # –ù–∞–π–∫—Ä–∞—â—ñ –º–æ–¥–µ–ª—ñ
        print("\nüèÜ –ù–ê–ô–ö–†–ê–©–Ü –ú–û–î–ï–õ–Ü:")
        
        if successful_results:
            fastest = max(successful_results, key=lambda x: x['avg_tokens_per_second'])
            best_quality = max(successful_results, key=lambda x: x['avg_quality_score'])
            most_reliable = max(successful_results, key=lambda x: x['successful_tests'] / x['total_tests'])
            
            print(f"   ‚ö° –ù–∞–π—à–≤–∏–¥—à–∞: {fastest['model_name']} ({fastest['avg_tokens_per_second']:.1f} —Ç–æ–∫–µ–Ω—ñ–≤/—Å)")
            print(f"   üéØ –ù–∞–π–∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å: {best_quality['model_name']} ({best_quality['avg_quality_score']:.1%})")
            print(f"   ‚úÖ –ù–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∞: {most_reliable['model_name']} ({most_reliable['successful_tests']}/{most_reliable['total_tests']} —Ç–µ—Å—Ç—ñ–≤)")
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–≤—ñ—Ç
        report = {
            'timestamp': datetime.now().isoformat(),
            'test_cases_count': len(self.test_cases),
            'models_compared': len(benchmark_results),
            'successful_benchmarks': len(successful_results),
            'benchmark_results': benchmark_results,
            'comparison': {
                'fastest': fastest['model_name'] if successful_results else None,
                'best_quality': best_quality['model_name'] if successful_results else None,
                'most_reliable': most_reliable['model_name'] if successful_results else None
            }
        }
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É temp —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
        temp_dir = project_root / "temp"
        temp_dir.mkdir(exist_ok=True, parents=True)
        
        report_path = temp_dir / f"gguf_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
        
        return report


def get_default_test_cases() -> List[Dict[str, str]]:
    """–û—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ —Ç–µ—Å—Ç–æ–≤—ñ –≤–∏–ø–∞–¥–∫–∏"""
    return [
        {
            'name': '–ü—Ä–æ—Å—Ç–∏–π –∑–∞–ø–∏—Ç',
            'prompt': 'What is Python?',
            'expected_keywords': ['python', 'programming', 'language'],
            'max_tokens': 128
        },
        {
            'name': '–ö–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è',
            'prompt': 'Write a Python function to calculate factorial:',
            'expected_keywords': ['def', 'factorial', 'return', 'function'],
            'max_tokens': 256
        },
        {
            'name': '–ü–æ—è—Å–Ω–µ–Ω–Ω—è –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó',
            'prompt': 'Explain what is machine learning in simple terms:',
            'expected_keywords': ['machine', 'learning', 'data', 'algorithm'],
            'max_tokens': 256
        },
        {
            'name': '–ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –∑–∞–≤–¥–∞–Ω–Ω—è',
            'prompt': 'Solve: What is 15 * 23? Show your work.',
            'expected_keywords': ['15', '23', '345', 'multiply'],
            'max_tokens': 128
        },
        {
            'name': '–°–∫–ª–∞–¥–Ω–∏–π –∑–∞–ø–∏—Ç',
            'prompt': 'How does a neural network learn? Explain the backpropagation process.',
            'expected_keywords': ['neural', 'network', 'backpropagation', 'gradient', 'weights'],
            'max_tokens': 512
        }
    ]


def main():
    parser = argparse.ArgumentParser(
        description="–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è GGUF —Ç–∞ TRM –º–æ–¥–µ–ª–µ–π",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
  # –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ GGUF –º–æ–¥–µ–ª—ñ
  python scripts/compare_gguf_models.py --all
  
  # –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ GGUF –º–æ–¥–µ–ª—ñ
  python scripts/compare_gguf_models.py \\
      --models models/gguf/phi-3.5-mini-instruct-q4_k_m.gguf \\
                models/gguf/tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf
  
  # –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ TRM –º–æ–¥–µ–ª—å –∑ GGUF –º–æ–¥–µ–ª—è–º–∏
  python scripts/compare_gguf_models.py \\
      --models models/trained/my_model.pt \\
                models/gguf/phi-3.5-mini-instruct-q4_k_m.gguf \\
      --trm-config models/trained/my_model_config.json
  
  # –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ —Ç—ñ–ª—å–∫–∏ TRM –º–æ–¥–µ–ª—ñ
  python scripts/compare_gguf_models.py \\
      --trm-models models/trained/model1.pt models/trained/model2.pt
  
  # –ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –∑ –∫–∞—Å—Ç–æ–º–Ω–∏–º–∏ —Ç–µ—Å—Ç–∞–º–∏
  python scripts/compare_gguf_models.py --all --test-file my_tests.json
        """
    )
    
    parser.add_argument(
        '--models',
        nargs='+',
        type=str,
        help='–®–ª—è—Ö–∏ –¥–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è (GGUF –∞–±–æ TRM, –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)'
    )
    
    parser.add_argument(
        '--trm-models',
        nargs='+',
        type=str,
        help='–®–ª—è—Ö–∏ –¥–æ TRM –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è'
    )
    
    parser.add_argument(
        '--trm-config',
        type=str,
        help='–®–ª—è—Ö –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó TRM –º–æ–¥–µ–ª—ñ (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –≤—Å—ñ—Ö TRM –º–æ–¥–µ–ª–µ–π)'
    )
    
    parser.add_argument(
        '--trm-configs',
        nargs='+',
        type=str,
        help='–®–ª—è—Ö–∏ –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π TRM –º–æ–¥–µ–ª–µ–π (–ø–æ –æ–¥–Ω—ñ–π –Ω–∞ –º–æ–¥–µ–ª—å)'
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='–ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ GGUF –º–æ–¥–µ–ª—ñ'
    )
    
    parser.add_argument(
        '--all-trm',
        action='store_true',
        help='–ü–æ—Ä—ñ–≤–Ω—è—Ç–∏ –≤—Å—ñ –∑–Ω–∞–π–¥–µ–Ω—ñ TRM –º–æ–¥–µ–ª—ñ'
    )
    
    parser.add_argument(
        '--test-file',
        type=str,
        help='JSON —Ñ–∞–π–ª –∑ —Ç–µ—Å—Ç–æ–≤–∏–º–∏ –≤–∏–ø–∞–¥–∫–∞–º–∏'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        default='auto',
        choices=['auto', 'cpu', 'cuda'],
        help='–ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è TRM –º–æ–¥–µ–ª–µ–π (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: auto)'
    )
    
    parser.add_argument(
        '--n-ctx',
        type=int,
        default=2048,
        help='–†–æ–∑–º—ñ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è GGUF –º–æ–¥–µ–ª–µ–π (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 2048)'
    )
    
    parser.add_argument(
        '--n-threads',
        type=int,
        default=None,
        help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è GGUF –º–æ–¥–µ–ª–µ–π (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)'
    )
    
    parser.add_argument(
        '--n-gpu-layers',
        type=int,
        default=0,
        help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤ –Ω–∞ GPU –¥–ª—è GGUF –º–æ–¥–µ–ª–µ–π (0 = —Ç—ñ–ª—å–∫–∏ CPU)'
    )
    
    args = parser.parse_args()
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è TRM
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    model_paths = []
    model_names = []
    model_types = []
    config_paths = []
    
    if args.all:
        # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ GGUF –º–æ–¥–µ–ª—ñ
        if not LLAMA_CPP_AVAILABLE:
            print("‚ùå llama-cpp-python –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ, –Ω–µ –º–æ–∂—É –ø–æ—Ä—ñ–≤–Ω—è—Ç–∏ GGUF –º–æ–¥–µ–ª—ñ")
            return
        
        manager = GGUFModelManager()
        models = manager.get_models()
        
        if not models:
            print("‚ùå GGUF –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            print(f"   –î–æ–¥–∞–π—Ç–µ .gguf —Ñ–∞–π–ª–∏ –≤ –ø–∞–ø–∫—É: {manager.models_dir}")
            return
        
        model_paths = [m['path'] for m in models]
        model_names = [m['name'] for m in models]
        model_types = ['gguf'] * len(models)
        config_paths = [None] * len(models)
        
        print(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ {len(models)} GGUF –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è:")
        for m in models:
            print(f"   - {m['name']}")
    
    elif args.all_trm:
        # –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ TRM –º–æ–¥–µ–ª—ñ
        models = find_trained_models()
        
        if not models:
            print("‚ùå TRM –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            print(f"   –î–æ–¥–∞–π—Ç–µ .pt —Ñ–∞–π–ª–∏ –≤ –ø–∞–ø–∫—É models/trained/")
            return
        
        model_paths = [m['path'] for m in models]
        model_names = [m['name'] for m in models]
        model_types = ['trm'] * len(models)
        config_paths = [None] * len(models)
        
        print(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ {len(models)} TRM –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è:")
        for m in models:
            print(f"   - {m['name']}")
    
    elif args.trm_models:
        # –¢—ñ–ª—å–∫–∏ TRM –º–æ–¥–µ–ª—ñ
        model_paths = args.trm_models
        model_names = [Path(p).stem for p in model_paths]
        model_types = ['trm'] * len(model_paths)
        
        # –û–±—Ä–æ–±–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        if args.trm_config:
            config_paths = [args.trm_config] * len(model_paths)
        elif args.trm_configs:
            config_paths = list(args.trm_configs)
            if len(config_paths) < len(model_paths):
                config_paths.extend([None] * (len(model_paths) - len(config_paths)))
        else:
            config_paths = [None] * len(model_paths)
    
    elif args.models:
        # –ó–º—ñ—à–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        model_paths = args.models
        model_names = []
        model_types = []
        
        for path in model_paths:
            path_obj = Path(path)
            model_names.append(path_obj.stem)
            
            # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —Ç–∏–ø –º–æ–¥–µ–ª—ñ
            if path_obj.suffix.lower() == '.gguf':
                model_types.append('gguf')
            elif path_obj.suffix.lower() in ['.pt', '.pth']:
                model_types.append('trm')
            else:
                model_types.append('auto')
        
        # –û–±—Ä–æ–±–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è TRM –º–æ–¥–µ–ª–µ–π
        config_paths = []
        for i, model_type in enumerate(model_types):
            if model_type == 'trm':
                if args.trm_config:
                    config_paths.append(args.trm_config)
                elif args.trm_configs and i < len(args.trm_configs):
                    config_paths.append(args.trm_configs[i])
                else:
                    config_paths.append(None)
            else:
                config_paths.append(None)
    
    else:
        print("‚ùå –í–∫–∞–∂—ñ—Ç—å --models, --trm-models, --all –∞–±–æ --all-trm")
        parser.print_help()
        return
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ—Å—Ç–æ–≤—ñ –≤–∏–ø–∞–¥–∫–∏
    if args.test_file and Path(args.test_file).exists():
        with open(args.test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
            if isinstance(test_data, list):
                test_cases = test_data
            elif isinstance(test_data, dict) and 'test_cases' in test_data:
                test_cases = test_data['test_cases']
            else:
                test_cases = get_default_test_cases()
    else:
        test_cases = get_default_test_cases()
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫
    benchmark = UnifiedModelBenchmark(test_cases)
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    comparison = benchmark.compare_models(
        model_paths,
        model_names,
        model_types=model_types,
        config_paths=config_paths,
        device=device,
        n_ctx=args.n_ctx,
        n_threads=args.n_threads,
        n_gpu_layers=args.n_gpu_layers
    )
    
    print("\n" + "="*70)
    print("‚úÖ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û")
    print("="*70)


if __name__ == "__main__":
    main()

