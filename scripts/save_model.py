"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ, —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤ —Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç—É –≤ –æ–∫—Ä–µ–º—É –ø–∞–ø–∫—É
–¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è
"""
import sys
import argparse
import shutil
import json
from pathlib import Path
from datetime import datetime
from typing import Optional

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch


def get_model_name_from_dataset(dataset_path: Path) -> str:
    """–í–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ –∑ –Ω–∞–∑–≤–∏ –¥–∞—Ç–∞—Å–µ—Ç—É"""
    return dataset_path.stem  # –ë–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è


def cleanup_training_files(project_root: Path, keep_model: Optional[Path] = None):
    """
    –û—á–∏—Å—Ç–∏—Ç–∏ —Ñ–∞–π–ª–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
    
    Args:
        project_root: –ö–æ—Ä—ñ–Ω—å –ø—Ä–æ–µ–∫—Ç—É
        keep_model: –ú–æ–¥–µ–ª—å, —è–∫—É –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –≤–∏–¥–∞–ª—è—Ç–∏ (—è–∫—â–æ –≤–æ–Ω–∞ –≤ models/trained/)
    """
    cleaned = []
    
    # –û—á–∏—Å—Ç–∏—Ç–∏ checkpoints/
    checkpoints_dir = project_root / "checkpoints"
    if checkpoints_dir.exists():
        try:
            shutil.rmtree(checkpoints_dir)
            checkpoints_dir.mkdir(exist_ok=True)
            cleaned.append("checkpoints/")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è checkpoints/: {e}")
    
    # –û—á–∏—Å—Ç–∏—Ç–∏ logs/
    logs_dir = project_root / "logs"
    if logs_dir.exists():
        try:
            # –í–∏–¥–∞–ª–∏—Ç–∏ –≤—Å—ñ .log —Ç–∞ .json —Ñ–∞–π–ª–∏, –∞–ª–µ –∑–±–µ—Ä–µ–≥—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            for log_file in logs_dir.glob("*.log"):
                log_file.unlink()
            for json_file in logs_dir.glob("*.json"):
                json_file.unlink()
            # –í–∏–¥–∞–ª–∏—Ç–∏ —Å–∏–º–≤–æ–ª—ñ—á–Ω–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —è–∫—â–æ –≤–æ–Ω–æ —î
            latest_link = logs_dir / "training_latest.log"
            if latest_link.exists() and latest_link.is_symlink():
                latest_link.unlink()
            cleaned.append("logs/")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è logs/: {e}")
    
    # –û—á–∏—Å—Ç–∏—Ç–∏ temp/
    temp_dir = project_root / "temp"
    if temp_dir.exists():
        try:
            shutil.rmtree(temp_dir)
            temp_dir.mkdir(exist_ok=True)
            cleaned.append("temp/")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è temp/: {e}")
    
    # –û—á–∏—Å—Ç–∏—Ç–∏ models/trained/ (–∫—Ä—ñ–º keep_model)
    models_trained_dir = project_root / "models" / "trained"
    if models_trained_dir.exists():
        try:
            for model_file in models_trained_dir.glob("*.pt"):
                if keep_model and model_file.resolve() == keep_model.resolve():
                    continue  # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—É –º–æ–¥–µ–ª—å
                model_file.unlink()
            # –í–∏–¥–∞–ª–∏—Ç–∏ config —Ñ–∞–π–ª–∏
            for config_file in models_trained_dir.glob("*_config.json"):
                config_file.unlink()
            cleaned.append("models/trained/")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è models/trained/: {e}")
    
    if cleaned:
        print(f"\n‚úÖ –û—á–∏—â–µ–Ω–æ: {', '.join(cleaned)}")
    else:
        print("\n‚ö†Ô∏è  –ù–µ–º–∞—î —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—è")


def find_latest_model(models_dir: Path = None) -> Optional[Path]:
    """–ó–Ω–∞–π—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—é –Ω–∞–≤—á–µ–Ω—É –º–æ–¥–µ–ª—å"""
    if models_dir is None:
        models_dir = project_root / "models" / "trained"
    
    if not models_dir.exists():
        return None
    
    # –®—É–∫–∞—Ç–∏ .pt —Ñ–∞–π–ª–∏
    model_files = list(models_dir.glob("*.pt"))
    if not model_files:
        return None
    
    # –ü–æ–≤–µ—Ä–Ω—É—Ç–∏ –Ω–∞–π–Ω–æ–≤—ñ—à–∏–π
    return max(model_files, key=lambda p: p.stat().st_mtime)


def find_latest_checkpoint(checkpoints_dir: Path = None) -> Optional[Path]:
    """–ó–Ω–∞–π—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π checkpoint"""
    if checkpoints_dir is None:
        checkpoints_dir = project_root / "checkpoints"
    
    if not checkpoints_dir.exists():
        return None
    
    checkpoint_file = checkpoints_dir / "checkpoint_latest.pt"
    if checkpoint_file.exists():
        return checkpoint_file
    
    # –®—É–∫–∞—Ç–∏ —ñ–Ω—à—ñ checkpoint'–∏
    checkpoint_files = list(checkpoints_dir.glob("checkpoint_*.pt"))
    if not checkpoint_files:
        return None
    
    return max(checkpoint_files, key=lambda p: p.stat().st_mtime)


def load_model_config(model_path: Path) -> Optional[dict]:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –º–æ–¥–µ–ª—ñ"""
    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–Ω–∞–π—Ç–∏ config —Ñ–∞–π–ª
    config_path = model_path.parent / f"{model_path.stem}_config.json"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∑ checkpoint (—è–∫—â–æ –≤—ñ–Ω –≤ —Ç—ñ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —â–æ —ñ –º–æ–¥–µ–ª—å)
    # –ê–±–æ –∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ checkpoints/
    checkpoint_dirs = [
        model_path.parent.parent / "checkpoints",  # –Ø–∫—â–æ –º–æ–¥–µ–ª—å –≤ models/trained/
        project_root / "checkpoints"  # –ì–ª–æ–±–∞–ª—å–Ω–∏–π checkpoints/
    ]
    
    for checkpoint_dir in checkpoint_dirs:
        checkpoint = find_latest_checkpoint(checkpoint_dir)
        if checkpoint and checkpoint.exists():
            try:
                data = torch.load(checkpoint, map_location='cpu')
                if 'config' in data:
                    return data['config']
            except:
                pass
    
    return None


def copy_checkpoints(source_dir: Path, dest_dir: Path):
    """–°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –≤—Å—ñ —á–µ–∫–ø–æ—ñ–Ω—Ç–∏"""
    if not source_dir.exists():
        print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {source_dir}")
        return
    
    dest_checkpoints = dest_dir / "checkpoints"
    dest_checkpoints.mkdir(parents=True, exist_ok=True)
    
    checkpoint_files = list(source_dir.glob("checkpoint_*.pt"))
    if not checkpoint_files:
        print("‚ö†Ô∏è  –ß–µ–∫–ø–æ—ñ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ")
        return
    
    copied = 0
    for checkpoint in checkpoint_files:
        try:
            shutil.copy2(checkpoint, dest_checkpoints / checkpoint.name)
            copied += 1
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è {checkpoint.name}: {e}")
    
    print(f"‚úÖ –°–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ {copied} —á–µ–∫–ø–æ—ñ–Ω—Ç—ñ–≤")


def copy_dataset(dataset_path: Path, dest_dir: Path):
    """–°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç"""
    if not dataset_path.exists():
        print(f"‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {dataset_path}")
        return
    
    dest_dataset_dir = dest_dir / "dataset"
    dest_dataset_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        if dataset_path.is_file():
            shutil.copy2(dataset_path, dest_dataset_dir / dataset_path.name)
        else:
            # –Ø–∫—â–æ —Ü–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è, —Å–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –≤—Å—é
            shutil.copytree(dataset_path, dest_dataset_dir / dataset_path.name, dirs_exist_ok=True)
        
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ: {dataset_path.name}")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É: {e}")


def create_model_readme(model_dir: Path, model_name: str, model_path: Optional[Path], 
                       dataset_path: Optional[Path], config: Optional[dict]):
    """–°—Ç–≤–æ—Ä–∏—Ç–∏ README –∑ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è–º–∏ –¥–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è"""
    readme_path = model_dir / "README.md"
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(f"# –ú–æ–¥–µ–ª—å: {model_name}\n\n")
        f.write(f"**–î–∞—Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## –°—Ç—Ä—É–∫—Ç—É—Ä–∞\n\n")
        f.write("```\n")
        f.write(f"{model_name}/\n")
        if model_path:
            f.write("‚îú‚îÄ‚îÄ model.pt                    # –ú–æ–¥–µ–ª—å\n")
        f.write("‚îú‚îÄ‚îÄ model_config.json           # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ\n")
        f.write("‚îú‚îÄ‚îÄ training_config.json        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è\n")
        f.write("‚îú‚îÄ‚îÄ checkpoints/                # –ß–µ–∫–ø–æ—ñ–Ω—Ç–∏ –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è\n")
        if dataset_path:
            f.write("‚îú‚îÄ‚îÄ dataset/                    # –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è\n")
        f.write("‚îî‚îÄ‚îÄ README.md                   # –¶–µ–π —Ñ–∞–π–ª\n")
        f.write("```\n\n")
        
        f.write("## –î–æ–Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ\n\n")
        f.write("### 1. –ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç—É\n\n")
        f.write("```bash\n")
        f.write("python scripts/train_model.py \\\n")
        if dataset_path:
            f.write(f"    --dataset {model_dir}/dataset/{dataset_path.name} \\\n")
        f.write(f"    --resume {model_dir}/checkpoints/checkpoint_latest.pt \\\n")
        f.write("    --checkpoint-dir checkpoints \\\n")
        f.write("    --checkpoint-interval 100\n")
        f.write("```\n\n")
        
        f.write("### 2. –î–æ–Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ\n\n")
        f.write("**–í–∞–∂–ª–∏–≤–æ:** –î–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ.\n\n")
        f.write("**–í–∞—Ä—ñ–∞–Ω—Ç A: –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ checkpoint —è–∫ –±–∞–∑—É**\n")
        f.write("```bash\n")
        f.write("python scripts/train_model.py \\\n")
        f.write("    --dataset path/to/new_dataset.json \\\n")
        f.write(f"    --resume {model_dir}/checkpoints/checkpoint_latest.pt \\\n")
        f.write("    --checkpoint-dir checkpoints \\\n")
        f.write("    --checkpoint-interval 100\n")
        f.write("```\n\n")
        if model_path:
            f.write("**–í–∞—Ä—ñ–∞–Ω—Ç B: –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ Python**\n")
            f.write("```python\n")
            f.write("import torch\n")
            f.write("from train.model_factory import create_model\n")
            f.write("from config.trm_config import TRMConfig\n\n")
            f.write("# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é\n")
            f.write("import json\n")
            f.write("with open('model_config.json', 'r') as f:\n")
            f.write("    model_config = json.load(f)\n\n")
            f.write("# –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å\n")
            f.write("model = create_model(**model_config)\n\n")
            f.write("# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤–∞–≥–∏\n")
            f.write("model.load_state_dict(torch.load('model.pt', map_location='cpu'))\n")
            f.write("```\n\n")
            f.write("–ü–æ—Ç—ñ–º –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ —Ü—é –º–æ–¥–µ–ª—å –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ.\n\n")
        
        if config:
            f.write("## –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è\n\n")
            f.write("### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ\n\n")
            if 'dim' in config:
                f.write(f"- **dim:** {config.get('dim')}\n")
            if 'depth' in config:
                f.write(f"- **depth:** {config.get('depth')}\n")
            if 'seq_len' in config:
                f.write(f"- **seq_len:** {config.get('seq_len')}\n")
            if 'vocab_size' in config:
                f.write(f"- **vocab_size:** {config.get('vocab_size')}\n")
            
            f.write("\n### –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è\n\n")
            training_config = config.get('training_config', {})
            if isinstance(training_config, dict):
                if 'batch_size' in training_config:
                    f.write(f"- **batch_size:** {training_config.get('batch_size')}\n")
                if 'learning_rate' in training_config:
                    f.write(f"- **learning_rate:** {training_config.get('learning_rate')}\n")
                if 'epochs' in training_config:
                    f.write(f"- **epochs:** {training_config.get('epochs')}\n")
        
        f.write("\n## –ü—Ä–∏–º—ñ—Ç–∫–∏\n\n")
        f.write("- –í—Å—ñ —á–µ–∫–ø–æ—ñ–Ω—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤ `checkpoints/` –¥–ª—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è\n")
        if dataset_path:
            f.write(f"- –î–∞—Ç–∞—Å–µ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ `dataset/` –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è\n")
        f.write("- –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤ JSON —Ñ–∞–π–ª–∞—Ö\n")
        f.write("- –î–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ `--resume` –∑ —à–ª—è—Ö–æ–º –¥–æ checkpoint'—É\n")
    
    print(f"‚úÖ README —Å—Ç–≤–æ—Ä–µ–Ω–æ: {readme_path}")


def main():
    parser = argparse.ArgumentParser(
        description="–ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å, —á–µ–∫–ø–æ—ñ–Ω—Ç–∏ —Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç –≤ –æ–∫—Ä–µ–º—É –ø–∞–ø–∫—É –¥–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default=None,
        help="–ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ (–±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —è–∫ –Ω–∞–∑–≤–∞ –ø–∞–ø–∫–∏). –Ø–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ, –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –Ω–∞–∑–≤—É –¥–∞—Ç–∞—Å–µ—Ç—É"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="–®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ (.pt —Ñ–∞–π–ª). –Ø–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ, –±—É–¥–µ –∑–Ω–∞–π–¥–µ–Ω–æ –æ—Å—Ç–∞–Ω–Ω—é –º–æ–¥–µ–ª—å"
    )
    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default=None,
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç–∞–º–∏ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: checkpoints/)"
    )
    parser.add_argument(
        "--dataset-path",
        type=str,
        default=None,
        help="–®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="saved_models",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: saved_models/)"
    )
    
    args = parser.parse_args()
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ
    model_name = args.model_name
    if model_name is None:
        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É
        if args.dataset_path:
            dataset_path = Path(args.dataset_path)
            if not dataset_path.is_absolute():
                dataset_path = project_root / dataset_path
            if dataset_path.exists():
                model_name = get_model_name_from_dataset(dataset_path)
                print(f"üìù –ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ –≤–∏–∑–Ω–∞—á–µ–Ω–∞ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É: {model_name}")
            else:
                print("‚ùå –ü–æ–º–∏–ª–∫–∞: --model-name –Ω–µ –≤–∫–∞–∑–∞–Ω–æ —ñ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                print("   –í–∫–∞–∂—ñ—Ç—å --model-name –∞–±–æ --dataset-path")
                return
        else:
            print("‚ùå –ü–æ–º–∏–ª–∫–∞: --model-name –Ω–µ –≤–∫–∞–∑–∞–Ω–æ")
            print("   –í–∫–∞–∂—ñ—Ç—å --model-name –∞–±–æ --dataset-path –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è")
            return
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    output_dir = project_root / args.output_dir
    output_dir.mkdir(exist_ok=True, parents=True)
    
    model_dir = output_dir / model_name
    if model_dir.exists():
        response = input(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ {model_dir} –≤–∂–µ —ñ—Å–Ω—É—î. –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç–∏? (y/N): ")
        if response.lower() != 'y':
            print("–°–∫–∞—Å–æ–≤–∞–Ω–æ")
            return
        shutil.rmtree(model_dir)
    
    model_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {model_name}")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è: {model_dir}\n")
    
    # –ó–Ω–∞–π—Ç–∏ –∞–±–æ —Å–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å
    model_path = None
    if args.model_path:
        model_path = Path(args.model_path)
        if not model_path.is_absolute():
            model_path = project_root / model_path
    else:
        model_path = find_latest_model()
    
    if model_path and model_path.exists():
        try:
            shutil.copy2(model_path, model_dir / "model.pt")
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ: {model_path.name}")
        except Exception as e:
            print(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            model_path = None
    else:
        print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —Ç—ñ–ª—å–∫–∏ —á–µ–∫–ø–æ—ñ–Ω—Ç–∏ —Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    config = None
    if model_path:
        config = load_model_config(model_path)
        if config:
            # –ó–±–µ—Ä–µ–≥—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
            model_config = {k: v for k, v in config.items() if k != 'training_config'}
            training_config = config.get('training_config', {})
            
            with open(model_dir / "model_config.json", 'w', encoding='utf-8') as f:
                json.dump(model_config, f, indent=2, ensure_ascii=False)
            
            with open(model_dir / "training_config.json", 'w', encoding='utf-8') as f:
                json.dump(training_config, f, indent=2, ensure_ascii=False)
            
            print("‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞")
    
    # –°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ —á–µ–∫–ø–æ—ñ–Ω—Ç–∏
    checkpoint_dir = project_root / (args.checkpoint_dir or "checkpoints")
    copy_checkpoints(checkpoint_dir, model_dir)
    
    # –°–∫–æ–ø—ñ—é–≤–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
    dataset_path = None
    if args.dataset_path:
        dataset_path = Path(args.dataset_path)
        if not dataset_path.is_absolute():
            dataset_path = project_root / dataset_path
        copy_dataset(dataset_path, model_dir)
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ README
    create_model_readme(model_dir, model_name, model_path, dataset_path, config)
    
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
    print(f"üìÅ –®–ª—è—Ö: {model_dir}")
    print(f"\nüí° –î–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –¥–∏–≤—ñ—Ç—å—Å—è —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó –≤ {model_dir}/README.md")
    
    # –ó–∞–ø–∏—Ç–∞—Ç–∏ –ø—Ä–æ –æ—á–∏—â–µ–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è
    print("\n" + "=" * 80)
    response = input("üßπ –û—á–∏—Å—Ç–∏—Ç–∏ —Ñ–∞–π–ª–∏ –Ω–∞–≤—á–∞–Ω–Ω—è (checkpoints/, logs/, temp/, models/trained/)? (y/N): ")
    if response.lower() == 'y':
        keep_model = None
        if model_path and model_path.exists():
            # –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å —è–∫—â–æ –≤–æ–Ω–∞ –≤ models/trained/
            if model_path.parent.name == "trained":
                keep_model = model_path
        cleanup_training_files(project_root, keep_model=keep_model)
    else:
        print("   –§–∞–π–ª–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–ª–∏—à–µ–Ω–æ –±–µ–∑ –∑–º—ñ–Ω")


if __name__ == "__main__":
    main()

