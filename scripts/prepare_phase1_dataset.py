#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –§–ê–ó–ò 1 - Language Pretraining Dataset
–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î Simple Wikipedia —Ç–∞ —Å—Ç–≤–æ—Ä—é—î plain text corpus –¥–ª—è causal language modeling

–ö–†–ò–¢–ò–ß–ù–Ü –í–ò–ú–û–ì–ò:
- –¢—ñ–ª—å–∫–∏ plain text (–ë–ï–ó instruction format)
- –ó–∞–º—ñ–Ω–∞ newlines –Ω–∞ spaces
- –û–±–º–µ–∂–µ–Ω–Ω—è –¥–æ ~15-20M —Ç–æ–∫–µ–Ω—ñ–≤ (CPU-safe)
- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤ datasets/pretrain_text.txt
"""

import os
import sys
from pathlib import Path
from datasets import load_dataset
from transformers import GPT2Tokenizer
import re

# –î–æ–¥–∞—Ç–∏ project root –¥–æ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def clean_text(text: str) -> str:
    """
    –û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –¥–ª—è language pretraining
    - –ó–∞–º—ñ–Ω–∞ newlines –Ω–∞ spaces
    - –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
    - –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤
    """
    # –ó–∞–º—ñ–Ω–∞ newlines –Ω–∞ spaces (—è–∫ –≤–∏–º–∞–≥–∞—î—Ç—å—Å—è)
    text = text.replace('\n', ' ').replace('\r', ' ')
    
    # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤
    text = re.sub(r'\s+', ' ', text)
    
    # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤ —Ç–∞ –∑–∞–ª–∏—à–µ–Ω–Ω—è —Ç—ñ–ª—å–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
    text = re.sub(r'[^\w\s\.,!?;:\-\(\)\'\"]+', ' ', text)
    
    # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤ –∑–Ω–æ–≤—É
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def main():
    print("üöÄ –§–ê–ó–ê 1 - –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ Language Pretraining Dataset")
    print("=" * 60)
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    output_file = project_root / "datasets" / "pretrain_text.txt"
    target_tokens = 20_000_000  # 20M —Ç–æ–∫–µ–Ω—ñ–≤ (CPU-safe)
    min_text_length = 50  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É datasets —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
    output_file.parent.mkdir(exist_ok=True)
    
    print(f"üìÅ –í–∏—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª: {output_file}")
    print(f"üéØ –¶—ñ–ª—å–æ–≤–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤: {target_tokens:,}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ GPT-2 tokenizer –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —Ç–æ–∫–µ–Ω—ñ–≤
    print("\nüî§ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GPT-2 tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ Simple Wikipedia
    print("\nüìñ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Simple Wikipedia dataset...")
    try:
        dataset = load_dataset("rahular/simple-wikipedia", split="train")
        print(f"   ‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(dataset):,} —Å—Ç–∞—Ç–µ–π")
    except Exception as e:
        print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
        print("   üîÑ –°–ø—Ä–æ–±—É–π—Ç–µ: pip install datasets")
        return False
    
    # –û–±—Ä–æ–±–∫–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    print(f"\nüìù –û–±—Ä–æ–±–∫–∞ —Å—Ç–∞—Ç–µ–π (—Ü—ñ–ª—å: {target_tokens:,} —Ç–æ–∫–µ–Ω—ñ–≤)...")
    
    total_tokens = 0
    processed_articles = 0
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for i, article in enumerate(dataset):
            if total_tokens >= target_tokens:
                break
                
            # –í–∏—Ç—è–≥—Ç–∏ —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—Ç—ñ
            text = article.get('text', '').strip()
            
            if len(text) < min_text_length:
                continue
                
            # –û—á–∏—Å—Ç–∏—Ç–∏ —Ç–µ–∫—Å—Ç
            cleaned_text = clean_text(text)
            
            if len(cleaned_text) < min_text_length:
                continue
            
            # –ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ —Ç–æ–∫–µ–Ω–∏
            tokens = tokenizer.encode(cleaned_text)
            token_count = len(tokens)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –Ω–µ –ø–µ—Ä–µ–≤–∏—â—É—î–º–æ –ª—ñ–º—ñ—Ç
            if total_tokens + token_count > target_tokens:
                # –û–±—Ä—ñ–∑–∞—Ç–∏ —Ç–µ–∫—Å—Ç —â–æ–± –≤–ª—ñ–∑—Ç–∏ –≤ –ª—ñ–º—ñ—Ç
                remaining_tokens = target_tokens - total_tokens
                if remaining_tokens > 100:  # –¢—ñ–ª—å–∫–∏ —è–∫—â–æ –∑–∞–ª–∏—à–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –º—ñ—Å—Ü—è
                    truncated_tokens = tokens[:remaining_tokens]
                    truncated_text = tokenizer.decode(truncated_tokens)
                    f.write(truncated_text + ' ')
                    total_tokens += len(truncated_tokens)
                break
            
            # –ó–∞–ø–∏—Å–∞—Ç–∏ —Ç–µ–∫—Å—Ç
            f.write(cleaned_text + ' ')
            total_tokens += token_count
            processed_articles += 1
            
            # –ü—Ä–æ–≥—Ä–µ—Å
            if processed_articles % 1000 == 0:
                progress = (total_tokens / target_tokens) * 100
                print(f"   üìä –û–±—Ä–æ–±–ª–µ–Ω–æ: {processed_articles:,} —Å—Ç–∞—Ç–µ–π, "
                      f"—Ç–æ–∫–µ–Ω—ñ–≤: {total_tokens:,} ({progress:.1f}%)")
    
    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   üìÑ –§–∞–π–ª: {output_file}")
    print(f"   üìä –°—Ç–∞—Ç–µ–π –æ–±—Ä–æ–±–ª–µ–Ω–æ: {processed_articles:,}")
    print(f"   üî§ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤: {total_tokens:,}")
    print(f"   üìè –†–æ–∑–º—ñ—Ä —Ñ–∞–π–ª—É: {output_file.stat().st_size / (1024*1024):.1f} MB")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
    print(f"\nüîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É...")
    with open(output_file, 'r', encoding='utf-8') as f:
        sample = f.read(500)
        print(f"   üìù –ü–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤:")
        print(f"   {sample}...")
    
    print(f"\nüéØ –§–ê–ó–ê 1 –≥–æ—Ç–æ–≤–∞ –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è!")
    print(f"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ñ–∞–π–ª: {output_file}")
    print(f"   –î–ª—è causal language modeling –Ω–∞ CPU")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


