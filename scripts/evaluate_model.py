"""
CLI —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ –∑ —Ä—ñ–∑–Ω–∏–º–∏ evaluators
"""
import sys
import argparse
import torch
from pathlib import Path
from typing import Optional

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tiny_recursive_model import TinyRecursiveModel
from tiny_recursive_model.utils import load_tokenizer
from inference.model_inference import load_trained_model
from train.evaluators import (
    BaseEvaluator,
    ARCEvaluator,
    SudokuEvaluator,
    MazeEvaluator,
    GeneralEvaluator
)
from train.datasets.trm_dataset import TRMDataset
from train.constants import DEFAULT_TOKENIZER_NAME


def get_evaluator(evaluator_type: str) -> BaseEvaluator:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ evaluator –∑–∞ —Ç–∏–ø–æ–º
    
    Args:
        evaluator_type: –¢–∏–ø evaluator ('arc', 'sudoku', 'maze', 'general')
    
    Returns:
        Evaluator instance
    """
    evaluator_type = evaluator_type.lower()
    
    if evaluator_type == 'arc':
        return ARCEvaluator()
    elif evaluator_type == 'sudoku':
        return SudokuEvaluator()
    elif evaluator_type == 'maze':
        return MazeEvaluator()
    elif evaluator_type == 'general':
        return GeneralEvaluator()
    else:
        raise ValueError(f"Unknown evaluator type: {evaluator_type}")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    parser = argparse.ArgumentParser(
        description="–û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞–≤—á–µ–Ω—É TRM –º–æ–¥–µ–ª—å –∑ —Ä—ñ–∑–Ω–∏–º–∏ evaluators",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
  # –û—Ü—ñ–Ω–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ –∑–∞–≥–∞–ª—å–Ω–∏—Ö –∑–∞–¥–∞—á–∞—Ö:
  python scripts/evaluate_model.py --model models/trained/model.pt --evaluator general --dataset datasets/eval/data.json
  
  # –û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞ ARC –∑–∞–¥–∞—á–∞—Ö:
  python scripts/evaluate_model.py --model models/trained/model.pt --evaluator arc --dataset datasets/puzzles/arc.json
  
  # –û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞ Sudoku:
  python scripts/evaluate_model.py --model models/trained/model.pt --evaluator sudoku --dataset datasets/puzzles/sudoku.json
  
  # –û—Ü—ñ–Ω–∏—Ç–∏ –Ω–∞ Maze:
  python scripts/evaluate_model.py --model models/trained/model.pt --evaluator maze --dataset datasets/puzzles/maze.json
        """
    )
    
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="–®–ª—è—Ö –¥–æ –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ (.pt —Ñ–∞–π–ª)"
    )
    
    parser.add_argument(
        "--evaluator",
        type=str,
        required=True,
        choices=["arc", "sudoku", "maze", "general"],
        help="–¢–∏–ø evaluator –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è"
    )
    
    parser.add_argument(
        "--dataset",
        type=str,
        required=True,
        help="–®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –æ—Ü—ñ–Ω–∫–∏"
    )
    
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: –≤—Å—ñ)"
    )
    
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="–ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è –æ—Ü—ñ–Ω–∫–∏ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: auto)"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="–®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (JSON —Ñ–∞–π–ª, –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: temp/evaluation_results.json)"
    )
    
    args = parser.parse_args()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤
    model_path = project_root / args.model
    if not model_path.exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {model_path}")
        return
    
    dataset_path = project_root / args.dataset
    if not dataset_path.exists():
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {dataset_path}")
        return
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –ø—Ä–∏—Å—Ç—Ä—ñ–π
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    print("=" * 80)
    print("üìä –û–¶–Ü–ù–ö–ê –ú–û–î–ï–õ–Ü")
    print("=" * 80)
    print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model_path.name}")
    print(f"üìö –î–∞—Ç–∞—Å–µ—Ç: {dataset_path.name}")
    print(f"üîç Evaluator: {args.evaluator}")
    print(f"üíª –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
    print(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    try:
        tokenizer, _, _ = load_tokenizer(DEFAULT_TOKENIZER_NAME)
        
        inference = load_trained_model(
            model_path=str(model_path),
            device=device,
            tokenizer_name=DEFAULT_TOKENIZER_NAME
        )
        
        model = inference.model
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
    print(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É...")
    try:
        dataset = TRMDataset(
            data_path=dataset_path,
            tokenizer=tokenizer,
            max_seq_len=512
        )
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {len(dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ evaluator
    print(f"\nüîç –°—Ç–≤–æ—Ä–µ–Ω–Ω—è evaluator...")
    try:
        evaluator = get_evaluator(args.evaluator)
        print(f"‚úÖ Evaluator —Å—Ç–≤–æ—Ä–µ–Ω–æ: {args.evaluator}")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è evaluator: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –æ—Ü—ñ–Ω–∫—É
    print(f"\nüöÄ –ü–æ—á–∞—Ç–æ–∫ –æ—Ü—ñ–Ω–∫–∏...")
    try:
        results = evaluator.evaluate(
            model=model,
            dataset=dataset,
            max_samples=args.max_samples,
            tokenizer=tokenizer
        )
        
        # –í–∏–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        print("\n" + "=" * 80)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò –û–¶–Ü–ù–ö–ò")
        print("=" * 80)
        print(evaluator.format_results(results))
        print("=" * 80)
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        output_path = args.output
        if output_path is None:
            temp_dir = project_root / "temp"
            temp_dir.mkdir(exist_ok=True, parents=True)
            output_path = temp_dir / f"evaluation_results_{args.evaluator}.json"
        else:
            output_path = project_root / output_path
        
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_path}")
        
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ü—ñ–Ω–∫–∏: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

