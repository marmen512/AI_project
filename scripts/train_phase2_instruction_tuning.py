#!/usr/bin/env python3
"""
–§–ê–ó–ê 2 - Instruction Tuning Script
–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ø—ñ—Å–ª—è –§–ê–ó–ò 1 –Ω–∞ instruction datasets

–ö–†–ò–¢–ò–ß–ù–Ü –í–ò–ú–û–ì–ò:
- –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ weights –∑ –§–ê–ó–ò 1 (–ù–ï random initialization)
- Instruction datasets (Alpaca, SQuAD, DailyDialog)
- –ú–∞–∫—Å–∏–º—É–º 1-2 epochs
- –ó—É–ø–∏–Ω–∫–∞ –ø—Ä–∏ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—ñ —è–∫–æ—Å—Ç—ñ
"""

import sys
import argparse
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel
import yaml
import json
from tqdm import tqdm
import logging
import os
import time
import itertools
import signal

# –î–æ–¥–∞—Ç–∏ project root –¥–æ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def _atomic_torch_save(obj, path: Path):
    tmp_path = path.with_suffix(path.suffix + ".tmp")
    torch.save(obj, tmp_path)
    os.replace(tmp_path, path)

def _save_phase2_checkpoint(
    checkpoint_path: Path,
    *,
    epoch: int,
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    loss: float,
    config: dict,
    tokenizer_vocab_size: int,
    global_step: int,
    batch_idx: int,
    epoch_completed: bool = False,
    epoch_loss_sum: float = 0.0,
    epoch_loss_count: int = 0,
    is_emergency: bool = False,
):
    payload = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': float(loss),
        'config': config,
        'phase': 2,
        'tokenizer_vocab_size': int(tokenizer_vocab_size),
        'train_state': {
            'global_step': int(global_step),
            'batch_idx': int(batch_idx),
            'epoch_completed': bool(epoch_completed),
            'epoch_loss_sum': float(epoch_loss_sum),
            'epoch_loss_count': int(epoch_loss_count),
            'is_emergency': bool(is_emergency),
        },
    }

    _atomic_torch_save(payload, checkpoint_path)

class InstructionDataset(Dataset):
    """
    Dataset –¥–ª—è –§–ê–ó–ò 2 - Instruction Tuning (CORRECT)
    - Instruction + Input = context (labels masked)
    - Output = supervised target
    """

    def __init__(self, data_files: list, tokenizer, max_seq_len: int = 256):
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.samples = []

        print("üìö –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è instruction datasets (safe mode)...")

        for data_file in data_files:
            print(f"   üìñ {data_file}")

            with open(data_file, "r", encoding="utf-8") as f:
                raw = json.load(f)

            if isinstance(raw, dict) and "data" in raw:
                data = raw["data"]
            elif isinstance(raw, list):
                data = raw
            else:
                print(f"   ‚ö†Ô∏è –ù–µ–≤—ñ–¥–æ–º–∏–π —Ñ–æ—Ä–º–∞—Ç: {data_file}")
                continue

            added = 0
            for sample in data:
                if self._is_valid(sample):
                    self.samples.append(sample)
                    added += 1

            print(f"      ‚úÖ –î–æ–¥–∞–Ω–æ {added:,} samples")

        print(f"   üìä –ó–∞–≥–∞–ª–æ–º instruction samples: {len(self.samples):,}")

    def _is_valid(self, sample):
        return (
            isinstance(sample, dict)
            and "instruction" in sample
            and "output" in sample
            and len(sample["instruction"].strip()) > 0
            and len(sample["output"].strip()) > 0
        )

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        instruction = sample["instruction"].strip()
        input_text = sample.get("input", "").strip()
        output = sample["output"].strip()

        # Unified template: if input exists, prepend to instruction
        if input_text:
            instruction = f"{instruction}\n\n{input_text}"

        prompt = f"Instruction:\n{instruction}\n\nResponse:\n"

        prompt_ids = self.tokenizer(
            prompt,
            add_special_tokens=False,
            truncation=True,
            max_length=self.max_seq_len,
        )["input_ids"]

        remaining = max(self.max_seq_len - len(prompt_ids), 0)
        output_ids = self.tokenizer(
            output,
            add_special_tokens=False,
            truncation=True,
            max_length=remaining,
        )["input_ids"]

        # Concatenate and mask: prompt tokens = -100, response tokens = real IDs
        input_ids = prompt_ids + output_ids
        labels = [-100] * len(prompt_ids) + output_ids

        # Truncate to max_seq_len
        input_ids = input_ids[: self.max_seq_len]
        labels = labels[: self.max_seq_len]

        # Pad with PAD token and mask labels
        pad_len = self.max_seq_len - len(input_ids)
        if pad_len > 0:
            input_ids += [self.tokenizer.pad_token_id] * pad_len
            labels += [-100] * pad_len

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
        }

def load_phase1_model(checkpoint_path: str, config, tokenizer):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å –∑ –§–ê–ó–ò 1 —Ç–∞ —Ä–æ–∑—à–∏—Ä–∏—Ç–∏ embeddings –¥–ª—è PAD —Ç–æ–∫–µ–Ω–∞"""
    print(f"üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –§–ê–ó–ò 1: {checkpoint_path}")
    
    if not Path(checkpoint_path).exists():
        raise FileNotFoundError(f"Checkpoint –§–ê–ó–ò 1 –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {checkpoint_path}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # Phase1 –º–æ–∂–µ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –∞–±–æ —á–∏—Å—Ç–∏–π state_dict (best_model.pt),
    # –∞–±–æ dict –∑ model_state_dict (last/emergency).
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        phase1_state_dict = checkpoint['model_state_dict']
    else:
        phase1_state_dict = checkpoint
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å –∑ —Ç—ñ—î—é –∂ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é (—Å–ø–æ—á–∞—Ç–∫—É –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–º vocab_size)
    original_vocab_size = 50257  # GPT-2 vocab size
    model_config = GPT2Config(
        vocab_size=original_vocab_size,
        n_positions=config['model']['seq_len'],
        n_embd=config['model']['dim'],
        n_layer=config['model']['depth'],
        n_head=int(config.get('model', {}).get('heads', 8)),
        n_inner=config['model']['dim'] * 4,
        activation_function="gelu",
        resid_pdrop=0.1,
        embd_pdrop=0.1,
        attn_pdrop=0.1,
        layer_norm_epsilon=1e-5,
        initializer_range=0.02,
        use_cache=False
    )
    
    model = GPT2LMHeadModel(model_config)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ weights –∑ –§–ê–ó–ò 1
    load_result = model.load_state_dict(phase1_state_dict, strict=False)

    missing = getattr(load_result, 'missing_keys', [])
    unexpected = getattr(load_result, 'unexpected_keys', [])
    if missing or unexpected:
        print("   ‚ö†Ô∏è –£–í–ê–ì–ê: –Ω–µ—ñ–¥–µ–∞–ª—å–Ω–∏–π match –≤–∞–≥ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ Phase1 ‚Üí Phase2")
        if missing:
            print(f"   ‚ö†Ô∏è missing_keys: {len(missing)}")
        if unexpected:
            print(f"   ‚ö†Ô∏è unexpected_keys: {len(unexpected)}")

    if isinstance(checkpoint, dict) and 'epoch' in checkpoint:
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑ epoch {checkpoint.get('epoch')}")
        if 'loss' in checkpoint:
            try:
                print(f"   üìä Loss –∑ –§–ê–ó–ò 1: {float(checkpoint['loss']):.4f}")
            except Exception:
                pass
    else:
        print("   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ (Phase1 state_dict)")
    
    # –ö–†–ò–¢–ò–ß–ù–û: –†–æ–∑—à–∏—Ä–∏—Ç–∏ embeddings —è–∫—â–æ –¥–æ–¥–∞–Ω–æ PAD —Ç–æ–∫–µ–Ω
    current_vocab_size = len(tokenizer)
    if current_vocab_size > original_vocab_size:
        print(f"   üîß –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è embeddings: {original_vocab_size} ‚Üí {current_vocab_size}")
        
        # –†–æ–∑—à–∏—Ä–∏—Ç–∏ input embeddings
        old_embeddings = model.transformer.wte.weight.data
        new_embeddings = torch.zeros(current_vocab_size, model.config.n_embd)
        new_embeddings[:original_vocab_size] = old_embeddings
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ç–æ–∫–µ–Ω–∏ (PAD) —Å–µ—Ä–µ–¥–Ω—ñ–º –∑–Ω–∞—á–µ–Ω–Ω—è–º —ñ—Å–Ω—É—é—á–∏—Ö embeddings
        new_embeddings[original_vocab_size:] = old_embeddings.mean(dim=0, keepdim=True)
        
        # –ó–∞–º—ñ–Ω–∏—Ç–∏ embeddings
        model.transformer.wte = nn.Embedding(current_vocab_size, model.config.n_embd)
        model.transformer.wte.weight.data = new_embeddings
        
        # –†–æ–∑—à–∏—Ä–∏—Ç–∏ output layer (lm_head)
        old_lm_head = model.lm_head.weight.data
        new_lm_head = torch.zeros(current_vocab_size, model.config.n_embd)
        new_lm_head[:original_vocab_size] = old_lm_head
        new_lm_head[original_vocab_size:] = old_lm_head.mean(dim=0, keepdim=True)
        
        model.lm_head = nn.Linear(model.config.n_embd, current_vocab_size, bias=False)
        model.lm_head.weight.data = new_lm_head
        
        # –û–Ω–æ–≤–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        model.config.vocab_size = current_vocab_size
        
        print(f"   ‚úÖ Embeddings —Ä–æ–∑—à–∏—Ä–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
    
    return model

def setup_logging(log_dir: str, phase: str = "phase2"):
    """–ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
    log_dir = Path(log_dir)
    log_dir.mkdir(exist_ok=True, parents=True)
    
    log_file = log_dir / f"{phase}_instruction_tuning.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def run_sanity_inference(model, tokenizer, device, step: int, logger):
    """Run online sanity inference every N steps to detect collapse."""
    sanity_prompts = [
        "What is 2 + 2?",
        "The capital of France is",
        "Give three tips for staying healthy."
    ]
    model.eval()
    for p in sanity_prompts:
        prompt = f"Instruction:\n{p}\n\nResponse:\n"
        enc = tokenizer(prompt, return_tensors='pt', add_special_tokens=False, truncation=True, max_length=256)
        input_ids = enc['input_ids'].to(device)
        with torch.no_grad():
            gen_ids = model.generate(
                input_ids=input_ids,
                max_new_tokens=16,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        gen_ids = gen_ids[0]
        completion_ids = gen_ids[int(input_ids.shape[1]):]
        out = tokenizer.decode(completion_ids, skip_special_tokens=True)
        logger.info(f"[Sanity step {step}] {p} -> {out}")
    model.train()

def is_collapsed_output(text: str) -> bool:
    """Detect collapse: >50% non-alphanumeric OR repeated single chars."""
    if not text:
        return True
    alnum_count = sum(c.isalnum() or c.isspace() for c in text)
    if alnum_count / len(text) < 0.5:
        return True
    # Detect repeated single characters (e.g., "22222", "aaaaa")
    if len(set(text.strip())) == 1 and len(text.strip()) > 3:
        return True
    return False

def train_epoch(
    model,
    dataloader,
    optimizer,
    tokenizer,
    device,
    epoch,
    logger,
    *,
    checkpoint_dir: Path,
    config: dict,
    tokenizer_vocab_size: int,
    global_step: int,
    save_every_steps: int,
    resume_batch_idx: int,
    resume_epoch_loss_sum: float,
    resume_epoch_loss_count: int,
):
    """–ù–∞–≤—á–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ epoch –¥–ª—è instruction tuning"""
    model.train()
    total_loss = float(resume_epoch_loss_sum)
    num_batches = len(dataloader)

    start_batch_idx = int(resume_batch_idx)
    if start_batch_idx < -1:
        start_batch_idx = -1
    if start_batch_idx >= num_batches:
        start_batch_idx = -1

    if start_batch_idx >= 0:
        logger.info(
            "üîÑ Exact resume: epoch=%s start_batch_idx=%s/%s (will continue from next batch)",
            epoch,
            start_batch_idx,
            num_batches,
        )
        dataloader_iter = itertools.islice(dataloader, start_batch_idx + 1, None)
        progress_bar = tqdm(
            dataloader_iter,
            desc=f"Phase 2 Epoch {epoch}",
            total=num_batches,
            initial=start_batch_idx + 1,
        )
        batch_enumerate_start = start_batch_idx + 1
    else:
        progress_bar = tqdm(dataloader, desc=f"Phase 2 Epoch {epoch}")
        batch_enumerate_start = 0

    batch_time_start = time.time()
    last_log_time = time.time()
    last_log_step = global_step

    training_cfg = config.get('training', {})
    loss_guard_enabled = bool(training_cfg.get('loss_guard_enabled', False))
    loss_guard_ema_beta = float(training_cfg.get('loss_guard_ema_beta', 0.98))
    loss_guard_warmup_steps = int(training_cfg.get('loss_guard_warmup_steps', 200))
    loss_guard_threshold_ratio = float(training_cfg.get('loss_guard_threshold_ratio', 0.15))
    loss_guard_patience_steps = int(training_cfg.get('loss_guard_patience_steps', 100))

    loss_ema = None
    best_loss_ema = None
    guard_bad_steps = 0
    local_step = 0
    guard_activated_logged = False

    skipped_all_ignored_batches = 0

    # Count of processed batches in this epoch so far (including resumed part)
    total_seen_batches = int(resume_epoch_loss_count)
    if total_seen_batches < 0:
        total_seen_batches = 0

    # Gradient accumulation and clipping
    gradient_accumulation_steps = int(training_cfg.get('gradient_accumulation_steps', 8))
    sanity_interval = int(training_cfg.get('sanity_interval', 100))
    max_grad_norm = float(training_cfg.get('max_grad_norm', 1.0))

    optimizer.zero_grad()
    accum_steps = 0
    collapse_consecutive = 0

    for batch_idx, batch in enumerate(progress_bar, start=batch_enumerate_start):
        try:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)

            # Skip batches with no supervision (all labels -100)
            if not torch.any(labels != -100):
                skipped_all_ignored_batches += 1
                if skipped_all_ignored_batches <= 10 or skipped_all_ignored_batches % 100 == 0:
                    logger.warning(
                        "‚ö†Ô∏è Skipping batch with all labels=-100 (no supervision): epoch=%s batch=%s/%s skipped=%s",
                        epoch,
                        batch_idx,
                        num_batches,
                        skipped_all_ignored_batches,
                    )
                continue

            # Forward pass
            outputs = model(input_ids=input_ids, labels=labels)
            loss = outputs.loss / gradient_accumulation_steps  # Scale loss

            # Early stop on NaN / Inf
            if not torch.isfinite(loss):
                logger.error("‚ùå NaN/Inf loss detected ‚Äî emergency save and stop")
                _save_phase2_checkpoint(
                    checkpoint_dir / "emergency_checkpoint.pt",
                    epoch=epoch,
                    model=model,
                    optimizer=optimizer,
                    loss=loss.item() if hasattr(loss, "item") else float("inf"),
                    config=config,
                    tokenizer_vocab_size=tokenizer_vocab_size,
                    global_step=global_step,
                    batch_idx=batch_idx,
                    epoch_completed=False,
                    is_emergency=True,
                )
                raise RuntimeError("NaN/Inf loss detected")

            # Backward pass
            loss.backward()
            accum_steps += 1

            # Step after accumulation
            if accum_steps % gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                optimizer.step()
                optimizer.zero_grad()

            global_step += 1
            local_step += 1
            total_loss += loss.item() * gradient_accumulation_steps  # Unscale for logging
            total_seen_batches += 1
            avg_loss = total_loss / max(total_seen_batches, 1)

            # Online sanity inference
            if global_step % sanity_interval == 0:
                run_sanity_inference(model, tokenizer, device, global_step, logger)
                # Simple collapse detection based on last logged outputs (could be enhanced)
                # Here we rely on loss guard and manual inspection; for strict auto-stop, integrate is_collapsed_output

            # EMA loss guard (unchanged)
            if loss_guard_enabled:
                loss_val = float(loss.item())
                if loss_ema is None:
                    loss_ema = loss_val
                else:
                    loss_ema = loss_guard_ema_beta * loss_ema + (1.0 - loss_guard_ema_beta) * loss_val

                if local_step >= loss_guard_warmup_steps:
                    if not guard_activated_logged:
                        logger.info(
                            "üõ°Ô∏è Loss guard active (local_step=%s, warmup_steps=%s)",
                            local_step,
                            loss_guard_warmup_steps,
                        )
                        guard_activated_logged = True

                    if best_loss_ema is None:
                        best_loss_ema = loss_ema
                    else:
                        best_loss_ema = min(best_loss_ema, loss_ema)

                    degrade_ratio = (loss_ema - best_loss_ema) / max(best_loss_ema, 1e-8)
                    if degrade_ratio > loss_guard_threshold_ratio:
                        guard_bad_steps += 1
                        if guard_bad_steps % 25 == 0:
                            logger.warning(
                                "‚ö†Ô∏è Loss guard: step=%s ema=%.4f best_ema=%.4f degrade=%.1f%% bad_steps=%s/%s",
                                global_step,
                                loss_ema,
                                best_loss_ema,
                                100.0 * degrade_ratio,
                                guard_bad_steps,
                                loss_guard_patience_steps,
                            )
                    else:
                        guard_bad_steps = 0

                    if guard_bad_steps >= loss_guard_patience_steps:
                        logger.error(
                            "‚ùå Loss guard triggered ‚Äî emergency save and stop (ema=%.4f best_ema=%.4f degrade=%.1f%%)",
                            loss_ema,
                            best_loss_ema,
                            100.0 * degrade_ratio,
                        )
                        _save_phase2_checkpoint(
                            checkpoint_dir / "emergency_checkpoint.pt",
                            epoch=epoch,
                            model=model,
                            optimizer=optimizer,
                            loss=loss_ema,
                            config=config,
                            tokenizer_vocab_size=tokenizer_vocab_size,
                            global_step=global_step,
                            batch_idx=batch_idx,
                            epoch_completed=False,
                            epoch_loss_sum=total_loss,
                            epoch_loss_count=total_seen_batches,
                            is_emergency=True,
                        )
                        raise RuntimeError("Loss guard triggered")

            # –û–Ω–æ–≤–∏—Ç–∏ progress bar
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_loss': f'{avg_loss:.4f}'
            })

            # Periodic autosave (power-outage resilience)
            if save_every_steps > 0 and (global_step % save_every_steps == 0):
                _save_phase2_checkpoint(
                    checkpoint_dir / "last_checkpoint.pt",
                    epoch=epoch,
                    model=model,
                    optimizer=optimizer,
                    loss=avg_loss,
                    config=config,
                    tokenizer_vocab_size=tokenizer_vocab_size,
                    global_step=global_step,
                    batch_idx=batch_idx,
                    epoch_completed=False,
                    epoch_loss_sum=total_loss,
                    epoch_loss_count=total_seen_batches,
                    is_emergency=False,
                )
                logger.info(
                    f"üíæ Autosave: epoch={epoch} step={global_step} batch={batch_idx}/{num_batches} avg_loss={avg_loss:.4f}"
                )

            # –õ–æ–≥—É–≤–∞–Ω–Ω—è (—á–∞—Å–æ–≤–µ + –ø–æ –∫—Ä–æ–∫–∞—Ö)
            if batch_idx % 50 == 0:
                now = time.time()
                dt = max(now - last_log_time, 1e-9)
                dsteps = max(global_step - last_log_step, 1)
                steps_per_sec = dsteps / dt
                tokens_per_step = int(input_ids.numel())
                tok_per_sec = tokens_per_step * steps_per_sec
                eta_steps = max(num_batches - (batch_idx + 1), 0)
                eta_sec = eta_steps / max(steps_per_sec, 1e-9)

                current_lr = optimizer.param_groups[0].get('lr', None)
                lr_str = f"{current_lr:.3e}" if isinstance(current_lr, float) else str(current_lr)

                logger.info(
                    "Phase 2 | epoch=%s batch=%s/%s step=%s lr=%s loss=%.4f avg_loss=%.4f tok/s=%.0f eta=%.1fmin",
                    epoch,
                    batch_idx,
                    num_batches,
                    global_step,
                    lr_str,
                    loss.item(),
                    avg_loss,
                    tok_per_sec,
                    eta_sec / 60.0,
                )
                last_log_time = now
                last_log_step = global_step
        except KeyboardInterrupt:
            logger.warning("üõë KeyboardInterrupt inside train loop ‚Äî emergency save")
            try:
                signal.signal(signal.SIGINT, signal.SIG_IGN)
            except Exception:
                pass
            _save_phase2_checkpoint(
                checkpoint_dir / "emergency_checkpoint.pt",
                epoch=epoch,
                model=model,
                optimizer=optimizer,
                loss=total_loss / max(batch_idx + 1, 1),
                config=config,
                tokenizer_vocab_size=tokenizer_vocab_size,
                global_step=global_step,
                batch_idx=batch_idx,
                epoch_completed=False,
                epoch_loss_sum=total_loss,
                epoch_loss_count=total_seen_batches,
                is_emergency=True,
            )
            raise

    avg_epoch_loss = total_loss / max(total_seen_batches, 1)
    logger.info(f"Phase 2 Epoch {epoch} –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–µ—Ä–µ–¥–Ω—ñ–π loss: {avg_epoch_loss:.4f}")
    if skipped_all_ignored_batches > 0:
        logger.warning(
            "‚ö†Ô∏è Epoch %s finished with %s skipped batches (all labels=-100)",
            epoch,
            skipped_all_ignored_batches,
        )

    return avg_epoch_loss, global_step

def main():
    parser = argparse.ArgumentParser(description="–§–ê–ó–ê 2 - Instruction Tuning")
    parser.add_argument("--config", type=str, default="config/phase2_instruction_tuning.yaml",
                       help="–®–ª—è—Ö –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω–æ–≥–æ —Ñ–∞–π–ª—É –§–ê–ó–ò 2")
    parser.add_argument("--phase1-model", type=str, required=False,
                       help="–®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ –∑ –§–ê–ó–ò 1 (ignored if --resume is set)")
    parser.add_argument("--resume", type=str, default=None,
                       help="–®–ª—è—Ö –¥–æ checkpoint –§–ê–ó–ò 2 –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è")
    
    args = parser.parse_args()
    
    if args.resume is None and args.phase1_model is None:
        parser.error("--phase1-model is required if --resume is not set")
    
    print("üöÄ –§–ê–ó–ê 2 - Instruction Tuning")
    print("=" * 60)
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —ñ—Å–Ω—É—î –º–æ–¥–µ–ª—å –∑ –§–ê–ó–ò 1 (only if not resuming)
    if args.resume is None and not Path(args.phase1_model).exists():
        print(f"‚ùå –ü–û–ú–ò–õ–ö–ê: –ú–æ–¥–µ–ª—å –∑ –§–ê–ó–ò 1 –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {args.phase1_model}")
        print("   –°–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–µ—Ä—à—ñ—Ç—å –§–ê–ó–£ 1 - Language Pretraining!")
        sys.exit(1)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    print(f"üìã –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –§–ê–ó–ò 2: {args.config}")
    if args.resume:
        print(f"üîÑ Resume from checkpoint: {args.resume}")
    else:
        print(f"üîó –ú–æ–¥–µ–ª—å –∑ –§–ê–ó–ò 1: {args.phase1_model}")
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è
    logger = setup_logging(config['training']['log_dir'])
    logger.info("–ü–æ—á–∞—Ç–æ–∫ –§–ê–ó–ò 2 - Instruction Tuning")
    
    # CPU –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    device = torch.device('cpu')
    torch.set_num_threads(config['cpu_optimization']['num_threads'])
    
    print(f"üíª –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ tokenizer (—Ç–æ–π –∂–µ —â–æ –≤ –§–ê–ó–Ü 1)
    print("üî§ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GPT-2 tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    
    # –ö–†–ò–¢–ò–ß–ù–û: –î–æ–¥–∞—Ç–∏ PAD —Ç–æ–∫–µ–Ω (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ EOS —è–∫ PAD)
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
        print(f"   ‚úÖ –î–æ–¥–∞–Ω–æ PAD —Ç–æ–∫–µ–Ω: {tokenizer.pad_token}")
        print(f"   üìä –ù–æ–≤–∏–π vocab size: {len(tokenizer)}")
    else:
        print(f"   ‚ÑπÔ∏è  PAD —Ç–æ–∫–µ–Ω –≤–∂–µ —ñ—Å–Ω—É—î: {tokenizer.pad_token}")
    
    # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ instruction datasets
    dataset_files = [config['dataset']['path']]
    if 'additional_datasets' in config['dataset']:
        dataset_files.extend(config['dataset']['additional_datasets'])
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—ñ–≤
    existing_files = []
    for file_path in dataset_files:
        if Path(file_path).exists():
            existing_files.append(file_path)
        else:
            print(f"‚ö†Ô∏è  Dataset –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {file_path}")
    
    if not existing_files:
        print("‚ùå –ü–û–ú–ò–õ–ö–ê: –ñ–æ–¥–µ–Ω instruction dataset –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        sys.exit(1)
    
    print(f"üìö –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏–º—É—Ç—å—Å—è datasets: {len(existing_files)}")
    for f in existing_files:
        print(f"   - {f}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ dataset
    dataset = InstructionDataset(
        data_files=existing_files,
        tokenizer=tokenizer,
        max_seq_len=config['model']['seq_len']
    )
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ dataloader
    dataloader = DataLoader(
        dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=config['cpu_optimization']['num_workers'],
        pin_memory=config['cpu_optimization']['pin_memory']
    )
    
    print(f"üìä Instruction dataset: {len(dataset):,} samples")
    print(f"üì¶ Batches per epoch: {len(dataloader):,}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å: –∞–±–æ –∑ Phase 1, –∞–±–æ –∑ resume checkpoint
    if args.resume:
        logger.info(f"üîÑ Resuming from checkpoint: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        # Rebuild model from Phase1 config first, then load state
        model = load_phase1_model(args.phase1_model if args.phase1_model else "checkpoints/phase1/best_model.pt", config, tokenizer)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=float(config['training']['learning_rate']),
            weight_decay=float(config['training']['weight_decay'])
        )
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        global_step = checkpoint.get('global_step', 0)
        start_epoch = checkpoint.get('epoch', 1)
        resume_batch_idx = checkpoint.get('batch_idx', -1)
        resume_epoch_loss_sum = checkpoint.get('epoch_loss_sum', 0.0)
        resume_epoch_loss_count = checkpoint.get('epoch_loss_count', 0)
        resumed_from_phase2 = True
        print(f"   ‚úÖ Resumed from step {global_step}, epoch {start_epoch}, batch {resume_batch_idx}")
    else:
        logger.info(f"üîÑ Loading Phase 1 model: {args.phase1_model}")
        model = load_phase1_model(args.phase1_model, config, tokenizer)
        model.to(device)
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=float(config['training']['learning_rate']),
            weight_decay=float(config['training']['weight_decay'])
        )
        global_step = 0
        start_epoch = 1
        resumed_from_phase2 = False
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É –¥–ª—è checkpoints
    checkpoint_dir = Path(config['training']['checkpoint_dir'])
    checkpoint_dir.mkdir(exist_ok=True, parents=True)

    save_every_steps = int(config['training'].get('save_every_steps', 50))
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –§–ê–ó–ò 2
    if args.resume is None and config['training'].get('auto_resume', True):
        last_checkpoint = checkpoint_dir / "last_checkpoint.pt"
        if last_checkpoint.exists():
            args.resume = str(last_checkpoint)
            print(f"üîÑ –ó–Ω–∞–π–¥–µ–Ω–æ checkpoint –§–ê–ó–ò 2 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è: {args.resume}")
    
    # –í—ñ–¥–Ω–æ–≤–∏—Ç–∏ –∑ checkpoint –§–ê–ó–ò 2 —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    start_epoch = 1
    resumed_from_phase2 = False
    global_step = 0
    resume_batch_idx = -1
    resume_epoch_loss_sum = 0.0
    resume_epoch_loss_count = 0
    
    if args.resume and Path(args.resume).exists():
        print(f"üîÑ –°–ø—Ä–æ–±–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ checkpoint –§–ê–ó–ò 2: {args.resume}")
        try:
            phase2_checkpoint = torch.load(args.resume, map_location='cpu')
            
            if phase2_checkpoint.get('phase') == 2:
                # –¶–µ checkpoint –§–ê–ó–ò 2 - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ–≤–Ω—ñ—Å—Ç—é
                model.load_state_dict(phase2_checkpoint['model_state_dict'])
                optimizer.load_state_dict(phase2_checkpoint['optimizer_state_dict'])

                train_state = phase2_checkpoint.get('train_state') or {}
                global_step = int(train_state.get('global_step', 0))

                resume_batch_idx = int(train_state.get('batch_idx', -1))
                resume_epoch_loss_sum = float(train_state.get('epoch_loss_sum', 0.0))
                resume_epoch_loss_count = int(train_state.get('epoch_loss_count', 0))

                ckpt_epoch = int(phase2_checkpoint.get('epoch', 1))
                epoch_completed = bool(train_state.get('epoch_completed', False))

                start_epoch = ckpt_epoch + 1 if epoch_completed else ckpt_epoch
                best_loss = phase2_checkpoint['loss']
                resumed_from_phase2 = True

                if epoch_completed:
                    resume_batch_idx = -1
                    resume_epoch_loss_sum = 0.0
                    resume_epoch_loss_count = 0
                
                print(f"   ‚úÖ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ –∑ checkpoint –§–ê–ó–ò 2")
                print(f"   üìä –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ epoch {start_epoch}")
                print(f"   üìâ –ü–æ–ø–µ—Ä–µ–¥–Ω—ñ–π loss: {best_loss:.4f}")
                
                logger.info(f"–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ –§–ê–ó–£ 2 –∑ checkpoint epoch {phase2_checkpoint['epoch']}, loss: {best_loss:.4f}")
            else:
                print(f"   ‚ö†Ô∏è  Checkpoint –Ω–µ —î checkpoint'–æ–º –§–ê–ó–ò 2, —ñ–≥–Ω–æ—Ä—É—î–º–æ")
                args.resume = None
                
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è checkpoint –§–ê–ó–ò 2: {e}")
            print("   üîÑ –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –º–æ–¥–µ–ª—ñ –§–ê–ó–ò 1")
            args.resume = None
    
    # –ù–∞–≤—á–∞–Ω–Ω—è –§–ê–ó–ò 2
    print(f"\nüéØ –ü–æ—á–∞—Ç–æ–∫ instruction tuning –∑ epoch {start_epoch} –¥–æ {config['training']['epochs']}...")
    print("‚ö†Ô∏è  –£–í–ê–ì–ê: –ó—É–ø–∏–Ω–∏–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è —è–∫—â–æ —è–∫—ñ—Å—Ç—å –ø–æ–≥—ñ—Ä—à–∏—Ç—å—Å—è!")
    
    if not resumed_from_phase2:
        best_loss = float('inf')
    patience_counter = 0
    max_patience = config['training'].get('early_stopping_patience', 2)  # –ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    min_improvement = 0.001  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è
    
    print(f"   üìä Early stopping: patience={max_patience}, min_improvement={min_improvement}")
    if resumed_from_phase2:
        print(f"   üîÑ –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è (–≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ –∑ checkpoint –§–ê–ó–ò 2)")
    else:
        print(f"   üÜï –ù–æ–≤–µ instruction tuning (–∑ –º–æ–¥–µ–ª—ñ –§–ê–ó–ò 1)")
    
    for epoch in range(start_epoch, config['training']['epochs'] + 1):
        print(f"\nüìö Phase 2 Epoch {epoch}/{config['training']['epochs']}")

        # –ù–∞–≤—á–∏—Ç–∏ epoch (–∑ autosave —Ç–∞ –¥–µ—Ç–∞–ª—å–Ω–∏–º –ª–æ–≥–æ–º)
        try:
            epoch_loss, global_step = train_epoch(
                model,
                dataloader,
                optimizer,
                tokenizer,
                device,
                epoch,
                logger,
                checkpoint_dir=checkpoint_dir,
                config=config,
                tokenizer_vocab_size=len(tokenizer),
                global_step=global_step,
                save_every_steps=save_every_steps,
                resume_batch_idx=resume_batch_idx if resumed_from_phase2 and epoch == start_epoch else -1,
                resume_epoch_loss_sum=resume_epoch_loss_sum if resumed_from_phase2 and epoch == start_epoch else 0.0,
                resume_epoch_loss_count=resume_epoch_loss_count if resumed_from_phase2 and epoch == start_epoch else 0,
            )
        except KeyboardInterrupt:
            logger.warning("üõë KeyboardInterrupt ‚Äî emergency save")
            try:
                signal.signal(signal.SIGINT, signal.SIG_IGN)
            except Exception:
                pass
            _save_phase2_checkpoint(
                checkpoint_dir / "emergency_checkpoint.pt",
                epoch=epoch,
                model=model,
                optimizer=optimizer,
                loss=float('inf'),
                config=config,
                tokenizer_vocab_size=len(tokenizer),
                global_step=global_step,
                batch_idx=-1,
                epoch_completed=False,
                epoch_loss_sum=0.0,
                epoch_loss_count=0,
                is_emergency=True,
            )
            print("\n‚ö†Ô∏è Phase 2 INTERRUPTED ‚Äî progress saved safely")
            print(f"üìç Resume: {checkpoint_dir / 'last_checkpoint.pt'}")
            try:
                sys.stdout.flush()
                sys.stderr.flush()
            except Exception:
                pass
            sys.exit(130)

        except Exception:
            logger.exception("‚ùå Exception during training ‚Äî emergency save")
            try:
                signal.signal(signal.SIGINT, signal.SIG_IGN)
            except Exception:
                pass
            _save_phase2_checkpoint(
                checkpoint_dir / "emergency_checkpoint.pt",
                epoch=epoch,
                model=model,
                optimizer=optimizer,
                loss=float('inf'),
                config=config,
                tokenizer_vocab_size=len(tokenizer),
                global_step=global_step,
                batch_idx=-1,
                epoch_completed=False,
                epoch_loss_sum=0.0,
                epoch_loss_count=0,
                is_emergency=True,
            )
            print("\n‚ùå Phase 2 STOPPED due to error. Emergency checkpoint saved.")
            print(f"üìç Emergency: {checkpoint_dir / 'emergency_checkpoint.pt'}")
            sys.exit(2)

        # After a successful epoch run, reset resume state for subsequent epochs
        resume_batch_idx = -1
        resume_epoch_loss_sum = 0.0
        resume_epoch_loss_count = 0
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —è–∫—ñ—Å—Ç—å –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º
        improvement = best_loss - epoch_loss
        if improvement > min_improvement:
            best_loss = epoch_loss
            patience_counter = 0
            
            # –ó–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
            checkpoint_path = checkpoint_dir / "best_instruction_model.pt"
            _save_phase2_checkpoint(
                checkpoint_path,
                epoch=epoch,
                model=model,
                optimizer=optimizer,
                loss=epoch_loss,
                config=config,
                tokenizer_vocab_size=len(tokenizer),
                global_step=global_step,
                batch_idx=-1,
                is_emergency=False,
                epoch_loss_sum=0.0,
                epoch_loss_count=0,
            )
            
            print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –Ω–∞–π–∫—Ä–∞—â—É instruction –º–æ–¥–µ–ª—å: {checkpoint_path}")
            print(f"   üìà –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è loss: {improvement:.6f}")
            logger.info(f"–ó–±–µ—Ä–µ–∂–µ–Ω–æ Phase 2 checkpoint –∑ loss: {epoch_loss:.4f}, –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvement:.6f}")
        else:
            patience_counter += 1
            if improvement > 0:
                print(f"‚ö†Ô∏è  –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª–µ ({improvement:.6f} < {min_improvement}). Patience: {patience_counter}/{max_patience}")
            else:
                print(f"‚ö†Ô∏è  Loss –ø–æ–≥—ñ—Ä—à–∏–≤—Å—è –Ω–∞ {-improvement:.6f}. Patience: {patience_counter}/{max_patience}")
            
            if patience_counter >= max_patience:
                print(f"üõë –ó—É–ø–∏–Ω—è—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è - —è–∫—ñ—Å—Ç—å –Ω–µ –ø–æ–∫—Ä–∞—â—É—î—Ç—å—Å—è –¥–æ—Å—Ç–∞—Ç–Ω—å–æ!")
                logger.warning(f"Early stopping - –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –ø—Ä–æ—Ç—è–≥–æ–º {max_patience} epochs")
                break
        
        # –ó–±–µ—Ä–µ–≥—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π checkpoint –¥–ª—è –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è
        last_checkpoint_path = checkpoint_dir / "last_checkpoint.pt"
        _save_phase2_checkpoint(
            last_checkpoint_path,
            epoch=epoch,
            model=model,
            optimizer=optimizer,
            loss=epoch_loss,
            config=config,
            tokenizer_vocab_size=len(tokenizer),
            global_step=global_step,
            batch_idx=-1,
            epoch_completed=True,
            epoch_loss_sum=0.0,
            epoch_loss_count=0,
            is_emergency=False,
        )
    
    print(f"\n‚úÖ –§–ê–ó–ê 2 –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   üèÜ –ù–∞–π–∫—Ä–∞—â–∏–π instruction loss: {best_loss:.4f}")
    print(f"   üíæ –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å: {checkpoint_dir / 'best_instruction_model.pt'}")
    print(f"   üìù –õ–æ–≥–∏: {config['training']['log_dir']}")
    
    print(f"\nüéâ –î–≤–æ—Ñ–∞–∑–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   üìö –§–ê–ó–ê 1: Language pretraining ‚úÖ")
    print(f"   üéØ –§–ê–ó–ê 2: Instruction tuning ‚úÖ")
    print(f"   ü§ñ –ì–æ—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!")
    
    logger.info("–§–ê–ó–ê 2 —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ - –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞!")

if __name__ == "__main__":
    main()
