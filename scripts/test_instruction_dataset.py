#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ InstructionDataset
–ü–µ—Ä–µ–≤—ñ—Ä—è—î –ø—Ä–∞–≤–∏–ª—å–Ω—ñ—Å—Ç—å –º–∞—Å–∫—É–≤–∞–Ω–Ω—è labels –¥–ª—è instruction tuning
"""

import sys
from pathlib import Path
import json

# –î–æ–¥–∞—Ç–∏ project root –¥–æ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_instruction_dataset():
    """–¢–µ—Å—Ç InstructionDataset –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º –º–∞—Å–∫—É–≤–∞–Ω–Ω—è–º"""
    
    try:
        import torch
        from transformers import GPT2Tokenizer
    except ImportError as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–º–ø–æ—Ä—Ç—É: {e}")
        print("   –ê–∫—Ç–∏–≤—É–π—Ç–µ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ: source venv/bin/activate")
        return False
    
    # –Ü–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ InstructionDataset
    sys.path.insert(0, str(project_root / "scripts"))
    from train_phase2_instruction_tuning import InstructionDataset
    
    print("üß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è InstructionDataset...")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ç–µ—Å—Ç–æ–≤–∏–π dataset —Ñ–∞–π–ª
    test_data = [
        {
            "instruction": "Explain what is AI",
            "input": "",
            "output": "AI is artificial intelligence that simulates human thinking."
        },
        {
            "instruction": "Translate to Ukrainian",
            "input": "Hello world",
            "output": "–ü—Ä–∏–≤—ñ—Ç —Å–≤—ñ—Ç"
        }
    ]
    
    test_file = project_root / "test_instruction_data.json"
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    print(f"   üìÑ –°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª: {test_file}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ tokenizer
    print("   üî§ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GPT-2 tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ dataset
    print("   üìö –°—Ç–≤–æ—Ä–µ–Ω–Ω—è InstructionDataset...")
    dataset = InstructionDataset(
        data_files=[str(test_file)],
        tokenizer=tokenizer,
        max_seq_len=64  # –ö–æ—Ä–æ—Ç–∫–∞ –¥–æ–≤–∂–∏–Ω–∞ –¥–ª—è —Ç–µ—Å—Ç—É
    )
    
    print(f"   üìä Dataset —Ä–æ–∑–º—ñ—Ä: {len(dataset)}")
    
    # –¢–µ—Å—Ç—É–≤–∞—Ç–∏ –ø–µ—Ä—à–∏–π sample
    print("\nüîç –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–µ—Ä—à–æ–≥–æ sample...")
    sample = dataset[0]
    
    input_ids = sample['input_ids']
    labels = sample['labels']
    
    print(f"   üìè Input IDs shape: {input_ids.shape}")
    print(f"   üìè Labels shape: {labels.shape}")
    
    # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
    print("\nüìù –î–µ–∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏:")
    
    # –ó–Ω–∞–π—Ç–∏ –¥–µ –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è output (–ø–µ—Ä—à—ñ –Ω–µ-masked labels)
    first_label_idx = None
    for i, label in enumerate(labels):
        if label != -100:
            first_label_idx = i
            break
    
    if first_label_idx is not None:
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç (–º–∞—î –±—É—Ç–∏ –∑–∞–º–∞—Å–∫–æ–≤–∞–Ω–∏–π)
        context_ids = input_ids[:first_label_idx]
        context_text = tokenizer.decode(context_ids, skip_special_tokens=True)
        print(f"   üîí –ö–æ–Ω—Ç–µ–∫—Å—Ç (masked): {context_text}")
        
        # Output (–º–∞—î –Ω–∞–≤—á–∞—Ç–∏—Å—è)
        output_labels = labels[first_label_idx:]
        output_labels_clean = [l for l in output_labels if l != -100]
        if output_labels_clean:
            output_text = tokenizer.decode(output_labels_clean, skip_special_tokens=True)
            print(f"   üéØ Output (supervised): {output_text}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è
        masked_count = sum(1 for l in labels if l == -100)
        supervised_count = sum(1 for l in labels if l != -100 and l != tokenizer.pad_token_id)
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è:")
        print(f"   üîí Masked tokens (–∫–æ–Ω—Ç–µ–∫—Å—Ç): {masked_count}")
        print(f"   üéØ Supervised tokens (output): {supervised_count}")
        print(f"   üìè –ó–∞–≥–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞: {len(labels)}")
        
        if masked_count > 0 and supervised_count > 0:
            print("   ‚úÖ –ú–∞—Å–∫—É–≤–∞–Ω–Ω—è –ø—Ä–∞–≤–∏–ª—å–Ω–µ!")
        else:
            print("   ‚ùå –ü—Ä–æ–±–ª–µ–º–∞ –∑ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è–º!")
            return False
    else:
        print("   ‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ supervised labels!")
        return False
    
    # –û—á–∏—Å—Ç–∏—Ç–∏ —Ç–µ—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª
    test_file.unlink()
    print(f"\nüßπ –í–∏–¥–∞–ª–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤–∏–π —Ñ–∞–π–ª: {test_file}")
    
    print("\n‚úÖ –¢–µ—Å—Ç InstructionDataset –ø—Ä–æ–π—à–æ–≤ —É—Å–ø—ñ—à–Ω–æ!")
    return True

if __name__ == "__main__":
    success = test_instruction_dataset()
    sys.exit(0 if success else 1)


