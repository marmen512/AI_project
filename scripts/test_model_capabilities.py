"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ
"""

import torch
from tiny_recursive_model import TinyRecursiveModel, MLPMixer1D
from tiny_recursive_model.utils import load_tokenizer, tokenize_and_pad, prepare_code_input
import json
from pathlib import Path

def load_model(model_path="ai_assistant_model.pt", dim=256, depth=2, seq_len=512, vocab_size=50257):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–≤—á–µ–Ω—É –º–æ–¥–µ–ª—å"""
    print("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    model = TinyRecursiveModel(
        dim=dim,
        num_tokens=vocab_size,
        network=MLPMixer1D(dim=dim, depth=depth, seq_len=seq_len)
    )
    model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.eval()
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
    return model

def prepare_input(tokenizer, context, query, seq_len=512):
    """–ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥ –¥–ª—è –º–æ–¥–µ–ª—ñ"""
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —É—Ç–∏–ª—ñ—Ç–∏ –∑ tiny_recursive_model
    input_text = prepare_code_input(context, query)
    
    # –û—Ç—Ä–∏–º–∞—Ç–∏ pad_token_id
    pad_token_id = 0
    if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:
        pad_token_id = tokenizer.pad_token_id
    
    # –¢–æ–∫–µ–Ω—ñ–∑—É–≤–∞—Ç–∏ —Ç–∞ –¥–æ–¥–∞—Ç–∏ padding
    input_ids = tokenize_and_pad(
        tokenizer,
        input_text,
        seq_len,
        pad_token_id=pad_token_id,
        truncation=True
    ).unsqueeze(0)
    
    return input_ids

def test_model(model, tokenizer, test_cases, seq_len=512):
    """–¢–µ—Å—Ç—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω–Ω—è—Ö"""
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"–¢–ï–°–¢ {i}: {test_case['name']}")
        print(f"{'='*60}")
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{test_case['context']}")
        print(f"\n–ó–∞–ø–∏—Ç: {test_case['query']}")
        
        try:
            # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥
            input_ids = prepare_input(tokenizer, test_case['context'], test_case['query'], seq_len)
            
            # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
            with torch.no_grad():
                pred_tokens, exit_steps = model.predict(
                    input_ids,
                    max_deep_refinement_steps=12,
                    halt_prob_thres=0.5
                )
            
            # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            output = tokenizer.decode(pred_tokens[0][:200], skip_special_tokens=True)
            steps = exit_steps[0].item() if len(exit_steps) > 0 else 0
            
            print(f"\nüì§ –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ:")
            print(f"{output[:500]}...")  # –ü–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤
            print(f"\nüî¢ –ö—Ä–æ–∫–∏ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: {steps}")
            
            results.append({
                'test': test_case['name'],
                'input': test_case['query'],
                'output': output,
                'steps': steps,
                'success': True
            })
            
        except Exception as e:
            print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            results.append({
                'test': test_case['name'],
                'input': test_case['query'],
                'error': str(e),
                'success': False
            })
    
    return results

def analyze_capabilities(results):
    """–ê–Ω–∞–ª—ñ–∑ –º–æ–∂–ª–∏–≤–æ—Å—Ç–µ–π –º–æ–¥–µ–ª—ñ"""
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–Ü–ó –ú–û–ñ–õ–ò–í–û–°–¢–ï–ô –ú–û–î–ï–õ–Ü")
    print("="*60)
    
    successful = sum(1 for r in results if r.get('success', False))
    total = len(results)
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –£—Å–ø—ñ—à–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤: {successful}/{total}")
    print(f"   –ü–æ–º–∏–ª–æ–∫: {total - successful}")
    
    print(f"\nüîç –ê–Ω–∞–ª—ñ–∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:")
    for result in results:
        if result.get('success'):
            output = result['output']
            # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î –∑–º—ñ—Å—Ç–æ–≤–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            has_code = any(c in output for c in ['def ', 'class ', 'import ', 'return ', '='])
            has_text = len(output.strip()) > 20
            
            print(f"\n   {result['test']}:")
            print(f"      - –ö—Ä–æ–∫–∏: {result['steps']}")
            print(f"      - –ú—ñ—Å—Ç–∏—Ç—å –∫–æ–¥: {'‚úÖ' if has_code else '‚ùå'}")
            print(f"      - –î–æ–≤–∂–∏–Ω–∞: {len(output)} —Å–∏–º–≤–æ–ª—ñ–≤")
        else:
            print(f"\n   {result['test']}: ‚ùå –ü–æ–º–∏–ª–∫–∞ - {result.get('error', 'Unknown')}")

def main():
    # –¢–µ—Å—Ç–æ–≤—ñ –≤–∏–ø–∞–¥–∫–∏
    test_cases = [
        {
            'name': '–ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ—Å—Ç–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó',
            'context': 'def add(a, b):\n    return a + b',
            'query': '–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —Ü—é —Ñ—É–Ω–∫—Ü—ñ—é'
        },
        {
            'name': '–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–æ–¥—É',
            'context': 'def sum_list(numbers):\n    total = 0\n    for num in numbers:\n        total += num\n    return total',
            'query': '–û–ø—Ç–∏–º—ñ–∑—É–π —Ü—é —Ñ—É–Ω–∫—Ü—ñ—é'
        },
        {
            'name': '–î–æ–¥–∞–≤–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ',
            'context': 'class User:\n    def __init__(self, name):\n        self.name = name',
            'query': '–î–æ–¥–∞–π –º–µ—Ç–æ–¥ get_email'
        },
        {
            'name': '–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–æ–∫',
            'context': 'def divide(a, b):\n    return a / b',
            'query': '–î–æ–¥–∞–π –æ–±—Ä–æ–±–∫—É –ø–æ–º–∏–ª–æ–∫'
        },
        {
            'name': '–†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥',
            'context': 'data = [1, 2, 3, 4, 5]\nresult = []\nfor x in data:\n    result.append(x * 2)',
            'query': '–ü–µ—Ä–µ–ø–∏—à–∏ —á–µ—Ä–µ–∑ list comprehension'
        },
        {
            'name': '–°–∫–ª–∞–¥–Ω–∏–π –∑–∞–ø–∏—Ç',
            'context': 'import requests\n\ndef fetch_data(url):\n    response = requests.get(url)\n    return response.json()',
            'query': '–î–æ–¥–∞–π timeout —Ç–∞ –æ–±—Ä–æ–±–∫—É –ø–æ–º–∏–ª–æ–∫'
        },
        {
            'name': '–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è',
            'context': 'def process(items):\n    return [x for x in items if x > 0]',
            'query': '–î–æ–¥–∞–π docstring —Ç–∞ type hints'
        }
    ]
    
    print("="*60)
    print("–¢–ï–°–¢–£–í–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü AI-–ê–°–ò–°–¢–ï–ù–¢–ê")
    print("="*60)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    print("\n1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer, vocab_size, pad_token_id = load_tokenizer("gpt2")
        print(f"‚úÖ GPT-2 tokenizer –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ (vocab_size={vocab_size})")
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è tokenizer: {e}")
        return
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
    print("\n2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    try:
        model = load_model()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        return
    
    # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    print("\n3. –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç—ñ–≤...")
    results = test_model(model, tokenizer, test_cases, seq_len=512)
    
    # –ê–Ω–∞–ª—ñ–∑
    analyze_capabilities(results)
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞–ø–∫—É temp —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
    project_root = Path(__file__).parent.parent
    temp_dir = project_root / "temp"
    temp_dir.mkdir(exist_ok=True, parents=True)
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    results_path = temp_dir / "test_results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {results_path}")
    
    # –ü–ª—é—Å–∏ —Ç–∞ –º—ñ–Ω—É—Å–∏
    print("\n" + "="*60)
    print("–ü–õ–Æ–°–ò –¢–ê –ú–Ü–ù–£–°–ò –ú–û–î–ï–õ–Ü")
    print("="*60)
    
    print("\n‚úÖ –ü–õ–Æ–°–ò:")
    print("   1. –†–µ–∫—É—Ä—Å–∏–≤–Ω–µ —É—Ç–æ—á–Ω–µ–Ω–Ω—è - –º–æ–¥–µ–ª—å –ø–æ–∫—Ä–æ–∫–æ–≤–æ –ø–æ–∫—Ä–∞—â—É—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ")
    print("   2. Early stopping - –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞—î –∫–æ–ª–∏ –∑—É–ø–∏–Ω–∏—Ç–∏—Å—è")
    print("   3. –ö–æ–º–ø–∞–∫—Ç–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ - –≤—ñ–¥–Ω–æ—Å–Ω–æ –º–∞–ª–∞ –º–æ–¥–µ–ª—å")
    print("   4. –ì–Ω—É—á–∫—ñ—Å—Ç—å - –º–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ç–∏–ø–∞–º–∏ –∑–∞–≤–¥–∞–Ω—å")
    print("   5. Latent states - –∑–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–º—ñ–∂–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é")
    
    print("\n‚ùå –ú–Ü–ù–£–°–ò:")
    print("   1. –ú–∞–ª–æ –¥–∞–Ω–∏—Ö - –Ω–∞–≤—á–µ–Ω–∞ —Ç—ñ–ª—å–∫–∏ –Ω–∞ 21 –ø—Ä–∏–∫–ª–∞–¥—ñ")
    print("   2. –û–±–º–µ–∂–µ–Ω–∞ —è–∫—ñ—Å—Ç—å - –ø–æ—Ç—Ä–µ–±—É—î –±—ñ–ª—å—à–µ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö")
    print("   3. –ú–∞–ª–∞ –º–æ–¥–µ–ª—å - dim=256, depth=2 –æ–±–º–µ–∂—É—î –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ")
    print("   4. –ö–æ—Ä–æ—Ç–∫—ñ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ - seq_len=512 –æ–±–º–µ–∂—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç")
    print("   5. –ù–∏–∑—å–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å - –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–æ—é")
    print("   6. –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É - –Ω–µ –ø–∞–º'—è—Ç–∞—î –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö –≤–∑–∞—î–º–æ–¥—ñ–π")
    print("   7. –û–±–º–µ–∂–µ–Ω–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ - –º–æ–∂–µ –Ω–µ —Ä–æ–∑—É–º—ñ—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–ø–∏—Ç–∏")
    
    print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø:")
    print("   1. –ó–±—ñ–ª—å—à–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–æ 1000+ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
    print("   2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±—ñ–ª—å—à—É –º–æ–¥–µ–ª—å (dim=512-1024, depth=4-6)")
    print("   3. –ó–±—ñ–ª—å—à–∏—Ç–∏ seq_len –¥–æ 2048-4096")
    print("   4. –ù–∞–≤—á–∏—Ç–∏ –Ω–∞ –±—ñ–ª—å—à—ñ–π –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –µ–ø–æ—Ö (20+)")
    print("   5. –î–æ–¥–∞—Ç–∏ fine-tuning –Ω–∞ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω–Ω—è—Ö")
    print("   6. –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±—ñ–ª—å—à —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π tokenizer –¥–ª—è –∫–æ–¥—É")
    print("   7. –î–æ–¥–∞—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —ñ—Å—Ç–æ—Ä—ñ—î—é –¥—ñ–∞–ª–æ–≥—ñ–≤")

if __name__ == "__main__":
    main()

