"""
On-device benchmark –¥–ª—è TRM –º–æ–¥–µ–ª—ñ
–†–µ–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏: tokens/sec, latency, memory
"""
import time
import psutil
import torch
from typing import Optional, Any
from pathlib import Path


def benchmark(
    model: torch.nn.Module,
    tokenizer: Any,
    seq: int = 256,
    runs: int = 10,
    device: str = "cpu"
) -> dict:
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç–∏ benchmark –Ω–∞ –º–æ–¥–µ–ª—ñ
    
    Args:
        model: TRM –º–æ–¥–µ–ª—å
        tokenizer: Tokenizer instance
        seq: –î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        runs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø—É—Å–∫—ñ–≤
        device: –ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    print(f"üî¨ –ó–∞–ø—É—Å–∫ benchmark: seq={seq}, runs={runs}, device={device}")
    
    # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ
    test_text = "hello world " * 10
    tokens = tokenizer.encode(test_text)[:seq]
    tokens_tensor = torch.tensor([tokens], device=device)
    
    # –ü–µ—Ä–µ–º—ñ—Å—Ç–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ device
    model = model.to(device)
    model.eval()
    
    # –í–∏–º—ñ—Ä—è—Ç–∏ –ø–∞–º'—è—Ç—å –¥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    process = psutil.Process()
    memory_before = process.memory_info().rss / 1e9  # GB
    
    # Warmup
    print("   Warmup...")
    with torch.no_grad():
        for _ in range(3):
            _ = model(tokens_tensor)
    
    # Benchmark
    print("   Benchmarking...")
    times = []
    with torch.no_grad():
        for i in range(runs):
            t0 = time.time()
            output = model(tokens_tensor)
            elapsed = time.time() - t0
            times.append(elapsed)
            if (i + 1) % 5 == 0:
                print(f"   Run {i+1}/{runs}: {elapsed:.4f}s")
    
    # –í–∏–º—ñ—Ä—è—Ç–∏ –ø–∞–º'—è—Ç—å –ø—ñ—Å–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    memory_after = process.memory_info().rss / 1e9  # GB
    memory_used = memory_after - memory_before
    
    # –û–±—á–∏—Å–ª–∏—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
    avg_latency = sum(times) / len(times)
    min_latency = min(times)
    max_latency = max(times)
    tokens_per_sec = seq / avg_latency
    
    results = {
        'avg_latency': avg_latency,
        'min_latency': min_latency,
        'max_latency': max_latency,
        'tokens_per_sec': tokens_per_sec,
        'memory_used_gb': memory_used,
        'memory_before_gb': memory_before,
        'memory_after_gb': memory_after,
        'runs': runs,
        'seq_len': seq,
    }
    
    # –í–∏–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print("\n" + "=" * 60)
    print("üìä BENCHMARK RESULTS")
    print("=" * 60)
    print(f"   Avg latency: {avg_latency:.4f}s")
    print(f"   Min latency: {min_latency:.4f}s")
    print(f"   Max latency: {max_latency:.4f}s")
    print(f"   Tokens/sec: {tokens_per_sec:.2f}")
    print(f"   Memory used: {memory_used:.2f} GB")
    print(f"   Memory total: {memory_after:.2f} GB")
    print("=" * 60)
    
    return results


if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø—É—Å–∫
    from tiny_recursive_model.utils import load_tokenizer
    from train.model_factory import create_model
    
    print("üî¨ –¢–µ—Å—Ç–æ–≤–∏–π benchmark")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ tokenizer
    tokenizer, vocab_size, _ = load_tokenizer("gpt2")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
    model = create_model(
        dim=256,
        vocab_size=vocab_size,
        depth=4,
        seq_len=256
    )
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç–∏ benchmark
    results = benchmark(model, tokenizer, seq=256, runs=10)
    print(f"\n‚úÖ Benchmark –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {results['tokens_per_sec']:.2f} tokens/sec")

