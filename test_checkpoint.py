#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∏–π —Ç–µ—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —á–µ–∫–ø–æ—ñ–Ω—Ç—É
"""
import sys
import torch
from pathlib import Path
import json

# –î–æ–¥–∞—Ç–∏ —à–ª—è—Ö –¥–æ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from tiny_recursive_model import TinyRecursiveModel, TransformerBackbone
from tiny_recursive_model.utils import load_tokenizer
from train.constants import DEFAULT_TOKENIZER_NAME

def test_checkpoint(checkpoint_path: str):
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—É"""
    print("=" * 60)
    print("üß™ –¢–ï–°–¢–£–í–ê–ù–ù–Ø –ß–ï–ö–ü–û–Ü–ù–¢–£")
    print("=" * 60)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —á–µ–∫–ø–æ—ñ–Ω—Ç
    print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—É: {checkpoint_path}")
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        print(f"‚úÖ –ß–µ–∫–ø–æ—ñ–Ω—Ç –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")
        
        # –ü–æ–∫–∞–∑–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —á–µ–∫–ø–æ—ñ–Ω—Ç
        print(f"\nüìä –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ —á–µ–∫–ø–æ—ñ–Ω—Ç:")
        if 'epoch' in checkpoint:
            print(f"   –ï–ø–æ—Ö–∞: {checkpoint['epoch']}")
        if 'step' in checkpoint:
            print(f"   –ö—Ä–æ–∫: {checkpoint['step']}")
        if 'loss' in checkpoint:
            print(f"   Loss: {checkpoint['loss']:.4f}")
        if 'best_loss' in checkpoint:
            print(f"   –ù–∞–π–∫—Ä–∞—â–∏–π loss: {checkpoint['best_loss']:.4f}")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª—ñ
        if 'model_state_dict' in checkpoint:
            model_state = checkpoint['model_state_dict']
            print(f"\nüèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ:")
            for key in list(model_state.keys())[:10]:  # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–µ—Ä—à—ñ 10 –∫–ª—é—á—ñ–≤
                print(f"   {key}: {model_state[key].shape}")
            if len(model_state.keys()) > 10:
                print(f"   ... —Ç–∞ —â–µ {len(model_state.keys()) - 10} –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")
        
        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        if 'config' in checkpoint:
            config = checkpoint['config']
            print(f"\n‚öôÔ∏è –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ:")
            print(f"   dim: {config.get('dim', 'N/A')}")
            print(f"   depth: {config.get('depth', 'N/A')}")
            print(f"   seq_len: {config.get('seq_len', 'N/A')}")
            print(f"   vocab_size: {config.get('vocab_size', 'N/A')}")
            
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —Å—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
            try:
                print(f"\nüîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
                
                # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
                tokenizer, _, _ = load_tokenizer(DEFAULT_TOKENIZER_NAME)
                vocab_size = len(tokenizer)
                
                # –°—Ç–≤–æ—Ä–∏—Ç–∏ backbone
                if config.get('use_transformer', False):
                    backbone = TransformerBackbone(
                        dim=config['dim'],
                        depth=config['depth'],
                        seq_len=config['seq_len'],
                        vocab_size=vocab_size,
                        transformer_model=config.get('transformer_model', 'gpt2')
                    )
                else:
                    from tiny_recursive_model import MLPMixer1D
                    backbone = MLPMixer1D(
                        dim=config['dim'],
                        depth=config['depth'],
                        seq_len=config['seq_len']
                    )
                
                # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
                model = TinyRecursiveModel(
                    dim=config['dim'],
                    num_tokens=vocab_size,
                    network=backbone,
                    max_recursion_depth=config.get('max_recursion_depth', 20),
                    adaptive_recursion=config.get('adaptive_recursion', True)
                )
                
                # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–∞–Ω –º–æ–¥–µ–ª—ñ
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å—Ç–≤–æ—Ä–µ–Ω–∞ —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
                
                # –ü—Ä–æ—Å—Ç–∏–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
                print(f"\nüß™ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó...")
                test_input = "–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?"
                
                # –¢–æ–∫–µ–Ω—ñ–∑—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥
                inputs = tokenizer.encode(test_input, return_tensors='pt')
                
                # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è
                with torch.no_grad():
                    outputs = model.generate(
                        inputs,
                        max_length=inputs.shape[1] + 20,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"   –í—Ö—ñ–¥: {test_input}")
                print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {generated_text}")
                
                print(f"\n‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π—à–æ–≤ —É—Å–ø—ñ—à–Ω–æ!")
                
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
                import traceback
                traceback.print_exc()
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—É: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # –¢–µ—Å—Ç—É–≤–∞—Ç–∏ –Ω–∞–π–∫—Ä–∞—â–∏–π —á–µ–∫–ø–æ—ñ–Ω—Ç
    checkpoint_path = "checkpoints/best_loss.ckpt"
    if Path(checkpoint_path).exists():
        test_checkpoint(checkpoint_path)
    else:
        print(f"‚ùå –ß–µ–∫–ø–æ—ñ–Ω—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {checkpoint_path}")
        print("–î–æ—Å—Ç—É–ø–Ω—ñ —á–µ–∫–ø–æ—ñ–Ω—Ç–∏:")
        checkpoints_dir = Path("checkpoints")
        if checkpoints_dir.exists():
            for ckpt in checkpoints_dir.glob("*.ckpt"):
                print(f"   {ckpt.name}")
