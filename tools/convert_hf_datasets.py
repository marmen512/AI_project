#!/usr/bin/env python3
"""
–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä HuggingFace –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ —É –≤–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π JSON —Ñ–æ—Ä–º–∞—Ç
–ö–æ–Ω–≤–µ—Ä—Ç—É—î: Alpaca, DailyDialog, Simple Wikipedia, SQuAD v1, SQuAD v2
"""

import json
from pathlib import Path
from typing import List, Dict, Any
import sys

def install_datasets():
    """–í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ datasets —è–∫—â–æ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ"""
    try:
        import datasets
        print("‚úÖ datasets –≤–∂–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        return True
    except ImportError:
        print("üì¶ –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ datasets...")
        import subprocess
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "datasets"])
            print("‚úÖ datasets –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è datasets: {e}")
            return False

def save_dataset(name: str, dtype: str, data: List[Dict], out_dir: Path):
    """–ó–±–µ—Ä–µ–≥—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç —É –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ"""
    out = {
        "metadata": {
            "name": name,
            "type": dtype,
            "source": "huggingface",
            "size": len(data),
            "description": f"Converted from HuggingFace dataset"
        },
        "data": data
    }
    
    output_file = out_dir / f"{name}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ {name}.json —Å—Ç–≤–æ—Ä–µ–Ω–æ ({len(data):,} –∑–∞–ø–∏—Å—ñ–≤)")
    return output_file

def convert_alpaca(out_dir: Path):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ Alpaca dataset"""
    print("\nüìö –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Alpaca...")
    from datasets import load_dataset
    
    try:
        alpaca = load_dataset("tatsu-lab/alpaca", split="train")
        print(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(alpaca):,} –∑–∞–ø–∏—Å—ñ–≤")
        
        alpaca_data = []
        for x in alpaca:
            # –û–±—Ä–æ–±–∏—Ç–∏ input - –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—Ä–æ–∂–Ω—ñ–º
            input_text = x.get("input", "").strip()
            
            alpaca_data.append({
                "instruction": x["instruction"].strip(),
                "input": input_text,
                "output": x["output"].strip()
            })
        
        return save_dataset("alpaca", "instruction", alpaca_data, out_dir)
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Alpaca: {e}")
        return None

def convert_dailydialog(out_dir: Path):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ DailyDialog dataset"""
    print("\nüí¨ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è DailyDialog...")
    from datasets import load_dataset
    
    try:
        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        dd = load_dataset("daily_dialog", split="train")
        print(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(dd):,} –¥—ñ–∞–ª–æ–≥—ñ–≤")
        
        dd_data = []
        for dialog in dd:
            utterances = dialog["dialog"]
            # –°—Ç–≤–æ—Ä–∏—Ç–∏ –ø–∞—Ä–∏ –∑–∞–ø–∏—Ç-–≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –¥—ñ–∞–ª–æ–≥—É
            for i in range(len(utterances) - 1):
                if utterances[i].strip() and utterances[i + 1].strip():
                    dd_data.append({
                        "instruction": utterances[i].strip(),
                        "input": "",
                        "output": utterances[i + 1].strip()
                    })
        
        return save_dataset("dailydialog", "dialog", dd_data, out_dir)
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è DailyDialog: {e}")
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –¥—ñ–∞–ª–æ–≥ –¥–∞—Ç–∞—Å–µ—Ç
        print("   –°—Ç–≤–æ—Ä—é—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –¥—ñ–∞–ª–æ–≥ –¥–∞—Ç–∞—Å–µ—Ç...")
        minimal_data = [
            {"instruction": "Hello", "input": "", "output": "Hi there! How can I help you?"},
            {"instruction": "How are you?", "input": "", "output": "I'm doing well, thank you for asking!"},
            {"instruction": "What's your name?", "input": "", "output": "I'm an AI assistant here to help you."},
            {"instruction": "Goodbye", "input": "", "output": "Goodbye! Have a great day!"}
        ]
        return save_dataset("dailydialog_minimal", "dialog", minimal_data, out_dir)

def convert_simple_wikipedia(out_dir: Path):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ Simple Wikipedia dataset"""
    print("\nüìñ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Simple Wikipedia...")
    from datasets import load_dataset
    
    try:
        wiki = load_dataset("rahular/simple-wikipedia", split="train")
        print(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(wiki):,} —Å—Ç–∞—Ç–µ–π")
        
        wiki_data = []
        for i, x in enumerate(wiki):
            if i >= 10000:  # –û–±–º–µ–∂–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                break
                
            text = x["text"].strip()
            
            if text and len(text) > 100:  # –§—ñ–ª—å—Ç—Ä –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤
                # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø–µ—Ä—à—ñ —Å–ª–æ–≤–∞ —è–∫ "title"
                words = text.split()
                if len(words) > 5:
                    title = " ".join(words[:3])  # –ü–µ—Ä—à—ñ 3 —Å–ª–æ–≤–∞ —è–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    content = text[:1500]  # –û–±–º–µ–∂–∏—Ç–∏ –¥–æ–≤–∂–∏–Ω—É
                    
                    wiki_data.append({
                        "instruction": f"Explain: {title}",
                        "input": "",
                        "output": content
                    })
        
        return save_dataset("simple_wiki", "knowledge", wiki_data, out_dir)
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Simple Wikipedia: {e}")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ñ –ø–æ–ª—è: {list(wiki[0].keys()) if len(wiki) > 0 else '–Ω–µ–º–∞—î –¥–∞–Ω–∏—Ö'}")
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π knowledge –¥–∞—Ç–∞—Å–µ—Ç
        print("   –°—Ç–≤–æ—Ä—é—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π knowledge –¥–∞—Ç–∞—Å–µ—Ç...")
        minimal_data = [
            {"instruction": "Explain: Artificial Intelligence", "input": "", "output": "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans."},
            {"instruction": "Explain: Machine Learning", "input": "", "output": "Machine Learning is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed."},
            {"instruction": "Explain: Neural Networks", "input": "", "output": "Neural Networks are computing systems inspired by biological neural networks that constitute animal brains."},
            {"instruction": "Explain: Deep Learning", "input": "", "output": "Deep Learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns."}
        ]
        return save_dataset("simple_wiki_minimal", "knowledge", minimal_data, out_dir)

def convert_squad_v1(out_dir: Path):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ SQuAD v1.1 dataset"""
    print("\n‚ùì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è SQuAD v1.1...")
    from datasets import load_dataset
    
    try:
        squad = load_dataset("rajpurkar/squad", split="train")
        print(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(squad):,} –ø–∏—Ç–∞–Ω—å")
        
        squad_data = []
        for x in squad:
            question = x["question"].strip()
            context = x["context"].strip()
            answers = x["answers"]["text"]
            
            if question and context and answers:
                squad_data.append({
                    "instruction": question,
                    "input": context,
                    "output": answers[0].strip()  # –ü–µ—Ä—à–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
                })
        
        return save_dataset("squad", "qa", squad_data, out_dir)
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è SQuAD v1: {e}")
        return None

def convert_squad_v2(out_dir: Path):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ SQuAD v2.0 dataset"""
    print("\n‚ùì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è SQuAD v2.0...")
    from datasets import load_dataset
    
    try:
        squad2 = load_dataset("rajpurkar/squad_v2", split="train")
        print(f"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(squad2):,} –ø–∏—Ç–∞–Ω—å")
        
        squad2_data = []
        for x in squad2:
            question = x["question"].strip()
            context = x["context"].strip()
            answers = x["answers"]["text"]
            
            # –¢—ñ–ª—å–∫–∏ –ø–∏—Ç–∞–Ω–Ω—è –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏ (–Ω–µ impossible)
            if question and context and answers:
                squad2_data.append({
                    "instruction": question,
                    "input": context,
                    "output": answers[0].strip()
                })
        
        return save_dataset("squad_v2", "qa", squad2_data, out_dir)
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è SQuAD v2: {e}")
        return None

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó"""
    print("=" * 80)
    print("üîÑ –£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ò–ô –ö–û–ù–í–ï–†–¢–ï–† HF ‚Üí JSON")
    print("=" * 80)
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏/–≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ datasets
    if not install_datasets():
        return
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –≤–∏—Ö—ñ–¥–Ω—É –ø–∞–ø–∫—É
    out_dir = Path("datasets")
    out_dir.mkdir(exist_ok=True)
    
    print(f"\nüìÅ –í–∏—Ö—ñ–¥–Ω–∞ –ø–∞–ø–∫–∞: {out_dir.absolute()}")
    
    # –°–ø–∏—Å–æ–∫ –∫–æ–Ω–≤–µ—Ä—Ç–æ—Ä—ñ–≤
    converters = [
        ("Alpaca", convert_alpaca),
        ("DailyDialog", convert_dailydialog),
        ("Simple Wikipedia", convert_simple_wikipedia),
        ("SQuAD v1.1", convert_squad_v1),
        ("SQuAD v2.0", convert_squad_v2)
    ]
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤—Å—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
    converted = []
    failed = []
    
    for name, converter in converters:
        try:
            result = converter(out_dir)
            if result:
                converted.append((name, result))
            else:
                failed.append(name)
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –≤ {name}: {e}")
            failed.append(name)
    
    # –ü—ñ–¥—Å—É–º–æ–∫
    print("\n" + "=" * 80)
    print("üìä –ü–Ü–î–°–£–ú–û–ö –ö–û–ù–í–ï–†–¢–ê–¶–Ü–á")
    print("=" * 80)
    
    if converted:
        print(f"\n‚úÖ –£—Å–ø—ñ—à–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–æ ({len(converted)}):")
        total_records = 0
        for name, filepath in converted:
            # –ü—Ä–æ—á–∏—Ç–∞—Ç–∏ —Ä–æ–∑–º—ñ—Ä
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                size = len(data['data'])
                total_records += size
                print(f"   üìÑ {filepath.name} - {size:,} –∑–∞–ø–∏—Å—ñ–≤")
        
        print(f"\nüìà –ó–∞–≥–∞–ª–æ–º: {total_records:,} –∑–∞–ø–∏—Å—ñ–≤")
    
    if failed:
        print(f"\n‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∫–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ ({len(failed)}):")
        for name in failed:
            print(f"   ‚ö†Ô∏è {name}")
    
    if converted:
        print(f"\nüéØ –ù–ê–°–¢–£–ü–ù–Ü –ö–†–û–ö–ò:")
        print(f"   1. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Ñ–∞–π–ª–∏ –≤ –ø–∞–ø—Ü—ñ datasets/")
        print(f"   2. –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è: ./start_training.sh")
        print(f"   3. –ú–æ–Ω—ñ—Ç–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å: python scripts/check_training_status.py")
    
    print("\n‚ú® –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()
