"""
–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è TRM –º–æ–¥–µ–ª—ñ
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–æ–¥—É–ª—å–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é

‚ö†Ô∏è DEPRECATED: –Ø–∫ –ø—Ä—è–º–∏–π entry point
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ: scripts/train_model.py ‚Üí runtime.bootstrap

–§—É–Ω–∫—Ü—ñ—è train_with_auto_config() –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –¥–ª—è backwards compatibility,
–∞–ª–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ runtime.bootstrap –Ω–∞–ø—Ä—è–º—É.
"""
import torch
from pathlib import Path
from typing import Optional, Tuple, TYPE_CHECKING

# Lazy import to avoid circular dependency
if TYPE_CHECKING:
    from tiny_recursive_model import TinyRecursiveModel
from config import GGUFModelManager
from config.trm_config import TRMConfig
# DEPRECATED: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ runtime.resume –∑–∞–º—ñ—Å—Ç—å config.training_resume
from runtime.resume import find_latest_checkpoint, get_checkpoint_info
# Backwards compatibility
try:
    from config.training_resume import TrainingResume
except ImportError:
    TrainingResume = None  # Fallback
from data import DatasetManager
from tiny_recursive_model.utils import load_tokenizer
from train.constants import (
    DEFAULT_DIM, DEFAULT_DEPTH, DEFAULT_SEQ_LEN,
    DEFAULT_NUM_REFINEMENT_BLOCKS, DEFAULT_NUM_LATENT_REFINEMENTS,
    DEFAULT_CHECKPOINT_INTERVAL, DEFAULT_TOKENIZER_NAME,
    DEFAULT_EPOCHS, DEFAULT_BATCH_SIZE, DEFAULT_LEARNING_RATE,
    DEFAULT_HALT_PROB_THRES, DEFAULT_HALT_LOSS_WEIGHT,
    DEFAULT_CHECKPOINT_DIR
)
from train.model_factory import create_model, get_model_config_dict
from train.trainer_factory import create_trainer
from train.curriculum import CurriculumScheduler
from train.metrics import TRMTrainingLogger
from rag import build_rag, RAGDatasetWrapper
from train.training_utils import (
    resolve_dataset_path, handle_duplicate_training,
    handle_resume_checkpoint, save_model_and_config,
    get_final_loss_from_checkpoint
)


def train_with_auto_config(
    dataset_name_or_path: Optional[str] = None,
    model_save_path: Optional[str] = None,
    auto_config: bool = True,
    epochs: Optional[int] = None,
    batch_size: Optional[int] = None,
    learning_rate: Optional[float] = None,
    use_gpu: Optional[bool] = None,
    checkpoint_dir: str = DEFAULT_CHECKPOINT_DIR,
    checkpoint_interval: int = DEFAULT_CHECKPOINT_INTERVAL,
    resume_from_checkpoint: Optional[str] = None,
    **kwargs
) -> Tuple[Optional['TinyRecursiveModel'], Optional[object], Optional[Path]]:
    """
    –ù–∞–≤—á–∞—Ç–∏ TRM –º–æ–¥–µ–ª—å –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é
    
    Args:
        dataset_name_or_path: –Ü–º'—è –¥–∞—Ç–∞—Å–µ—Ç—É –∞–±–æ —à–ª—è—Ö –¥–æ –Ω—å–æ–≥–æ
        model_save_path: –ö—É–¥–∏ –∑–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å (None = trained_models/auto_model.pt)
        auto_config: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
        epochs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
        batch_size: –†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
        learning_rate: Learning rate (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
        use_gpu: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ GPU (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏)
        checkpoint_dir: –ü–∞–ø–∫–∞ –¥–ª—è checkpoint'—ñ–≤
        checkpoint_interval: –ö–æ–∂–Ω—ñ —Å–∫—ñ–ª—å–∫–∏ –±–∞—Ç—á—ñ–≤ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ checkpoint
        **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    """
    print("=" * 70)
    print("üöÄ –ù–ê–í–ß–ê–ù–ù–Ø TRM –ú–û–î–ï–õ–Ü")
    print("=" * 70)
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU –∑ –ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
    if use_gpu is None:
        use_gpu = torch.cuda.is_available()
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å GPU –ø–µ—Ä–µ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º
    if use_gpu:
        print(f"\nüîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ GPU...")
        try:
            # –¢–µ—Å—Ç–æ–≤–∏–π —Ç–µ–Ω–∑–æ—Ä –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
            test_tensor = torch.randn(2, 2).cuda()
            _ = test_tensor + 1  # –ü—Ä–æ—Å—Ç–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è
            # –¢–µ—Å—Ç embedding –æ–ø–µ—Ä–∞—Ü—ñ—ó (—è–∫–∞ –≤–∏–∫–ª–∏–∫–∞—î –ø–æ–º–∏–ª–∫—É)
            test_embed = torch.nn.Embedding(10, 5).cuda()
            test_input = torch.randint(0, 10, (2, 3)).cuda()
            _ = test_embed(test_input)  # –¶—è –æ–ø–µ—Ä–∞—Ü—ñ—è –≤–∏–∫–ª–∏–∫–∞—î –ø–æ–º–∏–ª–∫—É –Ω–∞ –Ω–µ—Å—É–º—ñ—Å–Ω–∏—Ö GPU
            del test_tensor, test_embed, test_input
            torch.cuda.empty_cache()
            print(f"   ‚úÖ GPU —Å—É–º—ñ—Å–Ω–∏–π: {torch.cuda.get_device_name(0)}")
        except RuntimeError as e:
            error_str = str(e)
            if "HIP error" in error_str or "invalid device function" in error_str:
                print(f"   ‚ö†Ô∏è  GPU –Ω–µ —Å—É–º—ñ—Å–Ω–∏–π –∑ –ø–æ—Ç–æ—á–Ω–æ—é –≤–µ—Ä—Å—ñ—î—é PyTorch/ROCm")
                print(f"   üí° –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—î–º–æ—Å—è –Ω–∞ CPU")
                use_gpu = False
            else:
                raise
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤—ñ—Ä—Ü—ñ GPU: {e}")
            print(f"   üí° –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—î–º–æ—Å—è –Ω–∞ CPU")
            use_gpu = False
    
    # –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è
    training_resume = TrainingResume(checkpoint_dir=checkpoint_dir)
    
    # –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
    dataset_manager = DatasetManager()
    
    # –ó–Ω–∞–π—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
    dataset_path = resolve_dataset_path(dataset_name_or_path, dataset_manager)
    print(f"\nüìö –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–∞—Ç–∞—Å–µ—Ç: {dataset_path.name}")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    if auto_config:
        print("\n‚öôÔ∏è  –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è...")
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ TRMConfig –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        training_config = TRMConfig.from_dataset(
            dataset_path,
            auto_detect=True,
            dim=kwargs.get('dim', DEFAULT_DIM),
            depth=kwargs.get('depth', DEFAULT_DEPTH),
            seq_len=kwargs.get('seq_len', DEFAULT_SEQ_LEN),
            epochs=epochs or None,
            batch_size=batch_size or None,
            learning_rate=learning_rate or None
        )
        training_config.print_summary(dataset_path)
        
        # –ü–µ—Ä–µ–≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —è–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ –≤—Ä—É—á–Ω—É
        if epochs is not None:
            training_config.epochs = epochs
        if batch_size is not None:
            training_config.batch_size = batch_size
        if learning_rate is not None:
            training_config.learning_rate = learning_rate
    else:
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        training_config = TRMConfig(
            dim=kwargs.get('dim', DEFAULT_DIM),
            depth=kwargs.get('depth', DEFAULT_DEPTH),
            seq_len=kwargs.get('seq_len', DEFAULT_SEQ_LEN),
            epochs=epochs or DEFAULT_EPOCHS,
            batch_size=batch_size or DEFAULT_BATCH_SIZE,
            learning_rate=learning_rate or DEFAULT_LEARNING_RATE,
            max_recurrent_steps=kwargs.get('max_recurrent_steps', 12),
            halt_prob_thres=kwargs.get('halt_prob_thres', DEFAULT_HALT_PROB_THRES)
        )
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    print(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞...")
    tokenizer, vocab_size, pad_token_id = load_tokenizer(DEFAULT_TOKENIZER_NAME)
    if tokenizer is None:
        raise ValueError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä: {DEFAULT_TOKENIZER_NAME}")
    print(f"   ‚úÖ Vocab size: {vocab_size}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
    print(f"\nüìö –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É...")
    from train.datasets.trm_dataset import TRMDataset
    from train.dataset_utils import split_dataset
    seq_len = kwargs.get('seq_len', DEFAULT_SEQ_LEN)
    full_dataset = TRMDataset(
        data_path=str(dataset_path),
        tokenizer=tokenizer,
        max_seq_len=seq_len,
        pad_token_id=pad_token_id,
        cache_size=kwargs.get('cache_size', 1000),
        validate_format=kwargs.get('validate_format', True)
    )
    print(f"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç: {len(full_dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
    
    # –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –Ω–∞ train/validation
    train_ratio = kwargs.get('train_ratio', 0.9)
    train_dataset, val_dataset = split_dataset(full_dataset, train_ratio=train_ratio)
    print(f"   üìä Train: {len(train_dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤, Validation: {len(val_dataset)} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
    dataset = train_dataset  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ train_dataset –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
    
    # –î–æ–¥–∞—Ç–∏ RAG —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
    rag_config = kwargs.get('rag', None)
    if rag_config and rag_config.get('enabled', False):
        print(f"\nüß† –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è RAG...")
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
        # –î–ª—è –ø–æ—á–∞—Ç–∫—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ —Ç–µ–∫—Å—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É
        documents = []
        try:
            # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –≤–∏—Ç—è–≥–Ω—É—Ç–∏ —Ç–µ–∫—Å—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É (–ø–µ—Ä—à—ñ 1000 –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ)
            sample_size = min(1000, len(dataset))
            for i in range(sample_size):
                item = dataset[i]
                if isinstance(item, dict):
                    text = item.get('input', item.get('context', ''))
                    if text:
                        documents.append(text)
                elif isinstance(item, tuple) and len(item) > 0:
                    if isinstance(item[0], str):
                        documents.append(item[0])
        except Exception as e:
            print(f"   ‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥–Ω—É—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç—É: {e}")
        
        if documents:
            rag_retriever = build_rag(rag_config, documents)
            dataset = RAGDatasetWrapper(dataset, rag_retriever, k=rag_config.get('k', 5))
            print(f"   ‚úÖ RAG —É–≤—ñ–º–∫–Ω–µ–Ω–æ, –¥–∞—Ç–∞—Å–µ—Ç –æ–±–≥–æ—Ä–Ω—É—Ç–æ –≤ RAGDatasetWrapper")
        else:
            print(f"   ‚ö†Ô∏è RAG —É–≤—ñ–º–∫–Ω–µ–Ω–æ, –∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –±–µ–∑ RAG.")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ
    dim = kwargs.get('dim', DEFAULT_DIM)
    depth = kwargs.get('depth', DEFAULT_DEPTH)
    
    # –û—Ç—Ä–∏–º–∞—Ç–∏ teacher –º–æ–¥–µ–ª—å –∑ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—É
    dataset_metadata = training_resume.get_dataset_metadata(dataset_path)
    teacher_model_path = dataset_metadata.get('teacher_model_path') or dataset_metadata.get('teacher_model_name')
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –º–æ–¥–µ–ª—ñ
    model_config = get_model_config_dict(
        dim=dim,
        vocab_size=vocab_size,
        seq_len=seq_len,
        depth=depth,
        num_refinement_blocks=kwargs.get('num_refinement_blocks', DEFAULT_NUM_REFINEMENT_BLOCKS),
        num_latent_refinements=kwargs.get('num_latent_refinements', DEFAULT_NUM_LATENT_REFINEMENTS),
        max_recursion_depth=kwargs.get('max_recursion_depth', getattr(training_config, 'max_recursion_depth', 20)),
        adaptive_recursion=kwargs.get('adaptive_recursion', getattr(training_config, 'adaptive_recursion', False)),
        timeout_seconds=kwargs.get('timeout_seconds', None),
        thinking_cost_weight=kwargs.get('thinking_cost_weight', getattr(training_config, 'thinking_cost_weight', 0.01))
    )
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ –Ω–µ –Ω–∞–≤—á–∞—î–º–æ—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ –Ω–∞ —Ç–æ–º—É —Å–∞–º–æ–º—É
    auto_resume = kwargs.get('auto_resume', True)  # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏
    is_cancelled, reason, previous_training = handle_duplicate_training(
        training_resume, dataset_path, model_config, teacher_model_path, auto_resume=auto_resume
    )
    if is_cancelled:
        return None, None, None
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ –±—É–¥–µ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –∞–±–æ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –ø—ñ—Å–ª—è –µ–∫—Å—Ç—Ä–µ–Ω–æ–≥–æ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è
    # –Ø–∫—â–æ checkpoint –≤–∫–∞–∑–∞–Ω–æ —è–≤–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –π–æ–≥–æ, —ñ–Ω–∞–∫—à–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏
    if resume_from_checkpoint is None:
        resume_from_checkpoint = handle_resume_checkpoint(
            training_resume, previous_training, teacher_model_path, 
            dataset_path=dataset_path, model_config=model_config,
            auto_resume=auto_resume
        )
    elif resume_from_checkpoint and not Path(resume_from_checkpoint).exists():
        print(f"‚ö†Ô∏è  Checkpoint –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {resume_from_checkpoint}")
        print("   –ë—É–¥–µ —Ä–æ–∑–ø–æ—á–∞—Ç–æ –Ω–æ–≤–µ –Ω–∞–≤—á–∞–Ω–Ω—è")
        resume_from_checkpoint = None
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
    print(f"\nüèóÔ∏è  –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    model = create_model(
        dim=dim,
        vocab_size=vocab_size,
        depth=depth,
        seq_len=seq_len,
        num_refinement_blocks=kwargs.get('num_refinement_blocks', DEFAULT_NUM_REFINEMENT_BLOCKS),
        num_latent_refinements=kwargs.get('num_latent_refinements', DEFAULT_NUM_LATENT_REFINEMENTS),
        halt_loss_weight=kwargs.get('halt_loss_weight', DEFAULT_HALT_LOSS_WEIGHT),
        max_recursion_depth=kwargs.get('max_recursion_depth', getattr(training_config, 'max_recursion_depth', 20)),
        adaptive_recursion=kwargs.get('adaptive_recursion', getattr(training_config, 'adaptive_recursion', False)),
        timeout_seconds=kwargs.get('timeout_seconds', None),
        thinking_cost_weight=kwargs.get('thinking_cost_weight', getattr(training_config, 'thinking_cost_weight', 0.01))
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {total_params:,}")
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —à–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    if model_save_path is None:
        trained_models_dir = Path(__file__).parent.parent / "models" / "trained"
        trained_models_dir.mkdir(parents=True, exist_ok=True)
        model_save_path = trained_models_dir / f"trm_{Path(dataset_path).stem}.pt"
    else:
        model_save_path = Path(model_save_path)
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ curriculum scheduler —è–∫—â–æ —É–≤—ñ–º–∫–Ω–µ–Ω–æ
    curriculum_scheduler = None
    curriculum_config = kwargs.get('curriculum', None)
    if curriculum_config and curriculum_config.get('enabled', False):
        stages = curriculum_config.get('stages', [])
        if stages:
            curriculum_scheduler = CurriculumScheduler(stages)
            print(f"\nüìö –°—Ç–≤–æ—Ä–µ–Ω–æ CurriculumScheduler –∑ {len(stages)} –µ—Ç–∞–ø–∞–º–∏")
            print(f"   {curriculum_scheduler.describe()}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ training logger –¥–ª—è –º–µ—Ç—Ä–∏–∫
    log_dir = kwargs.get('log_dir', 'logs')
    training_logger = TRMTrainingLogger(Path(log_dir) / 'training_metrics.jsonl')
    print(f"\nüìä –°—Ç–≤–æ—Ä–µ–Ω–æ TRMTrainingLogger: {Path(log_dir) / 'training_metrics.jsonl'}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ trainer
    print(f"\nüéì –°—Ç–≤–æ—Ä–µ–Ω–Ω—è trainer...")
    trainer = create_trainer(
        model=model,
        dataset=dataset,
        learning_rate=training_config.learning_rate,
        batch_size=training_config.batch_size,
        epochs=training_config.epochs,
        max_recurrent_steps=training_config.max_recurrent_steps,
        use_gpu=use_gpu,
        checkpoint_dir=checkpoint_dir,
        checkpoint_interval=checkpoint_interval,
        gradient_accumulation_steps=training_config.gradient_accumulation_steps,
        warmup_steps=training_config.warmup_steps,
        halt_prob_thres=training_config.halt_prob_thres,
        mixed_precision=kwargs.get('mixed_precision', None),
        curriculum_scheduler=curriculum_scheduler,
        training_logger=training_logger
    )
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    print(f"\nüöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è...")
    print("\n" + "=" * 70)
    print("üìã –ü–ê–†–ê–ú–ï–¢–†–ò –ù–ê–í–ß–ê–ù–ù–Ø:")
    print("=" * 70)
    print(f"   üìö –î–∞—Ç–∞—Å–µ—Ç: {dataset_path.name}")
    print(f"   üéØ –ú–æ–¥–µ–ª—å:")
    print(f"      - dim: {dim}")
    print(f"      - depth: {depth}")
    print(f"      - seq_len: {seq_len}")
    print(f"      - vocab_size: {vocab_size}")
    print(f"      - num_refinement_blocks: {kwargs.get('num_refinement_blocks', DEFAULT_NUM_REFINEMENT_BLOCKS)}")
    print(f"      - num_latent_refinements: {kwargs.get('num_latent_refinements', DEFAULT_NUM_LATENT_REFINEMENTS)}")
    print(f"   üéì –ù–∞–≤—á–∞–Ω–Ω—è:")
    print(f"      - epochs: {training_config.epochs}")
    print(f"      - batch_size: {training_config.batch_size}")
    print(f"      - learning_rate: {training_config.learning_rate}")
    print(f"      - gradient_accumulation_steps: {training_config.gradient_accumulation_steps}")
    print(f"      - warmup_steps: {training_config.warmup_steps}")
    print(f"      - max_recurrent_steps: {training_config.max_recurrent_steps}")
    print(f"      - halt_prob_thres: {training_config.halt_prob_thres}")
    print(f"      - halt_loss_weight: {kwargs.get('halt_loss_weight', DEFAULT_HALT_LOSS_WEIGHT)}")
    print(f"   üíæ Checkpoint:")
    print(f"      - checkpoint_dir: {checkpoint_dir}")
    print(f"      - checkpoint_interval: {checkpoint_interval}")
    print(f"   üîß –Ü–Ω—à—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
    print(f"      - use_gpu: {use_gpu}")
    print(f"      - mixed_precision: {kwargs.get('mixed_precision', 'None')}")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   üìä –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {total_params:,}")
    print("=" * 70)
    if resume_from_checkpoint:
        print(f"\n   ‚èÆÔ∏è  –ü—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –∑ checkpoint: {Path(resume_from_checkpoint).name}")
    print("-" * 70)
    
    # –ù–∞–≤—á–∞–Ω–Ω—è –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º fallback –Ω–∞ CPU
    final_loss = None
    try:
        trainer(resume_from_checkpoint=resume_from_checkpoint)
        final_loss = get_final_loss_from_checkpoint(checkpoint_dir)
    except RuntimeError as e:
        error_str = str(e)
        if use_gpu and ("HIP error" in error_str or "invalid device function" in error_str):
            print(f"\n‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ GPU: {error_str}")
            print(f"üí° –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ CPU...")
            print("-" * 70)
            
            # –ü–µ—Ä–µ–º—ñ—Å—Ç–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ CPU
            model = model.cpu()
            torch.cuda.empty_cache()
            
            # –ü–µ—Ä–µ—Å—Ç–≤–æ—Ä–∏—Ç–∏ trainer –∑ CPU
            trainer = create_trainer(
                model=model,
                dataset=train_dataset,
                learning_rate=training_config.learning_rate,
                batch_size=training_config.batch_size,
                epochs=training_config.epochs,
                max_recurrent_steps=training_config.max_recurrent_steps,
                use_gpu=False,  # –ü—Ä–∏–º—É—Å–æ–≤–æ CPU
                checkpoint_dir=checkpoint_dir,
                checkpoint_interval=checkpoint_interval,
                gradient_accumulation_steps=training_config.gradient_accumulation_steps,
                warmup_steps=training_config.warmup_steps,
                halt_prob_thres=training_config.halt_prob_thres,
                mixed_precision=None  # –ë–µ–∑ mixed precision –Ω–∞ CPU
            )
            
            # –û–Ω–æ–≤–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
            use_gpu = False
            print(f"\nüîÑ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –æ–Ω–æ–≤–ª–µ–Ω–æ:")
            print(f"   - use_gpu: {use_gpu} (CPU)")
            print(f"   - mixed_precision: None")
            print("-" * 70)
            
            # –ü–æ–≤—Ç–æ—Ä–Ω–∏–π –∑–∞–ø—É—Å–∫ –Ω–∞ CPU
            try:
                trainer(resume_from_checkpoint=resume_from_checkpoint)
                final_loss = get_final_loss_from_checkpoint(checkpoint_dir)
            except Exception as e2:
                print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ CPU: {e2}")
                import traceback
                traceback.print_exc()
        else:
            print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {e}")
            import traceback
            traceback.print_exc()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ù–∞–≤—á–∞–Ω–Ω—è –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {e}")
        import traceback
        traceback.print_exc()
    
    print("-" * 70)
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ –∑–∞–ø–∏—Å –ø—Ä–æ –Ω–∞–≤—á–∞–Ω–Ω—è
    if final_loss is not None:
        try:
            latest_checkpoint = Path(checkpoint_dir) / "checkpoint_latest.pt"
            training_resume.save_training_record(
                dataset_path,
                model_config,
                final_loss,
                latest_checkpoint if latest_checkpoint.exists() else None,
                teacher_model_path=teacher_model_path
            )
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –∑–∞–ø–∏—Å –ø—Ä–æ –Ω–∞–≤—á–∞–Ω–Ω—è: {e}")
            import traceback
            traceback.print_exc()
    
    # –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å (—Ç—ñ–ª—å–∫–∏ –Ω–∞ –≥–æ–ª–æ–≤–Ω–æ–º—É –ø—Ä–æ—Ü–µ—Å—ñ)
    print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    if trainer.accelerator.is_main_process:
        try:
            save_model_and_config(model, model_save_path, model_config, training_config)
            print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {model_save_path}")
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 70)
    print("‚úÖ –ù–ê–í–ß–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 70)
    
    return model, tokenizer, model_save_path


def main():
    """CLI –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ù–∞–≤—á–∞–Ω–Ω—è TRM –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--dataset", type=str, default=None,
                       help="–Ü–º'—è –¥–∞—Ç–∞—Å–µ—Ç—É –∞–±–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É")
    parser.add_argument("--save", type=str, default=None,
                       help="–®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--no-auto-config", action="store_true",
                       help="–ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é")
    parser.add_argument("--epochs", type=int, default=None,
                       help="–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="–†–æ–∑–º—ñ—Ä –±–∞—Ç—á—É")
    parser.add_argument("--lr", type=float, default=None,
                       help="Learning rate")
    parser.add_argument("--dim", type=int, default=512,
                       help="–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--depth", type=int, default=4,
                       help="–ì–ª–∏–±–∏–Ω–∞ MLP Mixer")
    parser.add_argument("--seq-len", type=int, default=2048,
                       help="–î–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ")
    parser.add_argument("--cpu", action="store_true",
                       help="–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ CPU")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints",
                       help="–ü–∞–ø–∫–∞ –¥–ª—è checkpoint'—ñ–≤")
    
    args = parser.parse_args()
    
    train_with_auto_config(
        dataset_name_or_path=args.dataset,
        model_save_path=args.save,
        auto_config=not args.no_auto_config,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        use_gpu=not args.cpu,
        checkpoint_dir=args.checkpoint_dir,
        dim=args.dim,
        depth=args.depth,
        seq_len=args.seq_len
    )


if __name__ == "__main__":
    main()

