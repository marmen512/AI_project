"""
–£—Ç–∏–ª—ñ—Ç–∏ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
"""
from pathlib import Path
from typing import Optional, Tuple, Dict, Any, Union
import torch

from train.exceptions import DatasetNotFoundError
from train.constants import DEFAULT_SEQ_LEN
from data import DatasetManager
# DEPRECATED: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ runtime.resume –∑–∞–º—ñ—Å—Ç—å config.training_resume
from runtime.resume import find_latest_checkpoint, get_checkpoint_info
# Backwards compatibility
try:
    from config.training_resume import TrainingResume
except ImportError:
    TrainingResume = None  # Fallback


def resolve_dataset_path(
    dataset_name_or_path: Optional[str],
    dataset_manager: DatasetManager
) -> Path:
    """
    –í–∏—Ä—ñ—à–∏—Ç–∏ —à–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    
    Args:
        dataset_name_or_path: –Ü–º'—è –¥–∞—Ç–∞—Å–µ—Ç—É –∞–±–æ —à–ª—è—Ö
        dataset_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
    
    Returns:
        –®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
    
    Raises:
        DatasetNotFoundError: –Ø–∫—â–æ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ
    """
    if dataset_name_or_path is None:
        datasets = dataset_manager.list_datasets()
        if not datasets:
            raise DatasetNotFoundError("", "–î–∞—Ç–∞—Å–µ—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –î–æ–¥–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤ –ø–∞–ø–∫—É datasets/")
        return Path(datasets[0]['path'])
    
    if Path(dataset_name_or_path).exists():
        return Path(dataset_name_or_path)
    
    dataset = dataset_manager.get_dataset(dataset_name_or_path)
    if not dataset:
        raise DatasetNotFoundError(dataset_name_or_path)
    return Path(dataset['path'])


def handle_duplicate_training(
    training_resume: TrainingResume,
    dataset_path: Path,
    model_config: Dict[str, Any],
    teacher_model_path: Optional[str],
    auto_resume: bool = False
) -> Tuple[bool, Optional[str], Optional[Dict]]:
    """
    –û–±—Ä–æ–±–∏—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –¥—É–±–ª—é–≤–∞–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è
    
    Returns:
        (is_cancelled, reason, previous_training_info)
    """
    is_duplicate, reason, previous_training = training_resume.check_duplicate_training(
        dataset_path,
        model_config,
        teacher_model_path=teacher_model_path
    )
    
    if is_duplicate:
        print(f"\n‚ö†Ô∏è  –£–í–ê–ì–ê: {reason}")
        print("   –ú–æ–¥–µ–ª—å –≤–∂–µ –Ω–∞–≤—á–µ–Ω–∞ –Ω–∞ —Ü—å–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ –∑ —Ç–∞–∫–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é —Ç–∞ teacher –º–æ–¥–µ–ª–ª—é.")
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏ —è–∫—â–æ —Ü–µ –Ω–µ —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º
        import sys
        if auto_resume or not sys.stdin.isatty():
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —Ä–µ–∂–∏–º - –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏ –∑–∞–≤–∂–¥–∏
            response = 'y'
            print("   –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è (–Ω–µ—ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º)")
        else:
            response = input("   –ü—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è? (y/n): ").strip().lower()
        if response != 'y':
            print("   –ù–∞–≤—á–∞–Ω–Ω—è —Å–∫–∞—Å–æ–≤–∞–Ω–æ")
            return True, None, None
    
    return False, reason, previous_training


def handle_resume_checkpoint(
    training_resume: TrainingResume,
    previous_training: Optional[Dict],
    teacher_model_path: Optional[str] = None,
    dataset_path: Optional[Path] = None,
    model_config: Optional[Dict] = None,
    auto_resume: bool = False
) -> Optional[str]:
    """
    –û–±—Ä–æ–±–∏—Ç–∏ –ª–æ–≥—ñ–∫—É –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –∑ checkpoint
    
    Returns:
        –®–ª—è—Ö –¥–æ checkpoint'—É –∞–±–æ None
    """
    resume_from_checkpoint = None
    
    if previous_training:
        print(f"\nüìö –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø: –ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è")
        print(f"   –ü–æ–ø–µ—Ä–µ–¥–Ω—î –Ω–∞–≤—á–∞–Ω–Ω—è: teacher –º–æ–¥–µ–ª—å '{previous_training.get('teacher_model', '–Ω–µ–≤—ñ–¥–æ–º–æ')}'")
        print(f"   –ü–æ—Ç–æ—á–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è: teacher –º–æ–¥–µ–ª—å '{Path(teacher_model_path).stem if teacher_model_path else '–Ω–µ–≤—ñ–¥–æ–º–æ'}'")
        print(f"   ‚Üí –ë—É–¥–µ –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –≤—ñ–¥ –Ω–æ–≤–æ—ó teacher –º–æ–¥–µ–ª—ñ")
        
        if previous_training.get('checkpoint_path'):
            prev_checkpoint = Path(previous_training['checkpoint_path'])
            if prev_checkpoint.exists():
                print(f"   üí° –ó–Ω–∞–π–¥–µ–Ω–æ checkpoint –≤—ñ–¥ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è")
                import sys
                if auto_resume or not sys.stdin.isatty():
                    response = 'y'
                    print("   –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ checkpoint (–Ω–µ—ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º)")
                else:
                    response = input("   –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π checkpoint —è–∫ –±–∞–∑—É –¥–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è? (y/n): ").strip().lower()
                if response == 'y':
                    resume_from_checkpoint = str(prev_checkpoint)
                    print(f"   ‚úÖ –ë—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π checkpoint –¥–ª—è –¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è")
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î checkpoint –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –ø—ñ—Å–ª—è –µ–∫—Å—Ç—Ä–µ–Ω–æ–≥–æ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è
    if resume_from_checkpoint is None:
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —î –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è (–µ–∫—Å—Ç—Ä–µ–Ω–µ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è)
        should_resume, resume_checkpoint, checkpoint_info = training_resume.should_resume(
            dataset_path=dataset_path,
            model_config=model_config
        )
        
        if should_resume and resume_checkpoint and checkpoint_info:
            # –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
            is_final = checkpoint_info.get('is_final', False)
            epoch = checkpoint_info.get('epoch', 0)
            batch_count = checkpoint_info.get('batch_count', 0)
            total_epochs = checkpoint_info.get('epochs', 0)
            loss = checkpoint_info.get('loss', None)
            
            print(f"\nüîÑ –ó–ù–ê–ô–î–ï–ù–û –ù–ï–ó–ê–í–ï–†–®–ï–ù–ï –ù–ê–í–ß–ê–ù–ù–Ø (–µ–∫—Å—Ç—Ä–µ–Ω–µ –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è):")
            print(f"   üìç –ï–ø–æ—Ö–∞: {epoch}/{total_epochs if total_epochs > 0 else '?'}")
            print(f"   üìä –ë–∞—Ç—á—ñ–≤ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {batch_count}")
            if loss is not None:
                print(f"   üìâ –û—Å—Ç–∞–Ω–Ω—ñ–π loss: {loss:.6f}")
            print(f"   üíæ Checkpoint: {Path(resume_checkpoint).name}")
            print(f"   ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è...")
            
            # –ó–∞–≤–∂–¥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—Ä–æ–¥–æ–≤–∂—É–≤–∞—Ç–∏ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
            resume_from_checkpoint = str(resume_checkpoint)
    
    return resume_from_checkpoint


def save_model_and_config(
    model: Any,  # TinyRecursiveModel (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Any —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ñ—á–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏)
    model_save_path: Union[str, Path],
    model_config: Dict[str, Any],
    training_config: Any  # TrainingConfig (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Any —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ñ—á–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏)
) -> None:
    """
    –ó–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å —Ç–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    
    Args:
        model: –ù–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å
        model_save_path: –®–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        model_config: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
        training_config: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
    """
    import json
    
    # –ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ Path —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    model_save_path = Path(model_save_path)
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —è–∫—â–æ –Ω–µ —ñ—Å–Ω—É—î
    model_save_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        torch.save(model.state_dict(), model_save_path)
    except Exception as e:
        raise IOError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –º–æ–¥–µ–ª—å: {e}") from e
    
    config_path = Path(str(model_save_path).replace('.pt', '_config.json'))
    
    # –î–æ–¥–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ç–∏–ø backbone
    backbone_type = 'unknown'
    if hasattr(model, 'network'):
        if hasattr(model.network, '__class__'):
            if 'Transformer' in model.network.__class__.__name__:
                backbone_type = 'transformer'
            elif 'MLPMixer' in model.network.__class__.__name__:
                backbone_type = 'mlpmixer'
    
    full_config = {
        **model_config,
        'backbone_type': backbone_type,  # –î–æ–¥–∞—Ç–∏ —Ç–∏–ø backbone
        'training_config': training_config.to_dict() if hasattr(training_config, 'to_dict') else training_config
    }
    
    try:
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(full_config, f, indent=2, ensure_ascii=False)
    except Exception as e:
        raise IOError(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é: {e}") from e
    
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {model_save_path}")
    print(f"   ‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {config_path}")


def get_final_loss_from_checkpoint(checkpoint_dir: str) -> Optional[float]:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ–π loss –∑ checkpoint'—É
    
    Args:
        checkpoint_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∑ checkpoint'–∞–º–∏
    
    Returns:
        Loss –∞–±–æ None
    """
    latest_checkpoint = Path(checkpoint_dir) / "checkpoint_latest.pt"
    if latest_checkpoint.exists():
        try:
            checkpoint = torch.load(latest_checkpoint, map_location='cpu')
            return checkpoint.get('loss', checkpoint.get('final_loss', None))
        except Exception as e:
            # –õ–æ–≥—É–≤–∞—Ç–∏ –ø–æ–º–∏–ª–∫—É, –∞–ª–µ –Ω–µ –ø–µ—Ä–µ—Ä–∏–≤–∞—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ loss –∑ checkpoint: {e}")
    return None

