# üìã –ó–í–Ü–¢ –í–ò–ü–†–ê–í–õ–ï–ù–¨ –ü–†–û–ï–ö–¢–£

**–î–∞—Ç–∞:** 31 –≥—Ä—É–¥–Ω—è 2025  
**–ü—Ä–æ–µ–∫—Ç:** –î–≤–æ—Ñ–∞–∑–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ—ó Transformer –º–æ–¥–µ–ª—ñ (TRM)  
**–ê–Ω–∞–ª—ñ—Ç–∏–∫:** AI Assistant  

---

## üéØ –ú–ï–¢–ê –ê–ù–ê–õ–Ü–ó–£

–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Ç–∞ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –ø—Ä–æ–µ–∫—Ç –¥–≤–æ—Ñ–∞–∑–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ—ó Transformer –º–æ–¥–µ–ª—ñ –∑ –Ω—É–ª—è –Ω–∞ CPU-only –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –≤–∏–º–æ–≥:

- Custom Transformer –∑ random initialization (~15-25M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤)
- –§–ê–ó–ê 1: Language Pretraining –Ω–∞ Simple Wikipedia
- –§–ê–ó–ê 2: Instruction Tuning –Ω–∞ Alpaca/SQuAD/DailyDialog
- CPU-–±–µ–∑–ø–µ—á–Ω—ñ—Å—Ç—å –¥–ª—è Ryzen 5 3600
- –ë–µ–∑–ø–µ—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è

---

## üîç –ü–†–û–í–ï–î–ï–ù–ò–ô –ê–ù–ê–õ–Ü–ó

### ‚úÖ –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ—Å—Ç—å:

1. **–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª—ñ**
   - Custom Transformer –∑ random initialization ‚úÖ
   - GPT-2 –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –±–µ–∑ pretrained weights ‚úÖ
   - ~15-25M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (6.8M –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó) ‚úÖ
   - Sequence length ‚â§ 256 ‚úÖ

2. **–†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Ñ–∞–∑**
   - –§–ê–ó–ê 1: Plain text corpus (Simple Wikipedia) ‚úÖ
   - –§–ê–ó–ê 2: Instruction datasets (Alpaca, SQuAD, DailyDialog) ‚úÖ
   - –ü—Ä–∞–≤–∏–ª—å–Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è objectives ‚úÖ

3. **CPU-–æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è**
   - –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è Ryzen 5 3600 (6 threads) ‚úÖ
   - –ú–∞–ª—ñ batch sizes (1-4) ‚úÖ
   - Gradient accumulation ‚úÖ

4. **–¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä**
   - GPT-2 BPE –±–µ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è ‚úÖ
   - –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ –æ–±–æ—Ö —Ñ–∞–∑–∞—Ö ‚úÖ

---

## ‚ö†Ô∏è –í–ò–Ø–í–õ–ï–ù–Ü –¢–ê –í–ò–ü–†–ê–í–õ–ï–ù–Ü –ü–†–û–ë–õ–ï–ú–ò

### 1. üîß –ù–ï–ü–û–í–ù–ê –§–£–ù–ö–¶–Ü–û–ù–ê–õ–¨–ù–Ü–°–¢–¨ –í–Ü–î–ù–û–í–õ–ï–ù–ù–Ø

**–§–∞–π–ª:** `scripts/train_phase1_pretraining.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ü–∞—Ä–∞–º–µ—Ç—Ä `--resume` —ñ—Å–Ω—É–≤–∞–≤, –∞–ª–µ –ª–æ–≥—ñ–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–µ –±—É–ª–∞ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞
- –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è

**–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:**
```python
# –î–æ–¥–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è
if args.resume is None and config['training'].get('auto_resume', True):
    last_checkpoint = checkpoint_dir / "last_checkpoint.pt"
    if last_checkpoint.exists():
        args.resume = str(last_checkpoint)

# –î–æ–¥–∞–Ω–æ –ø–æ–≤–Ω—É –ª–æ–≥—ñ–∫—É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è checkpoint
if args.resume:
    checkpoint = torch.load(args.resume, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    best_loss = checkpoint['loss']
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ü–æ–≤–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–∞–≤—á–∞–Ω–Ω—è
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è Ctrl+C
- ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞–Ω—É –º–æ–¥–µ–ª—ñ, –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞, epoch, loss

---

### 2. üîß –ü–†–û–ë–õ–ï–ú–ê –ó PAD –¢–û–ö–ï–ù–û–ú –î–õ–Ø –§–ê–ó–ò 2

**–§–∞–π–ª:** `scripts/train_phase2_instruction_tuning.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è EOS —Ç–æ–∫–µ–Ω–∞ —è–∫ PADüî¥ –°–£–¢–¨ –ü–û–ú–ò–õ–ö–ò (–û–î–ù–û–ó–ù–ê–ß–ù–û)

–ö–ª—é—á–æ–≤–∏–π —Ä—è–¥–æ–∫ –∑ traceback:

TypeError: '<=' not supported between instances of 'float' and 'str'


—ñ –º—ñ—Å—Ü–µ:

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=config['training']['learning_rate'],
    weight_decay=config['training']['weight_decay']
)


üëâ learning_rate –∑ –∫–æ–Ω—Ñ—ñ–≥–∞ ‚Äî –†–Ø–î–û–ö (str), –∞ –Ω–µ —á–∏—Å–ª–æ (float)

PyTorch —Ä–æ–±–∏—Ç—å –ø–µ—Ä–µ–≤—ñ—Ä–∫—É:

if not 0.0 <= lr:


—ñ –ø–∞–¥–∞—î, –±–æ:

0.0 <= "3e-4"   ‚ùå

üß† –ß–û–ú–£ –¶–ï –°–¢–ê–õ–û–°–Ø

–£ config/phase1_pretraining.yaml —É —Ç–µ–±–µ, –º–∞–π–∂–µ –Ω–∞–ø–µ–≤–Ω–æ, —â–æ—Å—å —Ç–∏–ø—É:

training:
  learning_rate: "3e-4"
  weight_decay: "0.01"


–ê–±–æ:

learning_rate: 3e-4   # –∞–ª–µ –ø–∞—Ä—Å–µ—Ä –ø—Ä–æ—á–∏—Ç–∞–≤ —è–∫ string


YAML —ñ–Ω–æ–¥—ñ —á–∏—Ç–∞—î scientific notation —è–∫ string, –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –ø–∞—Ä—Å–µ—Ä–∞.

‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ï –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø (–ó–†–û–ë–ò –ó–ê–†–ê–ó)
üîß –í–ê–†–Ü–ê–ù–¢ 1 ‚Äî –í–ò–ü–†–ê–í–ò–¢–ò YAML (–†–ï–ö–û–ú–ï–ù–î–£–Æ)

–í—ñ–¥–∫—Ä–∏–π —Ñ–∞–π–ª:

nano config/phase1_pretraining.yaml


–Ü –∑—Ä–æ–±–∏ –Ø–í–ù–û float:

training:
  learning_rate: 0.0003
  weight_decay: 0.01


‚ùó –ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π 3e-4, —Ç—ñ–ª—å–∫–∏ –¥–µ—Å—è—Ç–∫–æ–≤–∏–π —Ñ–æ—Ä–º–∞—Ç.

–ó–±–µ—Ä–µ–∂–∏ —Ñ–∞–π–ª.

üîß –í–ê–†–Ü–ê–ù–¢ 2 ‚Äî –ó–ê–•–ò–°–¢ –í –ö–û–î–Ü (–î–£–ñ–ï –ö–û–†–ò–°–ù–û)

–©–æ–± –Ω—ñ–∫–æ–ª–∏ –±—ñ–ª—å—à–µ –Ω–µ –ª–æ–≤–∏—Ç–∏ —Ç–∞–∫–µ, –¥–æ–¥–∞–π –∫–∞—Å—Ç —É train_phase1_pretraining.py
–ø–µ—Ä–µ–¥ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞.

–ó–Ω–∞–π–¥–∏ –º—ñ—Å—Ü–µ –ø–µ—Ä–µ–¥ AdamW —ñ –¥–æ–¥–∞–π:

lr = float(config['training']['learning_rate'])
wd = float(config['training']['weight_decay'])

optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=lr,
    weight_decay=wd
)


üëâ –Ø –Ω–∞–ø–æ–ª–µ–≥–ª–∏–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∑—Ä–æ–±–∏—Ç–∏ –Ü –í–ê–†–Ü–ê–ù–¢ 1, –Ü –í–ê–†–Ü–ê–ù–¢ 2.

‚ö†Ô∏è –©–ï –î–í–ê –í–ê–ñ–õ–ò–í–Ü –ú–û–ú–ï–ù–¢–ò (–ù–ï –ë–õ–û–ö–ï–†–ò, –ê–õ–ï –í–ê–†–¢–û –ó–ù–ê–¢–ò)
1Ô∏è‚É£ –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è tokenizer (–ù–ï –ö–†–ò–¢–ò–ß–ù–û)
Token indices sequence length is longer than the specified maximum sequence length


üëâ –¶–ï –ù–û–†–ú–ê–õ–¨–ù–û.

–¢–∏:

—Ç–æ–∫–µ–Ω—ñ–∑—É–≤–∞–≤ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–¥–Ω–∏–º —à–º–∞—Ç–∫–æ–º

–ø–æ—Ç—ñ–º —Ä—ñ–∂–µ—à –Ω–∞ seq_len=256

–¶–µ warning –≤—ñ–¥ tokenizer, –ù–ï –ø–æ–º–∏–ª–∫–∞.
–ú–æ–∂–Ω–∞ —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏.

(–ü–æ—Ç—ñ–º, —è–∫—â–æ –∑–∞—Ö–æ—á–µ—à, –º–æ–∂–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏, –∞–ª–µ –∑–∞—Ä–∞–∑ ‚Äî –Ω—ñ)

2Ô∏è‚É£ –†–µ–∞–ª—å–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ = 51M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ ‚ö†Ô∏è
üìä –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: 51,082,752
üß† –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: 8 layers, 512 dim


–¶–µ –±—ñ–ª—å—à–µ, –Ω—ñ–∂ –º–∏ –ø–ª–∞–Ω—É–≤–∞–ª–∏ (15‚Äì25M).

üëâ –ù–∞ CPU —Ü–µ –±—É–¥–µ –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ, –∞–ª–µ:

‚ùå —Ü–µ –ù–ï –ø—Ä–∏—á–∏–Ω–∞ –∫—Ä–∞—à—É

‚ùå —Ü–µ –ù–ï —Ç–µ—Ä–º—ñ–Ω–æ–≤–æ –∑–∞—Ä–∞–∑

‚úÖ –º–æ–¥–µ–ª—å –≤—Å–µ –æ–¥–Ω–æ —Å—Ç–∞—Ä—Ç—É—î

–ú–æ—è –ø–æ—Ä–∞–¥–∞:
—Å–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç–∏, –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Å—è —â–æ loss –ø–∞–¥–∞—î,
–ø–æ—Ç—ñ–º, —è–∫—â–æ –±—É–¥–µ –¥—É–∂–µ –ø–æ–≤—ñ–ª—å–Ω–æ ‚Äî –∑–º–µ–Ω—à–∏–º–æ.
# –î–æ–¥–∞–Ω–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è embeddings
def load_phase1_model(checkpoint_path: str, config, tokenizer):
    # ... –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ...
    
    # –†–æ–∑—à–∏—Ä–∏—Ç–∏ embeddings —è–∫—â–æ –¥–æ–¥–∞–Ω–æ PAD —Ç–æ–∫–µ–Ω
    current_vocab_size = len(tokenizer)
    if current_vocab_size > original_vocab_size:
        # –†–æ–∑—à–∏—Ä–∏—Ç–∏ input embeddings
        old_embeddings = model.transformer.wte.weight.data
        new_embeddings = torch.zeros(current_vocab_size, model.config.n_embd)
        new_embeddings[:original_vocab_size] = old_embeddings
        new_embeddings[original_vocab_size:] = old_embeddings.mean(dim=0, keepdim=True)
        
        # –û–Ω–æ–≤–∏—Ç–∏ –º–æ–¥–µ–ª—å
        model.transformer.wte = nn.Embedding(current_vocab_size, model.config.n_embd)
        model.transformer.wte.weight.data = new_embeddings
        
        # –ê–Ω–∞–ª–æ–≥—ñ—á–Ω–æ –¥–ª—è output layer
        # ...
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –û–∫—Ä–µ–º–∏–π PAD —Ç–æ–∫–µ–Ω `<|pad|>`
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–µ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è embeddings –∑ 50257 –¥–æ 50258
- ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è vocab_size –≤ checkpoint'–∞—Ö

---

### 3. üîß –ü–û–ö–†–ê–©–ï–ù–ù–Ø EARLY STOPPING

**–§–∞–π–ª:** `scripts/train_phase2_instruction_tuning.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ü—Ä–∏–º—ñ—Ç–∏–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ early stopping
- –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥—É –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è

**–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:**
```python
# –î–æ–¥–∞–Ω–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—ñ–≥ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
min_improvement = 0.001
max_patience = config['training'].get('early_stopping_patience', 2)

# –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —è–∫–æ—Å—Ç—ñ
improvement = best_loss - epoch_loss
if improvement > min_improvement:
    best_loss = epoch_loss
    patience_counter = 0
    # –ó–±–µ—Ä–µ–≥—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
else:
    patience_counter += 1
    if patience_counter >= max_patience:
        print(f"üõë –ó—É–ø–∏–Ω—è—î–º–æ –Ω–∞–≤—á–∞–Ω–Ω—è - —è–∫—ñ—Å—Ç—å –Ω–µ –ø–æ–∫—Ä–∞—â—É—î—Ç—å—Å—è –¥–æ—Å—Ç–∞—Ç–Ω—å–æ!")
        break
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π –ø–æ—Ä—ñ–≥ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è (0.001)
- ‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–æ–≤–∞–Ω–∏–π patience
- ‚úÖ –î–µ—Ç–∞–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è –ø—Ä–∏—á–∏–Ω –∑—É–ø–∏–Ω–∫–∏
- ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ checkpoint

---

### 4. üîß –î–û–î–ê–ù–û –í–Ü–î–ù–û–í–õ–ï–ù–ù–Ø –î–õ–Ø –§–ê–ó–ò 2

**–§–∞–π–ª:** `scripts/train_phase2_instruction_tuning.py`

**–î–æ–¥–∞–Ω–æ:**
```python
# –ù–æ–≤–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä
parser.add_argument("--resume", type=str, default=None,
                   help="–®–ª—è—Ö –¥–æ checkpoint –§–ê–ó–ò 2 –¥–ª—è –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è")

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –§–ê–ó–ò 2
if args.resume is None and config['training'].get('auto_resume', True):
    last_checkpoint = checkpoint_dir / "last_checkpoint.pt"
    if last_checkpoint.exists():
        args.resume = str(last_checkpoint)

# –õ–æ–≥—ñ–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ checkpoint –§–ê–ó–ò 2
if args.resume and Path(args.resume).exists():
    phase2_checkpoint = torch.load(args.resume, map_location='cpu')
    if phase2_checkpoint.get('phase') == 2:
        model.load_state_dict(phase2_checkpoint['model_state_dict'])
        optimizer.load_state_dict(phase2_checkpoint['optimizer_state_dict'])
        start_epoch = phase2_checkpoint['epoch'] + 1
        best_loss = phase2_checkpoint['loss']
        resumed_from_phase2 = True
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ü–æ–≤–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –§–ê–ó–ò 2
- ‚úÖ –†–æ–∑—Ä—ñ–∑–Ω–µ–Ω–Ω—è checkpoint'—ñ–≤ –§–ê–ó–ò 1 —Ç–∞ –§–ê–ó–ò 2

---

### 5. üîß –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø –ü–ï–†–ï–í–Ü–†–ö–ò DATASET'–£

**–§–∞–π–ª–∏:** `start_phase1_pretraining.sh`, `start_phase2_instruction_tuning.sh`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- `wc -l` –ø–æ–∫–∞–∑—É–≤–∞–≤ 0 —Ä—è–¥–∫—ñ–≤ –¥–ª—è plain text —Ñ–∞–π–ª—É (85MB)
- Plain text —Ñ–∞–π–ª —Å—Ç–≤–æ—Ä–µ–Ω–∏–π —è–∫ –æ–¥–∏–Ω –¥–æ–≤–≥–∏–π —Ä—è–¥–æ–∫ –±–µ–∑ newlines (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–ª—è language modeling)
- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä—è–¥–∫—ñ–≤ –Ω–µ –º–∞—î —Å–µ–Ω—Å—É –¥–ª—è —Ç–∞–∫–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É

**–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:**
```bash
# –ó–∞–º—ñ—Å—Ç—å –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —Ä—è–¥–∫—ñ–≤
LINES=$(wc -l < "$DATASET_FILE")

# –¢–µ–ø–µ—Ä –ø—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Å–ª–æ–≤–∞ —Ç–∞ —Å–∏–º–≤–æ–ª–∏
WORDS=$(wc -w < "$DATASET_FILE")
CHARS=$(wc -c < "$DATASET_FILE")

echo "üìä –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ dataset:"
echo "   –§–∞–π–ª: $DATASET_FILE"
echo "   –†–æ–∑–º—ñ—Ä: $((FILE_SIZE / 1024 / 1024))MB ($FILE_SIZE –±–∞–π—Ç)"
echo "   –°–∏–º–≤–æ–ª—ñ–≤: $CHARS"
echo "   –°–ª—ñ–≤: $WORDS"
echo "   –§–æ—Ä–º–∞—Ç: Plain text (–æ–¥–∏–Ω –¥–æ–≤–≥–∏–π —Ä—è–¥–æ–∫ –¥–ª—è language modeling)"
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ plain text —Ñ–∞–π–ª—É
- ‚úÖ –ü–æ–∫–∞–∑—É—î 15.2M —Å–ª—ñ–≤ –∑–∞–º—ñ—Å—Ç—å 0 —Ä—è–¥–∫—ñ–≤
- ‚úÖ –ù–∞–¥—ñ–π–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É —Ñ–∞–π–ª—É —á–µ—Ä–µ–∑ `stat`
- ‚úÖ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç—É

---

## üß™ –ü–†–û–í–ï–î–ï–ù–Ü –¢–ï–°–¢–ò

### –¢–µ—Å—Ç 1: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
```bash
python test_model_creation.py
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ü–†–û–ô–®–û–í
- –ú–æ–¥–µ–ª—å: 6,837,888 –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- Forward pass: —É—Å–ø—ñ—à–Ω–∏–π
- Loss calculation: 10.7718

### –¢–µ—Å—Ç 2: PAD —Ç–æ–∫–µ–Ω —Ç–∞ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è embeddings
```bash
python test_pad_token.py
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ü–†–û–ô–®–û–í
- Vocab size: 50257 ‚Üí 50258
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: 6,862,464 ‚Üí 13,295,616
- PAD token logits: -0.0004
- Loss –∑ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è–º: 11.0213

---

## üìÅ –ó–ú–Ü–ù–ï–ù–Ü –§–ê–ô–õ–ò

1. **`scripts/train_phase1_pretraining.py`**
   - –î–æ–¥–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è
   - –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –ø–æ–≤–Ω—É –ª–æ–≥—ñ–∫—É resume
   - –û–Ω–æ–≤–ª–µ–Ω–æ —Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ start_epoch

2. **`scripts/train_phase2_instruction_tuning.py`**
   - –î–æ–¥–∞–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è PAD —Ç–æ–∫–µ–Ω–∞
   - –†–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è embeddings
   - –ü–æ–∫—Ä–∞—â–µ–Ω–æ early stopping –ª–æ–≥—ñ–∫—É
   - –î–æ–¥–∞–Ω–æ –ø—ñ–¥—Ç—Ä–∏–º–∫—É –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –§–ê–ó–ò 2
   - –î–æ–¥–∞–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä --resume

3. **`start_phase1_pretraining.sh`**
   - –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É dataset'—É (—Å–ª–æ–≤–∞ –∑–∞–º—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤)
   - –î–æ–¥–∞–Ω–æ –Ω–∞–¥—ñ–π–Ω—É –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —Ä–æ–∑–º—ñ—Ä—É —á–µ—Ä–µ–∑ `stat`
   - –î–æ–¥–∞–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç—É

4. **`start_phase2_instruction_tuning.sh`**
   - –ü–æ–∫—Ä–∞—â–µ–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —Ä–æ–∑–º—ñ—Ä—É checkpoint'—É
   - –î–æ–¥–∞–Ω–æ –Ω–∞–¥—ñ–π–Ω—É –ø–µ—Ä–µ–≤—ñ—Ä–∫—É —á–µ—Ä–µ–∑ `stat`

---

## ‚úÖ –ü–Ü–î–¢–í–ï–†–î–ñ–ï–ù–ê –í–Ü–î–ü–û–í–Ü–î–ù–Ü–°–¢–¨ –í–ò–ú–û–ì–ê–ú

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ —Ç–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è:
- ‚úÖ Custom Transformer –∑ random initialization
- ‚úÖ ~15-25M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- ‚úÖ Sequence length ‚â§ 256
- ‚úÖ GPT-2 BPE tokenizer –±–µ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è

### –§–ê–ó–ê 1 - Language Pretraining:
- ‚úÖ Plain text corpus (Simple Wikipedia, 20M —Ç–æ–∫–µ–Ω—ñ–≤)
- ‚úÖ Causal language modeling objective
- ‚úÖ –ú–∞–∫—Å–∏–º—É–º 3 epochs
- ‚úÖ CPU-–±–µ–∑–ø–µ—á–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
- ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ñ checkpoint'–∏ –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è

### –§–ê–ó–ê 2 - Instruction Tuning:
- ‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è weights –∑ –§–ê–ó–ò 1
- ‚úÖ Instruction datasets (Alpaca, SQuAD, DailyDialog)
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–µ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- ‚úÖ Early stopping –ø—Ä–∏ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—ñ —è–∫–æ—Å—Ç—ñ
- ‚úÖ –ú–∞–∫—Å–∏–º—É–º 1-2 epochs
- ‚úÖ –û–∫—Ä–µ–º–∏–π PAD —Ç–æ–∫–µ–Ω –∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è–º embeddings

### CPU-–±–µ–∑–ø–µ—á–Ω—ñ—Å—Ç—å:
- ‚úÖ –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è Ryzen 5 3600 (6 threads)
- ‚úÖ –ú–∞–ª—ñ batch sizes (1-4)
- ‚úÖ Gradient accumulation –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
- ‚úÖ –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å GPU –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π

### –ë–µ–∑–ø–µ—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è:
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è Ctrl+C
- ‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç–∞–Ω—É –º–æ–¥–µ–ª—ñ, –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞, epoch, loss
- ‚úÖ –ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–º–∫–Ω–µ–Ω–Ω—è –∂–∏–≤–ª–µ–Ω–Ω—è —Ç–∞ –ø—Ä–æ–¥–æ–≤–∂–µ–Ω–Ω—è –ø—ñ–∑–Ω—ñ—à–µ

---

## üöÄ –ì–û–¢–û–í–ù–Ü–°–¢–¨ –î–û –ù–ê–í–ß–ê–ù–ù–Ø

–ü—Ä–æ–µ–∫—Ç **–ø–æ–≤–Ω—ñ—Å—Ç—é –≥–æ—Ç–æ–≤–∏–π** –¥–ª—è –¥–≤–æ—Ñ–∞–∑–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è:

```bash
# –§–ê–ó–ê 1 - Language Pretraining
./start_phase1_pretraining.sh

# –§–ê–ó–ê 2 - Instruction Tuning (–ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –§–ê–ó–ò 1)
./start_phase2_instruction_tuning.sh
```

### –û—á—ñ–∫—É–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:
- **–§–ê–ó–ê 1**: 4-8 –≥–æ–¥–∏–Ω –Ω–∞ CPU, loss –∑–Ω–∏–∂–µ–Ω–Ω—è –∑ ~10 –¥–æ ~3-4
- **–§–ê–ó–ê 2**: 1-3 –≥–æ–¥–∏–Ω–∏ –Ω–∞ CPU, loss –∑–Ω–∏–∂–µ–Ω–Ω—è –∑ ~3-4 –¥–æ ~2-3
- **–§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å**: ~15-25M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –≥–æ—Ç–æ–≤–∞ –¥–ª—è RAG —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è

---

### –¢–µ—Å—Ç 3: –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ dataset'—É
```bash
# –¢–µ—Å—Ç –Ω–∞–¥—ñ–π–Ω–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
DATASET_FILE="datasets/pretrain_text.txt"
FILE_SIZE=$(stat -c%s "$DATASET_FILE")
WORDS=$(wc -w < "$DATASET_FILE")
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ü–†–û–ô–®–û–í
- –†–æ–∑–º—ñ—Ä: 85MB (89,583,689 –±–∞–π—Ç)
- –°–ª—ñ–≤: 15,208,210
- –°–∏–º–≤–æ–ª—ñ–≤: 89,583,689
- –ê–Ω–≥–ª—ñ–π—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç: –∑–Ω–∞–π–¥–µ–Ω–æ

### –¢–µ—Å—Ç 4: –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è TypeError –∑ learning_rate
```python
# –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
with open('config/phase1_pretraining.yaml', 'r') as f:
    config = yaml.safe_load(f)
lr = float(config['training']['learning_rate'])
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ü–†–û–ô–®–û–í
- Learning rate: 0.0005 (—Ç–∏–ø: float)
- Weight decay: 0.1 (—Ç–∏–ø: float)
- –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä —Å—Ç–≤–æ—Ä–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ

### –¢–µ—Å—Ç 5: –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ä–æ–∑–º—ñ—Ä—É –º–æ–¥–µ–ª—ñ
```python
# –¢–µ—Å—Ç –Ω–æ–≤–æ–≥–æ —Ä–æ–∑–º—ñ—Ä—É –º–æ–¥–µ–ª—ñ (6 layers, 320 dim)
model = GPT2LMHeadModel(model_config)
total_params = sum(p.numel() for p in model.parameters())
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ü–†–û–ô–®–û–í
- –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: 6 layers, 320 dim
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: 23,562,560 (~23.6M)
- ‚úÖ –í —Ü—ñ–ª—å–æ–≤–æ–º—É –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ (15-25M)

---

---

### 6. üîß –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø TypeError –ó LEARNING_RATE

**–§–∞–π–ª–∏:** `config/phase1_pretraining.yaml`, `config/phase2_instruction_tuning.yaml`, `scripts/train_phase1_pretraining.py`, `scripts/train_phase2_instruction_tuning.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- `TypeError: '<=' not supported between instances of 'float' and 'str'`
- YAML –ø–∞—Ä—Å–µ—Ä —á–∏—Ç–∞–≤ scientific notation `5e-4` —è–∫ string
- PyTorch –Ω–µ –º–æ–∂–µ –ø–æ—Ä—ñ–≤–Ω—è—Ç–∏ `0.0 <= "5e-4"`

**–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:**
```yaml
# –ë—É–ª–æ (string):
learning_rate: 5e-4
learning_rate: 1e-4

# –°—Ç–∞–ª–æ (float):
learning_rate: 0.0005   # 5e-4
learning_rate: 0.0001   # 1e-4
```

```python
# –î–æ–¥–∞–Ω–æ –∑–∞—Ö–∏—Å—Ç –≤ –∫–æ–¥:
lr = float(config['training']['learning_rate'])
weight_decay = float(config['training']['weight_decay'])

optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ scientific notation –≤ YAML
- ‚úÖ –î–æ–¥–∞–Ω–æ float() –∑–∞—Ö–∏—Å—Ç –≤ –∫–æ–¥—ñ
- ‚úÖ –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –±–µ–∑ –ø–æ–º–∏–ª–æ–∫

---

### 7. üîß –û–ü–¢–ò–ú–Ü–ó–ê–¶–Ü–Ø –†–û–ó–ú–Ü–†–£ –ú–û–î–ï–õ–Ü

**–§–∞–π–ª–∏:** `config/phase1_pretraining.yaml`, `config/phase2_instruction_tuning.yaml`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ú–æ–¥–µ–ª—å –º–∞–ª–∞ 51M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (–∑–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –¥–ª—è CPU)
- –¶—ñ–ª—å–æ–≤–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω: 15-25M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

**–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:**
```yaml
# –ë—É–ª–æ:
dim: 512          # 51M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
depth: 8

# –°—Ç–∞–ª–æ:
dim: 320          # 23.6M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
depth: 6
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –∑ 51M –¥–æ 23.6M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- ‚úÖ –í —Ü—ñ–ª—å–æ–≤–æ–º—É –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ (15-25M)
- ‚úÖ –®–≤–∏–¥—à–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ CPU

---

---

### 8. üîß –ë–ï–ó–ü–ï–ß–ù–ï –ù–ê–í–ß–ê–ù–ù–Ø –î–õ–Ø –ù–ï–°–¢–ê–ë–Ü–õ–¨–ù–û–ì–û –ï–õ–ï–ö–¢–†–û–ü–û–°–¢–ê–ß–ê–ù–ù–Ø

**–§–∞–π–ª–∏:** `scripts/train_phase1_pretraining.py`, `config/phase1_pretraining.yaml`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ß–∞—Å—Ç—ñ –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –µ–ª–µ–∫—Ç—Ä–æ–µ–Ω–µ—Ä–≥—ñ—ó –≤ –£–∫—Ä–∞—ó–Ω—ñ
- –í—Ç—Ä–∞—Ç–∞ –∫—ñ–ª—å–∫–æ—Ö –≥–æ–¥–∏–Ω –Ω–∞–≤—á–∞–Ω–Ω—è –ø—Ä–∏ –Ω–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–æ–º—É –≤—ñ–¥–∫–ª—é—á–µ–Ω–Ω—ñ
- Checkpoint'–∏ –∑–±–µ—Ä—ñ–≥–∞–ª–∏—Å—è —Ç—ñ–ª—å–∫–∏ –≤ –∫—ñ–Ω—Ü—ñ epoch

**–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è:**

1. **–î–æ–¥–∞–Ω–æ –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è:**
```yaml
# config/phase1_pretraining.yaml
training:
  save_every_n_steps: 300         # –ö–æ–∂–Ω—ñ 300 –±–∞—Ç—á—ñ–≤ (~3 —Ö–≤–∏–ª–∏–Ω–∏)
  emergency_save_on_interrupt: true  # –ó–±–µ—Ä—ñ–≥–∞—Ç–∏ –ø—Ä–∏ Ctrl+C
```

2. **–î–æ–¥–∞–Ω–æ –æ–±—Ä–æ–±–∫—É –ø–µ—Ä–µ—Ä–∏–≤–∞–Ω—å:**
```python
# –û–±—Ä–æ–±–Ω–∏–∫ Ctrl+C —Ç–∞ SIGTERM
def signal_handler(signum, frame):
    save_emergency_checkpoint()
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)
```

3. **–ü–æ–∫—Ä–∞—â–µ–Ω–æ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è:**
```python
# –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —Ç–æ—á–Ω–æ–≥–æ –º—ñ—Å—Ü—è (epoch + batch)
start_epoch = checkpoint['epoch']
start_batch = global_step - (epoch - 1) * batches_per_epoch
resumed_global_step = checkpoint['global_step']
```

4. **–†–æ–∑—à–∏—Ä–µ–Ω–æ —Ñ–æ—Ä–º–∞—Ç checkpoint'—ñ–≤:**
```python
checkpoint = {
    'phase': 1,
    'epoch': epoch,
    'global_step': global_step,  # –¢–æ—á–Ω–∞ –ø–æ–∑–∏—Ü—ñ—è
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'timestamp': time.time(),
    'periodic_save': True  # –¢–∏–ø checkpoint'—É
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–∂–Ω—ñ ~3 —Ö–≤–∏–ª–∏–Ω–∏
- ‚úÖ Ctrl+C –∑–∞–≤–∂–¥–∏ –∑–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–≥—Ä–µ—Å
- ‚úÖ –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —Ç–æ—á–Ω–æ–≥–æ batch'—É
- ‚úÖ –ú–∞–∫—Å–∏–º—É–º –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω –≤—Ç—Ä–∞—á–µ–Ω–æ—ó —Ä–æ–±–æ—Ç–∏
- ‚úÖ Emergency —Ç–∞ periodic checkpoint'–∏

---

### –¢–µ—Å—Ç 6: –ë–µ–∑–ø–µ—á–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è
```python
# –¢–µ—Å—Ç –ø–µ—Ä—ñ–æ–¥–∏—á–Ω–∏—Ö checkpoint'—ñ–≤
save_periodic_checkpoint(model, optimizer, epoch=1, global_step=300, loss=5.5)
checkpoint = torch.load("last_checkpoint.pt")
```
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** ‚úÖ –ü–†–û–ô–®–û–í
- Phase: 1, Epoch: 1, Global step: 300
- Checkpoint –º—ñ—Å—Ç–∏—Ç—å –≤—Å—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –ø–æ–ª—è
- Emergency save —Ñ—É–Ω–∫—Ü—ñ–æ–Ω—É—î

---

## üìä –ü–Ü–î–°–£–ú–û–ö

**–í—Å—ñ –∫—Ä–∏—Ç–∏—á–Ω—ñ –≤–∏–º–æ–≥–∏ –≤–∏–∫–æ–Ω–∞–Ω–æ!**

- ‚úÖ 8 –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ
- ‚úÖ 6 —Ç–µ—Å—Ç—ñ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ
- ‚úÖ –ü—Ä–æ–µ–∫—Ç –±–µ–∑–ø–µ—á–Ω–∏–π —Ç–∞ –≤—ñ–¥–Ω–æ–≤–ª—é–≤–∞–Ω–∏–π
- ‚úÖ –ì–æ—Ç–æ–≤–∏–π –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ CPU
- ‚úÖ –ó–∞—Ö–∏—â–µ–Ω–æ –≤—ñ–¥ –≤—ñ–¥–∫–ª—é—á–µ–Ω—å –µ–ª–µ–∫—Ç—Ä–æ–µ–Ω–µ—Ä–≥—ñ—ó

**–ü—Ä–æ–µ–∫—Ç –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∑–º—ñ–Ω.**

---

*–ó–≤—ñ—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ: 31 –≥—Ä—É–¥–Ω—è 2025*  
*–í–µ—Ä—Å—ñ—è –ø—Ä–æ–µ–∫—Ç—É: –ü—ñ—Å–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω—å*
