"""
–ú–æ–¥—É–ª—å –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
"""
import json
from pathlib import Path
from typing import List, Dict, Optional, Any


class DatasetManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"""
    
    def __init__(self, datasets_dir: str | Path = None):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
        
        Args:
            datasets_dir: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –∑ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
        """
        if datasets_dir is None:
            project_root = Path(__file__).parent.parent
            # –®—É–∫–∞—Ç–∏ –≤ –∫—ñ–ª—å–∫–æ—Ö –º—ñ—Å—Ü—è—Ö
            possible_dirs = [
                project_root / "temp" / "datasets",
                project_root / "datasets",
            ]
            
            for dir_path in possible_dirs:
                if dir_path.exists():
                    datasets_dir = dir_path
                    break
            
            if datasets_dir is None:
                datasets_dir = project_root / "temp" / "datasets"
                datasets_dir.mkdir(parents=True, exist_ok=True)
        
        self.datasets_dir = Path(datasets_dir)
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
    
    def list_datasets(self) -> List[Dict[str, Any]]:
        """
        –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ –¥–∞—Ç–∞—Å–µ—Ç–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç–∏
        """
        datasets = []
        
        for json_file in self.datasets_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
                if isinstance(data, dict) and 'data' in data:
                    samples = len(data['data'])
                    metadata = data.get('metadata', {})
                elif isinstance(data, dict) and 'metadata' in data:
                    samples = len(data.get('data', []))
                    metadata = data.get('metadata', {})
                elif isinstance(data, list):
                    samples = len(data)
                    metadata = {}
                else:
                    samples = 0
                    metadata = {}
                
                dataset_info = {
                    'path': str(json_file.absolute()),
                    'name': json_file.stem,
                    'filename': json_file.name,
                    'size_mb': json_file.stat().st_size / (1024 * 1024),
                    'samples': samples,
                    'modified': json_file.stat().st_mtime,
                    'teacher_model': metadata.get('teacher_model_name') if metadata else None
                }
                datasets.append(dataset_info)
            except Exception as e:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —Ñ–∞–π–ª–∏ —è–∫—ñ –Ω–µ —î –≤–∞–ª—ñ–¥–Ω–∏–º–∏ JSON
                continue
        
        # –°–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –¥–∞—Ç–æ—é (–Ω–æ–≤—ñ—à—ñ —Å–ø–æ—á–∞—Ç–∫—É)
        datasets.sort(key=lambda x: x['modified'], reverse=True)
        
        return datasets
    
    def get_dataset(self, name: str) -> Optional[Dict[str, Any]]:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞ —ñ–º'—è–º
        
        Args:
            name: –Ü–º'—è –¥–∞—Ç–∞—Å–µ—Ç—É (—á–∞—Å—Ç–∏–Ω–∞ –∞–±–æ –ø–æ–≤–Ω–µ)
        
        Returns:
            –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç –∞–±–æ None
        """
        datasets = self.list_datasets()
        name_lower = name.lower()
        
        for dataset in datasets:
            if name_lower in dataset['name'].lower():
                return dataset
        
        return None
    
    def load_dataset(self, name_or_path: str) -> List[Dict[str, str]]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
        
        Args:
            name_or_path: –Ü–º'—è –¥–∞—Ç–∞—Å–µ—Ç—É –∞–±–æ —à–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        """
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —á–∏ —Ü–µ —à–ª—è—Ö
        if Path(name_or_path).exists():
            dataset_path = Path(name_or_path)
        else:
            dataset = self.get_dataset(name_or_path)
            if not dataset:
                raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {name_or_path}")
            dataset_path = Path(dataset['path'])
        
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
        if isinstance(data, dict) and 'data' in data:
            return data['data']
        elif isinstance(data, list):
            return data
        else:
            return []
    
    def save_dataset(self, data: List[Dict[str, str]], name: str) -> Path:
        """
        –ó–±–µ—Ä–µ–≥—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç
        
        Args:
            data: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
            name: –Ü–º'—è –¥–∞—Ç–∞—Å–µ—Ç—É (–±–µ–∑ .json)
        
        Returns:
            –®–ª—è—Ö –¥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É
        """
        if not name.endswith('.json'):
            name += '.json'
        
        output_path = self.datasets_dir / name
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        return output_path
    
    def print_summary(self) -> None:
        """–í–∏–≤–µ—Å—Ç–∏ –ø—ñ–¥—Å—É–º–æ–∫ –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤"""
        datasets = self.list_datasets()
        
        if not datasets:
            print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ {self.datasets_dir}")
            print(f"   –î–æ–¥–∞–π—Ç–µ .json —Ñ–∞–π–ª–∏ –≤ –ø–∞–ø–∫—É: {self.datasets_dir}")
            return
        
        print(f"\nüìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(datasets)} –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ –≤ {self.datasets_dir}:")
        print("-" * 70)
        
        total_samples = 0
        for i, dataset in enumerate(datasets, 1):
                print(f"{i}. {dataset['name']}")
                print(f"   üìÅ {dataset['filename']}")
                print(f"   üìä –†–æ–∑–º—ñ—Ä: {dataset['size_mb']:.2f} MB")
                print(f"   üìù –ü—Ä–∏–∫–ª–∞–¥—ñ–≤: {dataset['samples']:,}")
                if dataset.get('teacher_model'):
                    print(f"   üéì Teacher –º–æ–¥–µ–ª—å: {dataset['teacher_model']}")
                print()
                total_samples += dataset['samples']
        
        print(f"üìä –ó–∞–≥–∞–ª–æ–º: {total_samples:,} –ø—Ä–∏–∫–ª–∞–¥—ñ–≤")
        print()


def main():
    """CLI –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ö–µ—Ä—É–≤–∞–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏")
    parser.add_argument("--list", action="store_true", help="–°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤")
    parser.add_argument("--info", type=str, help="–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç")
    
    args = parser.parse_args()
    
    manager = DatasetManager()
    
    if args.info:
        dataset = manager.get_dataset(args.info)
        if dataset:
            print(f"\nüìä –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç '{dataset['name']}':")
            print(f"   –®–ª—è—Ö: {dataset['path']}")
            print(f"   –†–æ–∑–º—ñ—Ä: {dataset['size_mb']:.2f} MB")
            print(f"   –ü—Ä–∏–∫–ª–∞–¥—ñ–≤: {dataset['samples']:,}")
        else:
            print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {args.info}")
    else:
        manager.print_summary()


if __name__ == "__main__":
    main()

