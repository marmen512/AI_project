"""
–ú–æ–¥—É–ª—å –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É –Ω–∞–≤—á–µ–Ω–æ—ó TRM –º–æ–¥–µ–ª—ñ
"""
import torch
from pathlib import Path
from typing import Optional, List, Dict, Tuple
import json

from tiny_recursive_model import TinyRecursiveModel, MLPMixer1D, TransformerBackbone
from config.model_manager import ModelManager
from tiny_recursive_model.utils import tokenize_and_pad, prepare_code_input, load_tokenizer


class TRMInference:
    """–ö–ª–∞—Å –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –Ω–∞–≤—á–µ–Ω–æ—é TRM –º–æ–¥–µ–ª–ª—é"""
    
    def __init__(
        self,
        model: TinyRecursiveModel,
        tokenizer,
        device: str = 'cpu',
        max_seq_len: int = 2048,
        timeout_seconds: Optional[float] = None  # Timeout –¥–ª—è recursion (–±–µ–∑–ø–µ–∫–∞)
    ):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
        
        Args:
            model: –ù–∞–≤—á–µ–Ω–∞ TRM –º–æ–¥–µ–ª—å
            tokenizer: –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
            device: –ü—Ä–∏—Å—Ç—Ä—ñ–π ('cpu' –∞–±–æ 'cuda')
            max_seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.max_seq_len = max_seq_len
        self.timeout_seconds = timeout_seconds  # Timeout –¥–ª—è recursion (–±–µ–∑–ø–µ–∫–∞)
        
        # –ü–µ—Ä–µ–º—ñ—Å—Ç–∏—Ç–∏ –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π
        self.model.to(self.device)
        self.model.eval()
    
    def predict(
        self,
        context: str,
        query: str,
        max_deep_refinement_steps: int = 12,
        halt_prob_thres: float = 0.5,
        temperature: float = 0.7,
        top_k: int = 50,
        deterministic: bool = False
    ) -> Dict[str, any]:
        """
        –ó—Ä–æ–±–∏—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
        
        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–¥—É
            query: –ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            max_deep_refinement_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ —É—Ç–æ—á–Ω–µ–Ω–Ω—è
            halt_prob_thres: –ü–æ—Ä—ñ–≥ –¥–ª—è —Ä–∞–Ω–Ω—å–æ–≥–æ –≤–∏—Ö–æ–¥—É
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è sampling (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–∞—Ä–∞–∑)
            top_k: Top-k sampling (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–∞—Ä–∞–∑)
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥
        input_text = prepare_code_input(context, query)
        
        pad_token_id = 0
        if hasattr(self.tokenizer, 'pad_token_id') and self.tokenizer.pad_token_id is not None:
            pad_token_id = self.tokenizer.pad_token_id
        
        input_ids = tokenize_and_pad(
            self.tokenizer,
            input_text,
            self.max_seq_len,
            pad_token_id=pad_token_id
        ).unsqueeze(0).to(self.device)
        
        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑ timeout (–ø–µ—Ä–µ–¥–∞—Ç–∏ timeout_seconds —è–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ)
        # Default timeout: 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±–µ–∑–ø–µ–∫–∏ (–∑–∞–ø–æ–±—ñ–≥–∞—î –∑–∞–≤–∏—Å–∞–Ω–Ω—é)
        timeout = self.timeout_seconds if self.timeout_seconds is not None else 30.0
        with torch.no_grad():
            pred_tokens, exit_steps = self.model.predict(
                input_ids,
                max_deep_refinement_steps=max_deep_refinement_steps,
                halt_prob_thres=halt_prob_thres,
                timeout_seconds=timeout  # –ü–µ—Ä–µ–¥–∞—Ç–∏ timeout –¥–ª—è –±–µ–∑–ø–µ–∫–∏
            )
        
        # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if hasattr(self.tokenizer, 'decode'):
            pred_tokens_clean = pred_tokens[0].cpu().numpy()
            # –ó–Ω–∞–π—Ç–∏ —Ä–µ–∞–ª—å–Ω–∏–π –∫—ñ–Ω–µ—Ü—å (–≤–∏–¥–∞–ª–∏—Ç–∏ padding)
            output = self.tokenizer.decode(pred_tokens_clean, skip_special_tokens=True)
        else:
            output = ''.join([self.tokenizer.inv_vocab.get(int(t), '?') for t in pred_tokens[0]])
        
        return {
            'completion': output,
            'exit_steps': exit_steps[0].item(),
            'tokens': pred_tokens[0].cpu().tolist(),
            'input_length': input_ids.shape[1]
        }
    
    def batch_predict(
        self,
        examples: List[Dict[str, str]],
        max_deep_refinement_steps: int = 12,
        halt_prob_thres: float = 0.5
    ) -> List[Dict[str, any]]:
        """
        Batch –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –∫—ñ–ª—å–∫–æ—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
        
        Args:
            examples: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ –∑ 'context' —Ç–∞ 'query'
            max_deep_refinement_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤
            halt_prob_thres: –ü–æ—Ä—ñ–≥ –¥–ª—è —Ä–∞–Ω–Ω—å–æ–≥–æ –≤–∏—Ö–æ–¥—É
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        """
        results = []
        for example in examples:
            result = self.predict(
                example.get('context', ''),
                example.get('query', ''),
                max_deep_refinement_steps=max_deep_refinement_steps,
                halt_prob_thres=halt_prob_thres
            )
            result['context'] = example.get('context', '')
            result['query'] = example.get('query', '')
            results.append(result)
        
        return results
    
    def interactive_mode(self):
        """–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
        print("\n" + "=" * 70)
        print("ü§ñ –Ü–ù–¢–ï–†–ê–ö–¢–ò–í–ù–ò–ô –†–ï–ñ–ò–ú TRM –ú–û–î–ï–õ–Ü")
        print("=" * 70)
        print("–í–≤–µ–¥—ñ—Ç—å 'exit' –¥–ª—è –≤–∏—Ö–æ–¥—É\n")
        
        while True:
            try:
                context = input("üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç (–∫–æ–¥):\n> ").strip()
                if context.lower() == 'exit':
                    break
                
                query = input("‚ùì –ó–∞–ø–∏—Ç:\n> ").strip()
                if query.lower() == 'exit':
                    break
                
                print("\nüîÑ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ...")
                result = self.predict(context, query)
                
                print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç (–∫—Ä–æ–∫—ñ–≤ —É—Ç–æ—á–Ω–µ–Ω–Ω—è: {result['exit_steps']}):")
                print("-" * 70)
                print(result['completion'])
                print("-" * 70)
                print()
                
            except KeyboardInterrupt:
                print("\n\nüëã –í–∏—Ö—ñ–¥ –∑ —ñ–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º—É")
                break
            except Exception as e:
                print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\n")


def load_trained_model(
    model_path: str | Path = None,
    model_name: str = None,  # –ù–û–í–ò–ô: –º–æ–∂–Ω–∞ –≤–∫–∞–∑–∞—Ç–∏ —ñ–º'—è –º–æ–¥–µ–ª—ñ
    config_path: Optional[str | Path] = None,
    device: str = 'cpu',
    tokenizer_name: str = "gpt2"
) -> TRMInference:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–≤—á–µ–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É
    
    Args:
        model_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª—ñ (.pt) (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ, —è–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ model_name)
        model_name: –ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—à—É–∫—É –≤ models/trained/ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        config_path: –®–ª—è—Ö –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ (JSON, –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
        device: –ü—Ä–∏—Å—Ç—Ä—ñ–π ('cpu' –∞–±–æ 'cuda')
        tokenizer_name: –ù–∞–∑–≤–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞
    
    Returns:
        TRMInference –æ–±'—î–∫—Ç
    """
    manager = ModelManager()
    
    # –Ø–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ —ñ–º'—è, –∑–Ω–∞–π—Ç–∏ –º–æ–¥–µ–ª—å
    if model_name:
        model_info = manager.get_model_by_name(model_name)
        if model_info:
            model_path = model_info['path']
            # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ config –∑ –º–æ–¥–µ–ª—ñ —è–∫—â–æ —î
            if model_info.get('config_path') and not config_path:
                config_path = model_info['config_path']
            print(f"üì¶ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –º–æ–¥–µ–ª—å: {model_info['name']}")
        else:
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å '{model_name}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ models/trained/")
    
    # –Ø–∫—â–æ —à–ª—è—Ö –Ω–µ –≤–∫–∞–∑–∞–Ω–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—é –º–æ–¥–µ–ª—å
    if model_path is None:
        model_info = manager.get_default_model()
        if model_info:
            model_path = model_info['path']
            if model_info.get('config_path') and not config_path:
                config_path = model_info['config_path']
            print(f"üì¶ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –æ—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—å: {model_info['name']}")
        else:
            raise FileNotFoundError("–ú–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ models/trained/")
    
    model_path = Path(model_path)
    
    if not model_path.exists():
        raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {model_path}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —è–∫—â–æ —î
    config = {}
    if config_path:
        config_path = Path(config_path)
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
    else:
        # –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –∑–Ω–∞–π—Ç–∏ config –ø–æ—Ä—É—á –∑ –º–æ–¥–µ–ª–ª—é
        config_path = model_path.with_suffix('.json')
        if not config_path.exists():
            config_path = model_path.parent / f"{model_path.stem}_config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –∞–±–æ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó)
    dim = config.get('dim', 768)  # –î–µ—Ñ–æ–ª—Ç –¥–ª—è GPT-2
    vocab_size = config.get('vocab_size', 50257)
    seq_len = config.get('seq_len', 1024)  # –î–µ—Ñ–æ–ª—Ç –¥–ª—è GPT-2
    depth = config.get('depth', 12)  # –î–µ—Ñ–æ–ª—Ç –¥–ª—è GPT-2
    
    # –í–∏–∑–Ω–∞—á–∏—Ç–∏ —á–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è Transformer
    use_transformer = config.get('use_transformer', False)
    transformer_model = config.get('transformer_model', 'gpt2')
    transformer_pretrained = config.get('transformer_pretrained', True)
    transformer_cache_dir = config.get('transformer_cache_dir', None)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    tokenizer, actual_vocab_size, _ = load_tokenizer(tokenizer_name)
    vocab_size = actual_vocab_size  # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Ä–µ–∞–ª—å–Ω–∏–π —Ä–æ–∑–º—ñ—Ä —Å–ª–æ–≤–Ω–∏–∫–∞
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ network (MLPMixer –∞–±–æ Transformer)
    if use_transformer:
        network = TransformerBackbone(
            dim=dim,
            depth=depth,
            seq_len=seq_len,
            pretrained=transformer_pretrained,
            model_name=transformer_model,
            cache_dir=transformer_cache_dir
        )
        # –û–Ω–æ–≤–∏—Ç–∏ dim –∑ —Ä–µ–∞–ª—å–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        dim = network.dim
        depth = network.depth
        seq_len = network.seq_len
    else:
        network = MLPMixer1D(dim=dim, depth=depth, seq_len=seq_len)
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
    model = TinyRecursiveModel(
        dim=dim,
        num_tokens=vocab_size,
        network=network,
        num_refinement_blocks=config.get('num_refinement_blocks', 3),
        num_latent_refinements=config.get('num_latent_refinements', 6),
        halt_loss_weight=config.get('halt_loss_weight', 1.0),
        max_recursion_depth=config.get('max_recursion_depth', 20),
        adaptive_recursion=config.get('adaptive_recursion', False),
        timeout_seconds=config.get('timeout_seconds', None),
        thinking_cost_weight=config.get('thinking_cost_weight', 0.01)
    )
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –≤–∞–≥–∏
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å –æ–±'—î–∫—Ç
    return TRMInference(model, tokenizer, device=device, max_seq_len=seq_len)


def find_trained_models(models_dir: str | Path = None) -> List[Dict[str, any]]:
    """
    –ó–Ω–∞–π—Ç–∏ –≤—Å—ñ –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ
    
    Args:
        models_dir: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –∑ –º–æ–¥–µ–ª—è–º–∏ (None = –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –º–æ–¥–µ–ª—ñ
    """
    if models_dir is None:
        project_root = Path(__file__).parent.parent
        # –®—É–∫–∞—Ç–∏ –≤ –∫—ñ–ª—å–∫–æ—Ö –º—ñ—Å—Ü—è—Ö
        possible_dirs = [
            project_root / "models" / "trained",
            project_root / "trained_models",
        ]
        
        for dir_path in possible_dirs:
            if dir_path.exists():
                models_dir = dir_path
                break
        
        if models_dir is None:
            models_dir = project_root / "models" / "trained"
            models_dir.mkdir(parents=True, exist_ok=True)
    else:
        models_dir = Path(models_dir)
    
    models = []
    for model_file in models_dir.glob("*.pt"):
        model_info = {
            'path': str(model_file.absolute()),
            'name': model_file.stem,
            'filename': model_file.name,
            'size_mb': model_file.stat().st_size / (1024 * 1024),
            'modified': model_file.stat().st_mtime
        }
        models.append(model_info)
    
    # –°–æ—Ä—Ç—É–≤–∞—Ç–∏ –∑–∞ –¥–∞—Ç–æ—é –º–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (–Ω–æ–≤—ñ—à—ñ —Å–ø–æ—á–∞—Ç–∫—É)
    models.sort(key=lambda x: x['modified'], reverse=True)
    
    return models


def main():
    """CLI –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –Ω–∞–≤—á–µ–Ω–æ—é –º–æ–¥–µ–ª–ª—é"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–†–æ–±–æ—Ç–∞ –∑ –Ω–∞–≤—á–µ–Ω–æ—é TRM –º–æ–¥–µ–ª–ª—é")
    parser.add_argument("--model", type=str, help="–®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ (.pt)")
    parser.add_argument("--model-name", type=str, dest="model_name", help="–ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—à—É–∫—É –≤ models/trained/")
    parser.add_argument("--config", type=str, help="–®–ª—è—Ö –¥–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (JSON)")
    parser.add_argument("--device", type=str, default="cpu", choices=['cpu', 'cuda'])
    parser.add_argument("--interactive", action="store_true", help="–Ü–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∏–π —Ä–µ–∂–∏–º")
    parser.add_argument("--list", action="store_true", help="–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π")
    
    args = parser.parse_args()
    
    manager = ModelManager()
    
    if args.list:
        manager.list_models()
        return
    
    if not args.model and not args.model_name:
        model_info = manager.get_default_model()
        if model_info:
            args.model = model_info['path']
            print(f"üéØ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –æ—Å—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—å: {model_info['name']}")
        else:
            print("‚ùå –í–∫–∞–∂—ñ—Ç—å --model (—à–ª—è—Ö) –∞–±–æ --model-name (–Ω–∞–∑–≤–∞) –∞–±–æ –¥–æ–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å –≤ models/trained/")
            return
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
    if args.model_name:
        print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑–∞ —ñ–º'—è–º: {args.model_name}")
    else:
        print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {args.model}")
    inference = load_trained_model(
        model_path=args.model,
        model_name=args.model_name,
        config_path=args.config,
        device=args.device
    )
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞!")
    
    if args.interactive:
        inference.interactive_mode()
    else:
        # –¢–µ—Å—Ç–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥
        result = inference.predict(
            "def hello():\n    return 'world'",
            "–î–æ–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä name"
        )
        print("\nüìù –¢–µ—Å—Ç–æ–≤–∏–π –ø—Ä–∏–∫–ª–∞–¥:")
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: def hello():\n    return 'world'")
        print(f"–ó–∞–ø–∏—Ç: –î–æ–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä name")
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(result['completion'])


if __name__ == "__main__":
    main()

