"""
TRM-–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
–ó–∞–º—ñ—Å—Ç—å LLM-—Å—Ç–∞–π–ª –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω—ñ –¥–ª—è TRM
"""
from dataclasses import dataclass
from typing import Optional
from pathlib import Path
import json


@dataclass
class TRMConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è TRM –Ω–∞–≤—á–∞–Ω–Ω—è - –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    # –í–µ—Ä—Å—ñ—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    config_version: str = "1.0"  # –í–µ—Ä—Å—ñ—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è –º—ñ–≥—Ä–∞—Ü—ñ—ó
    
    # –ú–æ–¥–µ–ª—å (TRM-–æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏)
    dim: int = 256  # –ó–∞–º—ñ—Å—Ç—å 1024 - –º–µ–Ω—à–µ –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
    depth: int = 4
    seq_len: int = 256  # –ó–∞–º—ñ—Å—Ç—å 4096 - –Ω–∞–±–∞–≥–∞—Ç–æ –º–µ–Ω—à–µ –¥–ª—è TRM
    vocab_size: Optional[int] = None  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑ tokenizer
    
    # –ù–∞–≤—á–∞–Ω–Ω—è
    batch_size: int = 4  # –ó–∞–º—ñ—Å—Ç—å 1 - –±—ñ–ª—å—à–µ –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
    effective_batch_size: int = 16  # batch_size * gradient_accumulation
    epochs: int = 10
    learning_rate: float = 1e-4
    
    # –†–µ–∫—É—Ä—Å—ñ—è
    max_recurrent_steps: int = 12
    halt_prob_thres: float = 0.5
    max_recursion_depth: int = 20  # –ù–û–í–ò–ô: guard –¥–ª—è —Ä–µ–∫—É—Ä—Å—ñ—ó –≤ –º–æ–¥–µ–ª—ñ
    adaptive_recursion: bool = False  # –£–≤—ñ–º–∫–Ω—É—Ç–∏ adaptive recursion gate (–ø–æ—Ç—Ä—ñ–±–µ–Ω thinking cost)
    
    # Curriculum learning (–¥–ª—è TRM –≤–∞–∂–ª–∏–≤–æ!)
    curriculum_enabled: bool = True
    curriculum_start_len: int = 64
    curriculum_max_len: int = 256
    curriculum_stages: int = 4  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ—Ç–∞–ø—ñ–≤
    curriculum_epochs_per_stage: int = 3  # –°–∫—ñ–ª—å–∫–∏ –µ–ø–æ—Ö –Ω–∞ –∫–æ–∂–Ω–æ–º—É —Ä—ñ–≤–Ω—ñ
    
    # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
    warmup_steps: int = 1000
    weight_decay: float = 1.0
    
    # Thinking cost (–¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –º–∏—Å–ª–µ–Ω–Ω—è, –Ω–µ –ø—Ä–æ—Å—Ç–æ –º—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—ó –∫—Ä–æ–∫—ñ–≤)
    thinking_cost_weight: float = 0.01  # –í–∞–≥–∞ thinking cost –≤ loss
    
    # Dataset
    cache_size: int = 1000  # –ö–µ—à –¥–ª—è lazy loading
    validate_format: bool = True
    
    @property
    def gradient_accumulation_steps(self) -> int:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ–±—á–∏—Å–ª–∏—Ç–∏ –∑ effective_batch_size"""
        return max(1, self.effective_batch_size // self.batch_size)
    
    def validate(self):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∫–æ—Ä–µ–∫—Ç–Ω—ñ—Å—Ç—å –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó"""
        assert self.dim > 0, "dim –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ > 0"
        assert 0 < self.halt_prob_thres <= 1, "halt_prob_thres –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –≤ (0, 1]"
        assert self.max_recurrent_steps > 0, "max_recurrent_steps –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ > 0"
        assert self.seq_len >= self.curriculum_start_len, "seq_len –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ >= curriculum_start_len"
        assert self.curriculum_max_len >= self.curriculum_start_len, "curriculum_max_len –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ >= curriculum_start_len"
        assert self.batch_size > 0, "batch_size –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ > 0"
        assert self.effective_batch_size >= self.batch_size, "effective_batch_size –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ >= batch_size"
    
    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ —Å–ª–æ–≤–Ω–∏–∫"""
        return {
            'config_version': self.config_version,
            'dim': self.dim,
            'depth': self.depth,
            'seq_len': self.seq_len,
            'batch_size': self.batch_size,
            'effective_batch_size': self.effective_batch_size,
            'epochs': self.epochs,
            'learning_rate': self.learning_rate,
            'max_recurrent_steps': self.max_recurrent_steps,
            'halt_prob_thres': self.halt_prob_thres,
            'max_recursion_depth': self.max_recursion_depth,
            'curriculum_enabled': self.curriculum_enabled,
            'curriculum_start_len': self.curriculum_start_len,
            'curriculum_max_len': self.curriculum_max_len,
            'gradient_accumulation_steps': self.gradient_accumulation_steps,
        }
    
    @classmethod
    def from_dict(cls, d: dict) -> 'TRMConfig':
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞"""
        config_version = d.get('config_version', '1.0')
        # –ú—ñ–≥—Ä–∞—Ü—ñ—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (–¥–ª—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –≤–µ—Ä—Å—ñ–π)
        if config_version == '1.0':
            pass  # –ü–æ—Ç–æ—á–Ω–∞ –≤–µ—Ä—Å—ñ—è
        # –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –º—ñ–≥—Ä–∞—Ü—ñ—ó –¥–ª—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –≤–µ—Ä—Å—ñ–π
        
        # –§—ñ–ª—å—Ç—Ä—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –≤–∞–ª—ñ–¥–Ω—ñ –ø–æ–ª—è
        valid_fields = {k: v for k, v in d.items() if k in cls.__dataclass_fields__}
        return cls(**valid_fields)
    
    @classmethod
    def from_dataset(
        cls,
        dataset_path: str | Path,
        auto_detect: bool = True,
        **overrides
    ) -> 'TRMConfig':
        """
        –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞—Ç–∞—Å–µ—Ç—É
        
        Args:
            dataset_path: –®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
            auto_detect: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞—Ç–∞—Å–µ—Ç—É
            **overrides: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
        
        Returns:
            TRMConfig –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        if not auto_detect:
            return cls(**overrides)
        
        dataset_path = Path(dataset_path)
        
        # –û—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç
        dataset_size = 0
        dataset_samples = 0
        
        if dataset_path.exists():
            dataset_size = dataset_path.stat().st_size
            
            try:
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, list):
                    dataset_samples = len(data)
                elif isinstance(data, dict) and 'data' in data:
                    dataset_samples = len(data['data'])
            except:
                pass
        
        # –û—Ü—ñ–Ω–∏—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—É –ø–∞–º'—è—Ç—å
        try:
            import psutil
            available_memory_gb = psutil.virtual_memory().available / (1024 ** 3)
        except:
            available_memory_gb = 8.0
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        # Batch size –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–æ–∑–º—ñ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞ –ø–∞–º'—è—Ç—ñ
        if dataset_samples < 100:
            batch_size = 2
            epochs = 20
        elif dataset_samples < 500:
            batch_size = 4
            epochs = 15
        elif dataset_samples < 2000:
            batch_size = 4 if available_memory_gb < 8 else 8
            epochs = 12
        elif dataset_samples < 10000:
            batch_size = 8 if available_memory_gb < 16 else 16
            epochs = 10
        else:
            batch_size = 16 if available_memory_gb < 16 else 32
            epochs = 8
        
        # Learning rate –Ω–∞ –æ—Å–Ω–æ–≤—ñ batch_size
        learning_rate = 1e-4
        if batch_size >= 16:
            learning_rate = 2e-4
        elif batch_size <= 2:
            learning_rate = 5e-5
        
        # Gradient accumulation
        effective_batch_size = 16
        if batch_size < 4:
            effective_batch_size = 8
        elif batch_size >= 16:
            effective_batch_size = 32
        
        # Warmup steps
        batches_per_epoch = max(1, dataset_samples // batch_size)
        warmup_steps = min(2000, max(100, batches_per_epoch * 2))
        
        # Max recurrent steps
        max_recurrent_steps = 12
        if dataset_samples > 10000:
            max_recurrent_steps = 16
        elif dataset_samples < 100:
            max_recurrent_steps = 8
        
        # –°—Ç–≤–æ—Ä–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        config = cls(
            batch_size=batch_size,
            effective_batch_size=effective_batch_size,
            epochs=epochs,
            learning_rate=learning_rate,
            max_recurrent_steps=max_recurrent_steps,
            warmup_steps=warmup_steps,
            **overrides
        )
        
        return config
    
    def print_summary(self, dataset_path: Optional[str | Path] = None) -> None:
        """–í–∏–≤–µ—Å—Ç–∏ –ø—ñ–¥—Å—É–º–æ–∫ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó"""
        print("\n" + "=" * 70)
        print("‚öôÔ∏è  –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø TRM –ù–ê–í–ß–ê–ù–ù–Ø")
        print("=" * 70)
        
        if dataset_path:
            dataset_path = Path(dataset_path)
            if dataset_path.exists():
                dataset_size = dataset_path.stat().st_size
                print(f"\nüìä –î–∞—Ç–∞—Å–µ—Ç:")
                print(f"   - –§–∞–π–ª: {dataset_path.name}")
                print(f"   - –†–æ–∑–º—ñ—Ä: {dataset_size / (1024*1024):.2f} MB")
        
        print(f"\nüéØ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ:")
        print(f"   - dim: {self.dim}")
        print(f"   - depth: {self.depth}")
        print(f"   - seq_len: {self.seq_len}")
        if self.curriculum_enabled:
            print(f"   - curriculum: {self.curriculum_start_len} ‚Üí {self.curriculum_max_len}")
        
        print(f"\nüéì –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è:")
        print(f"   - epochs: {self.epochs}")
        print(f"   - batch_size: {self.batch_size}")
        print(f"   - effective_batch_size: {self.effective_batch_size}")
        print(f"   - gradient_accumulation_steps: {self.gradient_accumulation_steps}")
        print(f"   - learning_rate: {self.learning_rate:.2e}")
        print(f"   - warmup_steps: {self.warmup_steps}")
        print(f"   - max_recurrent_steps: {self.max_recurrent_steps}")
        print(f"   - halt_prob_thres: {self.halt_prob_thres}")
        print(f"   - max_recursion_depth: {self.max_recursion_depth}")
        
        print("=" * 70 + "\n")


@dataclass
class CurriculumStage:
    """–ï—Ç–∞–ø curriculum learning (—è–∫ policy, –Ω–µ –∂–æ—Ä—Å—Ç–∫–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è —Ç–∏–ø–∞–º–∏ –∑–∞–¥–∞—á)"""
    seq_len: int
    epochs: int
    description: str
    task_difficulty: Optional[float] = None  # –°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å –∑–∞–¥–∞—á—ñ (0.0 - 1.0)
    max_recursion: Optional[int] = None  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≥–ª–∏–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å—ñ—ó –¥–ª—è —Ü—å–æ–≥–æ –µ—Ç–∞–ø—É
    # –ü—Ä–∏–º—ñ—Ç–∫–∞: —Ç–∏–ø–∏ –∑–∞–¥–∞—á –Ω–µ –∫–µ—Ä—É—é—Ç—å—Å—è —Ç—É—Ç - —Ü–µ —Ä–æ–±–∏—Ç—å—Å—è —á–µ—Ä–µ–∑ dataset filters


class CurriculumScheduler:
    """–ü–ª–∞–Ω—É–≤–∞–ª—å–Ω–∏–∫ curriculum learning –¥–ª—è TRM"""
    
    def __init__(
        self,
        start_len: int = 64,
        max_len: int = 256,
        stages: int = 4,
        epochs_per_stage: int = 3
    ):
        self.stages = self._create_stages(start_len, max_len, stages, epochs_per_stage)
        self.current_stage = 0
        self.current_epoch_in_stage = 0
    
    def _create_stages(self, start: int, max: int, n: int, epochs: int) -> list[CurriculumStage]:
        """–°—Ç–≤–æ—Ä–∏—Ç–∏ –µ—Ç–∞–ø–∏ curriculum"""
        if n == 1:
            return [CurriculumStage(seq_len=max, epochs=epochs, description=f"Stage 1: seq_len={max}")]
        
        step = (max - start) / (n - 1)
        stages = []
        for i in range(n):
            seq_len = int(start + step * i)
            stages.append(CurriculumStage(
                seq_len=seq_len,
                epochs=epochs,
                description=f"Stage {i+1}/{n}: seq_len={seq_len}"
            ))
        return stages
    
    def get_current_seq_len(self) -> int:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ—Ç–æ—á–Ω—É seq_len"""
        return self.stages[self.current_stage].seq_len
    
    def advance_epoch(self):
        """–ü–µ—Ä–µ–π—Ç–∏ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –µ–ø–æ—Ö–∏"""
        self.current_epoch_in_stage += 1
        if self.current_epoch_in_stage >= self.stages[self.current_stage].epochs:
            if self.current_stage < len(self.stages) - 1:
                self.current_stage += 1
                self.current_epoch_in_stage = 0
                print(f"üìà Curriculum: {self.stages[self.current_stage].description}")
    
    def is_complete(self) -> bool:
        """–ß–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ curriculum"""
        return (self.current_stage == len(self.stages) - 1 and 
                self.current_epoch_in_stage >= self.stages[-1].epochs)
    
    def get_current_stage_info(self) -> str:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø–æ—Ç–æ—á–Ω–∏–π –µ—Ç–∞–ø"""
        stage = self.stages[self.current_stage]
        return f"{stage.description} (epoch {self.current_epoch_in_stage + 1}/{stage.epochs})"


