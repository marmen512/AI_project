"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è
"""
import json
from pathlib import Path
from typing import Optional, Dict, Any
import os


class TrainingConfig:
    """–ë–∞–∑–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è"""
    
    def __init__(
        self,
        epochs: int = 10,
        batch_size: int = 4,
        learning_rate: float = 1e-4,
        max_recurrent_steps: int = 12,
        gradient_accumulation_steps: int = 4,
        warmup_steps: int = 1000,
        **kwargs
    ):
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.max_recurrent_steps = max_recurrent_steps
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.warmup_steps = warmup_steps
        self.extra_params = kwargs
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É–≤–∞—Ç–∏ –≤ —Å–ª–æ–≤–Ω–∏–∫"""
        return {
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate,
            'max_recurrent_steps': self.max_recurrent_steps,
            'gradient_accumulation_steps': self.gradient_accumulation_steps,
            'warmup_steps': self.warmup_steps,
            **self.extra_params
        }


class AutoTrainingConfig:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞ —Ä–µ—Å—É—Ä—Å—ñ–≤
    DEPRECATED: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ config.yaml –∑ runtime.bootstrap –∑–∞–º—ñ—Å—Ç—å —Ü—å–æ–≥–æ
    """
    import warnings
    warnings.warn(
        "AutoTrainingConfig –∑–∞—Å—Ç–∞—Ä—ñ–≤. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ config.yaml –∑ runtime.bootstrap",
        DeprecationWarning,
        stacklevel=2
    )
    
    def __init__(self, dataset_path: str | Path, models_dir: str | Path = None):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        
        Args:
            dataset_path: –®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
            models_dir: –®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –∑ –º–æ–¥–µ–ª—è–º–∏ (–¥–ª—è –æ—Ü—ñ–Ω–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤)
        """
        self.dataset_path = Path(dataset_path)
        self.models_dir = Path(models_dir) if models_dir else None
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É (–±–µ–∑ side-effects)
        self.dataset_size = self._get_dataset_size()
        self.dataset_samples = self._count_samples()
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ (–±–µ–∑ side-effects)
        self.config = self._auto_configure()
    
    def _get_dataset_size(self) -> int:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ —Ä–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—É –≤ –±–∞–π—Ç–∞—Ö"""
        if not self.dataset_path.exists():
            return 0
        return self.dataset_path.stat().st_size
    
    def _count_samples(self) -> int:
        """–ü—ñ–¥—Ä–∞—Ö—É–≤–∞—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤ —É –¥–∞—Ç–∞—Å–µ—Ç—ñ"""
        if not self.dataset_path.exists():
            return 0
        
        try:
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                return len(data)
            elif isinstance(data, dict) and 'data' in data:
                return len(data['data'])
            return 0
        except:
            return 0
    
    def _estimate_available_memory(self) -> float:
        """–û—Ü—ñ–Ω–∏—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—É –ø–∞–º'—è—Ç—å (GB)"""
        try:
            import psutil
            return psutil.virtual_memory().available / (1024 ** 3)
        except:
            # –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ 8GB
            return 8.0
    
    def _auto_configure(self) -> TrainingConfig:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏"""
        samples = self.dataset_samples
        dataset_size_mb = self.dataset_size / (1024 * 1024)
        available_memory_gb = self._estimate_available_memory()
        
        # –í–∏–∑–Ω–∞—á–∏—Ç–∏ batch_size –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–æ–∑–º—ñ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞ –ø–∞–º'—è—Ç—ñ
        if samples < 100:
            batch_size = 2
            epochs = 20  # –ë—ñ–ª—å—à–µ –µ–ø–æ—Ö –¥–ª—è –º–∞–ª–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
        elif samples < 500:
            batch_size = 4
            epochs = 15
        elif samples < 2000:
            batch_size = 4 if available_memory_gb < 8 else 8
            epochs = 12
        elif samples < 10000:
            batch_size = 8 if available_memory_gb < 16 else 16
            epochs = 10
        else:
            batch_size = 16 if available_memory_gb < 16 else 32
            epochs = 8
        
        # Learning rate –Ω–∞ –æ—Å–Ω–æ–≤—ñ batch_size
        learning_rate = 1e-4
        if batch_size >= 16:
            learning_rate = 2e-4
        elif batch_size <= 2:
            learning_rate = 5e-5
        
        # Gradient accumulation –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        gradient_accumulation_steps = max(1, 16 // batch_size)
        
        # Warmup steps –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–æ–∑–º—ñ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç—É
        batches_per_epoch = max(1, samples // batch_size)
        warmup_steps = min(2000, max(100, batches_per_epoch * 2))
        
        # Max recurrent steps
        max_recurrent_steps = 12
        if samples > 10000:
            max_recurrent_steps = 16
        elif samples < 100:
            max_recurrent_steps = 8
        
        return TrainingConfig(
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate,
            max_recurrent_steps=max_recurrent_steps,
            gradient_accumulation_steps=gradient_accumulation_steps,
            warmup_steps=warmup_steps
        )
    
    def get_config(self) -> TrainingConfig:
        """–û—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é"""
        return self.config
    
    def print_summary(self) -> None:
        """–í–∏–≤–µ—Å—Ç–∏ –ø—ñ–¥—Å—É–º–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó"""
        print("\n" + "=" * 70)
        print("‚öôÔ∏è  –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ê –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –ù–ê–í–ß–ê–ù–ù–Ø")
        print("=" * 70)
        print(f"\nüìä –ê–Ω–∞–ª—ñ–∑ –¥–∞—Ç–∞—Å–µ—Ç—É:")
        print(f"   - –§–∞–π–ª: {self.dataset_path.name}")
        print(f"   - –†–æ–∑–º—ñ—Ä: {self.dataset_size / (1024*1024):.2f} MB")
        print(f"   - –ü—Ä–∏–∫–ª–∞–¥—ñ–≤: {self.dataset_samples:,}")
        
        print(f"\nüéØ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
        config = self.config
        print(f"   - –ï–ø–æ—Ö: {config.epochs}")
        print(f"   - Batch size: {config.batch_size}")
        print(f"   - Learning rate: {config.learning_rate:.2e}")
        print(f"   - Max recurrent steps: {config.max_recurrent_steps}")
        print(f"   - Gradient accumulation: {config.gradient_accumulation_steps}")
        print(f"   - Warmup steps: {config.warmup_steps}")
        
        # –û—Ü—ñ–Ω–∫–∞ —á–∞—Å—É
        batches_per_epoch = max(1, self.dataset_samples // config.batch_size)
        total_batches = batches_per_epoch * config.epochs
        estimated_time_min = total_batches * 0.5  # –ü—Ä–∏–±–ª–∏–∑–Ω–æ 0.5 —Å–µ–∫ –Ω–∞ –±–∞—Ç—á
        
        print(f"\n‚è±Ô∏è  –û—Ü—ñ–Ω–∫–∞:")
        print(f"   - –ë–∞—Ç—á—ñ–≤ –Ω–∞ –µ–ø–æ—Ö—É: {batches_per_epoch}")
        print(f"   - –ó–∞–≥–∞–ª–æ–º –±–∞—Ç—á—ñ–≤: {total_batches}")
        print(f"   - –û—Ä—ñ—î–Ω—Ç–æ–≤–Ω–∏–π —á–∞—Å: ~{estimated_time_min/60:.1f} —Ö–≤–∏–ª–∏–Ω")
        print("=" * 70 + "\n")
    
    def save_config(self, output_file: str = "training_config.json") -> None:
        """–ó–±–µ—Ä–µ–≥—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –≤ JSON"""
        config_dict = {
            'dataset': {
                'path': str(self.dataset_path),
                'size_mb': self.dataset_size / (1024 * 1024),
                'samples': self.dataset_samples
            },
            'config': self.config.to_dict()
        }
        
        output_path = Path(output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {output_path}")


def main():
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó"""
    import sys
    
    if len(sys.argv) > 1:
        dataset_path = sys.argv[1]
    else:
        # –®—É–∫–∞—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç —É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –º—ñ—Å—Ü—è—Ö
        project_root = Path(__file__).parent.parent
        possible_datasets = [
            project_root / "phi3_training_dataset.json",
            project_root / "datasets" / "train.json",
            Path("phi3_training_dataset.json"),
        ]
        
        dataset_path = None
        for path in possible_datasets:
            if path.exists():
                dataset_path = path
                break
        
        if dataset_path is None:
            print("‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–∫–∞–∂—ñ—Ç—å —à–ª—è—Ö: python training_config.py <dataset_path>")
            return
    
    auto_config = AutoTrainingConfig(dataset_path)
    auto_config.print_summary()
    auto_config.save_config()


if __name__ == "__main__":
    main()





















