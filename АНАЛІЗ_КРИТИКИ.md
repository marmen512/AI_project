# –ê–Ω–∞–ª—ñ–∑ –∫—Ä–∏—Ç–∏–∫–∏ –ø—Ä–æ–µ–∫—Ç—É "2"

## üéØ –ó–ê–ì–ê–õ–¨–ù–ê –û–¶–Ü–ù–ö–ê

**–ö—Ä–∏—Ç–∏–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –Ω–∞ ~85-90%** - –∞–≤—Ç–æ—Ä –¥–æ–±—Ä–µ —Ä–æ–∑—É–º—ñ—î –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ —Ç–∞ –º–∞—î —á—ñ—Ç–∫—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó.

---

## ‚úÖ –©–û –ü–†–ê–í–ò–õ–¨–ù–û (–ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –≤ –∫–æ–¥—ñ)

### 1. **"–ù–µ–º–∞—î core —à–∞—Ä—É"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –î—ñ–π—Å–Ω–æ –Ω–µ–º–∞—î –ø–∞–ø–∫–∏ `core/` –∑ `interfaces.py`, `types.py`, `constants.py`
- –ë—ñ–∑–Ω–µ—Å-–ª–æ–≥—ñ–∫–∞ —Ä–æ–∑–∫–∏–¥–∞–Ω–∞ –ø–æ —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥—É–ª—è—Ö
- –ù–µ–º–∞—î —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö dataclasses –¥–ª—è Batch/Output

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ —Å–ø—Ä–∞–≤–¥—ñ –ø—Ä–æ–±–ª–µ–º–∞ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è.

---

### 2. **"Recursion = fixed depth"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `tiny_recursive_model/trm.py`:**
```python
def predict(self, seq, halt_prob_thres=0.5, max_deep_refinement_steps=12):
    # max_deep_refinement_steps - —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    for step in range_from_one(max_deep_refinement_steps):
        # ...
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- `max_deep_refinement_steps` –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –∑–∑–æ–≤–Ω—ñ, –Ω–µ –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –º–æ–¥–µ–ª–ª—é
- `halt_prob_thres` —Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ä–∞–Ω–Ω—å–æ–≥–æ –≤–∏—Ö–æ–¥—É, –Ω–µ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ depth
- –ù–µ–º–∞—î adaptive recursion gate —è–∫–∏–π –±–∏ –¥–∏–Ω–∞–º—ñ—á–Ω–æ –≤–∏–∑–Ω–∞—á–∞–≤ depth

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, recursion –¥—ñ–π—Å–Ω–æ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–π. –ú–æ–¥–µ–ª—å –Ω–µ –≤–∏—Ä—ñ—à—É—î —Å–∫—ñ–ª—å–∫–∏ "–¥—É–º–∞—Ç–∏".

---

### 3. **"–õ—ñ–Ω—ñ–π–Ω–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –∫–æ—Ä–ø—É—Å—É"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `train/datasets/trm_dataset.py`:**
```python
def __getitem__(self, idx):
    item = self._load_item(idx)
    context = item.get('context', item.get('input', ''))
    completion = item.get('completion', item.get('output', ''))
    # –¢–æ–∫–µ–Ω—ñ–∑—É—î –ø—Ä–æ—Å—Ç–æ context + completion
    # –ù–µ–º–∞—î doc_id, segment_id, document boundaries
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –∑–±–µ—Ä—ñ–≥–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –º–µ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
- –ù–µ–º–∞—î `doc_id` –∞–±–æ `segment_id`
- –ú–æ–∂–ª–∏–≤–∞ –∑–º—ñ—à—É–≤–∞–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –≤ –æ–¥–Ω–æ–º—É –±–∞—Ç—á—ñ
- Instruction format –º–æ–∂–µ –∑–º—ñ—à—É–≤–∞—Ç–∏—Å—è –∑ raw text

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ —Å–ø—Ä–∞–≤–∂–Ω—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–ª—è —è–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è.

---

### 4. **"Trainer —Ä–æ–±–∏—Ç—å –∑–∞–±–∞–≥–∞—Ç–æ"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `tiny_recursive_model/trainer.py`:**
- –í—Å—è –ª–æ–≥—ñ–∫–∞ –≤ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—ñ: loop, curriculum, checkpointing, validation
- `curriculum_scheduler`, `training_logger`, `resource_monitor` –ø–µ—Ä–µ–¥–∞—é—Ç—å—Å—è —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏, –∞–ª–µ –≤–∏–∫–ª–∏–∫–∞—é—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ Trainer
- –ù–µ–º–∞—î callback-based –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Trainer –∫–µ—Ä—É—î curriculum, checkpoint —á–∞—Å—Ç–æ—Ç–æ—é, validation - —Ü–µ anti-pattern
- –í–∞–∂–∫–æ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ –æ–∫—Ä–µ–º—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
- –í–∞–∂–∫–æ –¥–æ–¥–∞–≤–∞—Ç–∏ –Ω–æ–≤—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –±–µ–∑ –∑–º—ñ–Ω–∏ Trainer

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ —Å–ø—Ä–∞–≤–¥—ñ –ø—Ä–æ–±–ª–µ–º–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏.

---

### 5. **"–ù–µ–º–∞—î state object"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—É `TrainState` –∞–±–æ –ø–æ–¥—ñ–±–Ω–æ–≥–æ
- –í—Å–µ –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü—ñ—ó
- `checkpoint` –∑–±–µ—Ä—ñ–≥–∞—î —Å—Ç–∞–Ω, –∞–ª–µ –Ω–µ–º–∞—î —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–≥–æ state object

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –°–∫–ª–∞–¥–Ω–æ –≤—ñ–¥–Ω–æ–≤–ª—é–≤–∞—Ç–∏ —Å—Ç–∞–Ω
- –°–∫–ª–∞–¥–Ω–æ –¥–µ–±–∞–∂–∏—Ç–∏
- –°–∫–ª–∞–¥–Ω–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞—Ç–∏

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ –ø—Ä–æ–±–ª–µ–º–∞.

---

### 6. **"Entropy —Ä–∞—Ö—É—î—Ç—å—Å—è –¥–ª—è –≥–∞–ª–æ—á–∫–∏"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `train/logging.py`:**
```python
entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1).mean().item()
self.metrics['entropies'].append(entropy)
# ...
summary['avg_entropy'] = sum(self.metrics['entropies']) / len(self.metrics['entropies'])
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Entropy –ø—Ä–æ—Å—Ç–æ –ª–æ–≥—É—î—Ç—å—Å—è, –±–µ–∑ –∞–Ω–∞–ª—ñ–∑—É
- –ù–µ–º–∞—î –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è entropy vs depth
- –ù–µ–º–∞—î –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è entropy vs epoch
- –ù–µ–º–∞—î entropy delta (–∑–º—ñ–Ω–∏ –º—ñ–∂ –µ–ø–æ—Ö–∞–º–∏)

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, entropy –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.

---

### 7. **"–ù–µ–º–∞—î qualitative eval"** ‚ö†Ô∏è –ß–ê–°–¢–ö–û–í–û –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –Ñ `train/evaluators/` –∑ —Ä—ñ–∑–Ω–∏–º–∏ evaluators (ARC, Sudoku, Maze, General)
- –Ñ `scripts/evaluate_model.py`
- –ê–ª–µ –Ω–µ –±–∞—á—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è eval_samples/ –∑ –ø—Ä–∏–∫–ª–∞–¥–∞–º–∏ –ø–æ –µ–ø–æ—Ö–∞—Ö

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ß–∞—Å—Ç–∫–æ–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ - —î evaluators, –∞–ª–µ –Ω–µ–º–∞—î –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.

---

### 8. **"–ù–µ–º–∞—î deterministic mode"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–¥–ª—è inference)

**–§–∞–∫—Ç–∏ –∑ `inference/model_inference.py`:**
- –ù–µ –±–∞—á—É `torch.manual_seed()` –≤ inference
- –ù–µ –±–∞—á—É `torch.use_deterministic_algorithms()`

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, –Ω–µ–º–∞—î deterministic mode –¥–ª—è debug.

---

### 9. **"–ù–µ–º–∞—î timeout / max_compute"** ‚ö†Ô∏è –ß–ê–°–¢–ö–û–í–û –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –í `trainer.py` —î timeout –¥–ª—è batch —Ç–∞ step (max_batch_time, max_step_time)
- –ê–ª–µ –≤ `trm.py.predict()` –Ω–µ–º–∞—î timeout –¥–ª—è recursion
- `max_deep_refinement_steps` - —Ü–µ –Ω–µ timeout, –∞ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∞ –º–µ–∂–∞

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ß–∞—Å—Ç–∫–æ–≤–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ - —î timeout –¥–ª—è training, –∞–ª–µ –Ω–µ –¥–ª—è inference recursion.

---

## ‚ö†Ô∏è –©–û –ù–ï –í–ü–ï–í–ù–ï–ù–ò–ô

### **"Memory –Ω–µ –º–∞—î –ø–æ–ª—ñ—Ç–∏–∫–∏"**

**–§–∞–∫—Ç–∏:**
- –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ memory –≤ `tiny_recursive_model/trm.py`
- –ú–æ–∂–µ –±—É—Ç–∏ memory –≤ RAG —á–∞—Å—Ç–∏–Ω—ñ (`rag/memory_store.py`), –∞–ª–µ —Ü–µ —ñ–Ω—à–∞ —Ç–µ–º–∞

**–í–∏—Å–Ω–æ–≤–æ–∫:** –Ø–∫—â–æ –º–æ–≤–∞ –ø—Ä–æ memory –≤ —Å–∞–º—ñ–π –º–æ–¥–µ–ª—ñ - –Ω–µ –≤–ø–µ–≤–Ω–µ–Ω–∏–π, –º–æ–∂–ª–∏–≤–æ —ó—ó –≤–∑–∞–≥–∞–ª—ñ –Ω–µ–º–∞—î. –Ø–∫—â–æ –ø—Ä–æ RAG memory - —Ü–µ –æ–∫—Ä–µ–º–∞ —Ç–µ–º–∞.

---

## üìä –ü–Ü–î–°–£–ú–û–ö

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (~85-90%):
1. ‚úÖ –ù–µ–º–∞—î core —à–∞—Ä—É
2. ‚úÖ Recursion = fixed depth (–Ω–µ adaptive)
3. ‚úÖ –õ—ñ–Ω—ñ–π–Ω–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –±–µ–∑ document boundaries
4. ‚úÖ Trainer –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π (–Ω–µ–º–∞—î callbacks)
5. ‚úÖ –ù–µ–º–∞—î state object
6. ‚úÖ Entropy –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
7. ‚úÖ –ù–µ–º–∞—î deterministic mode (–¥–ª—è inference)
8. ‚ö†Ô∏è –ù–µ–º–∞—î timeout –¥–ª—è inference recursion (—î –¥–ª—è training)
9. ‚ö†Ô∏è Qualitative eval —î, –∞–ª–µ –Ω–µ–º–∞—î –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤

### ‚ö†Ô∏è –ù–ï –í–ü–ï–í–ù–ï–ù–ò–ô:
- Memory policy - –Ω–µ –∑–Ω–∞–π—à–æ–≤ memory –≤ –º–æ–¥–µ–ª—ñ (–º–æ–∂–µ —ó—ó –Ω–µ–º–∞—î?)

---

---

## üîç –î–û–î–ê–¢–ö–û–í–Ü –ü–£–ù–ö–¢–ò –ö–†–ò–¢–ò–ö–ò (–ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ)

–ê–≤—Ç–æ—Ä —Ç–∞–∫–æ–∂ –≤–∫–∞–∑–∞–≤ –Ω–∞ 7 –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø—Ä–æ–±–ª–µ–º, –≤—Å—ñ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—é—Ç—å—Å—è:

### 1Ô∏è‚É£ **TRM ‚â† reasoning loop** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- –ù–µ–º–∞—î thinking cost –≤ loss (`L_total = L_task + Œª * recursion_steps`)
- –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º—ñ–∑—É—î —è–∫—ñ—Å—Ç—å output, –Ω–µ –º—ñ–Ω—ñ–º—ñ–∑—É—î thinking cost
- **–ö–†–ò–¢–ò–ß–ù–û** - —Ü–µ –≥–ª–∏–±—à–∞ –ø—Ä–æ–±–ª–µ–º–∞ –Ω—ñ–∂ –ø—Ä–æ—Å—Ç–æ fixed depth

### 2Ô∏è‚É£ **Curriculum —Å–ª–∞–±–∫–∏–π** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- –ö–µ—Ä—É—î —Ç—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (seq_len, dim, batch)
- –ù–ï –∫–µ—Ä—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é –∑–∞–¥–∞—á—ñ, —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–∏—Ö, reasoning length

### 3Ô∏è‚É£ **–ù–µ–º–∞—î contamination control** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- Pretrain/instruction/eval –º–æ–∂—É—Ç—å –∑–º—ñ—à—É–≤–∞—Ç–∏—Å—å
- –ù–µ–º–∞—î fingerprinting, split manifest
- –†–∏–∑–∏–∫ eval leakage

### 4Ô∏è‚É£ **Trainer –Ω–µ —Ç–µ—Å—Ç–æ–≤–∞–Ω–∏–π** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- –ù–µ–º–∞—î unit-—Ç–µ—Å—Ç—ñ–≤ –¥–ª—è Trainer
- Research shortcut —è–∫–∏–π —Ç—Ä–µ–±–∞ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏

### 5Ô∏è‚É£ **ResourceMonitor –Ω–µ –≤–ø–ª–∏–≤–∞—î** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- –¢—ñ–ª—å–∫–∏ –ª–æ–≥—É—î, –Ω–µ throttle'–∏—Ç—å, –Ω–µ –∑–º—ñ–Ω—é—î batch size

### 6Ô∏è‚É£ **–ù–µ–º–∞—î best model checkpointing** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- Checkpoint –ø–æ step, –Ω–µ –ø–æ —è–∫–æ—Å—Ç—ñ
- –ü–∞—Ä–∞–º–µ—Ç—Ä `save_best_model` —î, –∞–ª–µ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó –Ω–µ–º–∞—î

### 7Ô∏è‚É£ **–ù–µ–º–∞—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Inference/Training** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û
- –°–ø—ñ–ª—å–Ω–∏–π `TRMConfig` –¥–ª—è –æ–±–æ—Ö
- –ü–æ—Ç—Ä—ñ–±–Ω–æ `InferConfig` vs `TrainConfig`

**–î–∏–≤. `–ì–õ–ò–ë–û–ö–ò–ô_–ê–ù–ê–õ–Ü–ó_–ö–†–ò–¢–ò–ö–ò.md` –¥–ª—è –¥–µ—Ç–∞–ª–µ–π.**

---

## üéØ –§–Ü–ù–ê–õ–¨–ù–ò–ô –í–ò–°–ù–û–í–û–ö

**–ö—Ä–∏—Ç–∏–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –Ω–∞ ~95-100%** - –∞–≤—Ç–æ—Ä –¥–æ–±—Ä–µ —Ä–æ–∑—É–º—ñ—î –í–°–Ü –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏.

### –†–µ–∞–ª—å–Ω–∏–π —Å—Ç–∞–Ω –ø—Ä–æ—î–∫—Ç—É:

üü° **Research-grade, –Ω–µ production**

‚úÖ **–°–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏:**
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞ —ñ–¥–µ—è TRM
- –ß–∏—Å—Ç–∞ –º–æ–¥—É–ª—å–Ω—ñ—Å—Ç—å (runtime.bootstrap, factories/)
- CPU-—Ñ–æ–∫—É—Å
- –†–µ–∞–ª—å–Ω–∞ —Ä–µ–∫—É—Ä—Å—ñ—è (–Ω–µ fake)

‚ùå **–°–ª–∞–±–∫—ñ —Å—Ç–æ—Ä–æ–Ω–∏ (–≤—Å—ñ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω—ñ):**
- Fixed depth (–Ω–µ adaptive)
- –ù–µ–º–∞—î thinking cost –≤ loss ‚ö†Ô∏è **–ö–†–ò–¢–ò–ß–ù–û**
- –õ—ñ–Ω—ñ–π–Ω–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –±–µ–∑ boundaries
- Trainer –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π (–Ω–µ–º–∞—î callbacks)
- –ù–µ–º–∞—î state object
- Entropy –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è
- Curriculum –∫–µ—Ä—É—î —Ç—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- –ù–µ–º–∞—î contamination control
- Trainer –Ω–µ —Ç–µ—Å—Ç–æ–≤–∞–Ω–∏–π
- ResourceMonitor –Ω–µ –≤–ø–ª–∏–≤–∞—î
- –ù–µ–º–∞—î best model checkpointing
- –ù–µ–º–∞—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Inference/Training

---

## üöÄ –ü–†–ò–û–†–ò–¢–ï–¢–ò –î–õ–Ø PRODUCTION

### –ö—Ä–∏—Ç–∏—á–Ω—ñ (—Ç—Ä–µ–±–∞ –≤–ø—Ä–æ–≤–∞–¥–∏—Ç–∏):

1. **Thinking cost –≤ loss** - `L_total = L_task + Œª * recursion_steps` ‚ö†Ô∏è **–ù–ê–ô–ö–†–ò–¢–ò–ß–ù–Ü–®–ï**
2. **Best model checkpointing** - best_loss.ckpt, best_eval_score.ckpt
3. **Contamination control** - Dataset fingerprinting, split manifest
4. **Trainer —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è** - Unit tests, mockable components
5. **Adaptive recursion gate** - –ú–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î —Å–∫—ñ–ª—å–∫–∏ –¥—É–º–∞—Ç–∏

### –í–∞–∂–ª–∏–≤—ñ (–ø–æ–∫—Ä–∞—â–∞—Ç—å —è–∫—ñ—Å—Ç—å):

6. **Callback-based trainer** - –†–æ–∑–¥—ñ–ª–∏—Ç–∏ –ª–æ–≥—ñ–∫—É
7. **Document-aware dataset** - –ú–µ–∂—ñ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
8. **Curriculum –¥–ª—è —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ** - Task difficulty, data types
9. **ResourceMonitor –∫–æ–Ω—Ç—Ä–æ–ª—å** - Auto throttle, batch shrink
10. **TrainState + resume** - –¶–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π state
11. **Entropy-driven early stop** - –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü—ñ—ó
12. **Qualitative eval snapshots** - –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —è–∫–æ—Å—Ç—ñ
13. **–†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Inference/Training** - InferConfig vs TrainConfig

---

## üîç –î–û–î–ê–¢–ö–û–í–Ü –ö–†–ò–¢–ò–ß–ù–Ü –ú–û–ú–ï–ù–¢–ò (—â–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ –≤ –ø–µ—Ä—à–æ–º—É –∞–Ω–∞–ª—ñ–∑—ñ)

### 1. **TRM ‚â† reasoning loop (–≥–ª–∏–±—à–∞ –ø—Ä–æ–±–ª–µ–º–∞, –Ω—ñ–∂ fixed depth)** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- `recursion = repeated refinement` (–ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ –≤ –∫–æ–¥—ñ)
- `halt_prob = heuristic exit` (—î –≤ –∫–æ–¥—ñ)
- **–ù–µ–º–∞—î –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ–≥–æ —Å–∏–≥–Ω–∞–ª—É "—è –∑–∞–≤–µ—Ä—à–∏–≤ –º–∏—Å–ª–µ–Ω–Ω—è"**
- **–ù–µ–º–∞—î thinking cost –≤ loss**

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –£ trainer.py loss –æ–±—á–∏—Å–ª—é—î—Ç—å—Å—è —Ç–∞–∫:
loss, (main_loss, halt_loss), outputs, latents, pred, halt = self.model(...)
# –ù–µ–º–∞—î: L_total = L_task + Œª * recursion_steps
```

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º—ñ–∑—É—î —è–∫—ñ—Å—Ç—å output –ø—ñ—Å–ª—è N –ø–æ–≤—Ç–æ—Ä—ñ–≤
- –ù–ï –º—ñ–Ω—ñ–º—ñ–∑—É—î thinking cost
- –ù–µ–º–∞—î pressure to stop early
- –ù–µ–º–∞—î penalty –∑–∞ –∑–∞–π–≤—É recursion

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ö—Ä–∏—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –ë–µ–∑ thinking cost –Ω–∞–≤—ñ—Ç—å adaptive gate –±—É–¥–µ –æ–±–º–∞–Ω—é–≤–∞—Ç–∏.

---

### 2. **Curriculum scheduler —Ñ–æ—Ä–º–∞–ª—å–Ω–æ —î, –∞–ª–µ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ —Å–ª–∞–±–∫–∏–π** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `train/curriculum/curriculum_scheduler.py`:**
```python
class CurriculumStage:
    name: str
    seq_len: int      # –¢—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    dim: int
    batch: int
    epochs: int
    # –ù–µ–º–∞—î: task_difficulty, data_types, reasoning_length
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Curriculum –∫–µ—Ä—É—î –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (seq_len, dim, batch)
- –ù–ï –∫–µ—Ä—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é –∑–∞–¥–∞—á—ñ
- –ù–µ–º–∞—î progression –ø–æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–∏—Ö
- –ù–µ–º–∞—î progression –ø–æ reasoning length

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ –≤—á–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–¥–∞—á—ñ —Ä–∞–Ω—ñ—à–µ –ø—Ä–æ—Å—Ç–∏—Ö
- –ù–µ–º–∞—î "learning trajectory"

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, curriculum —Å–ø—Ä–∞–≤–¥—ñ —Å–ª–∞–±–∫–∏–π.

---

### 3. **Dataset: –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –∫–æ–Ω—Ç—Ä–æ–ª—å contamination** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ `contamination`, `fingerprint`, `manifest`, `split` –≤ `train/datasets/`
- –ù–µ–º–∞—î dataset fingerprinting
- –ù–µ–º–∞—î hash-based –∫–æ–Ω—Ç—Ä–æ–ª—é

**–ü—Ä–æ–±–ª–µ–º–∞:**
- pretrain / instruction / eval –º–æ–∂—É—Ç—å –∑–º—ñ—à—É–≤–∞—Ç–∏—Å—å
- –ù–µ–º–∞—î dataset split manifest
- –†–∏–∑–∏–∫ eval leakage
- –†–∏–∑–∏–∫ instruction bias –≤ pretrain

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ —Å–ø—Ä–∞–≤–∂–Ω—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–ª—è production.

---

### 4. **Trainer –Ω–µ –ø—Ä–æ—Å—Ç–æ "–ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π" ‚Äî –≤—ñ–Ω –Ω–µ —Ç–µ—Å—Ç–æ–≤–∞–Ω–∏–π** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- `tests/test_trm.py` - —Ç—ñ–ª—å–∫–∏ –±–∞–∑–æ–≤—ñ —Ç–µ—Å—Ç–∏ –º–æ–¥–µ–ª—ñ
- –ù–µ–º–∞—î unit-—Ç–µ—Å—Ç—ñ–≤ –¥–ª—è Trainer
- –ù–µ–º–∞—î mockable –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- Trainer = orchestration + logic –≤ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—ñ

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ë—É–¥—å-—è–∫–∞ –∑–º—ñ–Ω–∞ = —Ä–∏–∑–∏–∫ —Ä–µ–≥—Ä–µ—Å—ñ—ó
- –°–∫–ª–∞–¥–Ω–æ –≥–∞—Ä–∞–Ω—Ç—É–≤–∞—Ç–∏ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –ø—Ä–∏ –¥–æ–≤–≥–æ–º—É train

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ research shortcut, —è–∫–∏–π —Ç—Ä–µ–±–∞ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –ø–µ—Ä–µ–¥ production.

---

### 5. **ResourceMonitor —î, –∞–ª–µ –Ω–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `train/resource_monitor.py`:**
```python
def get_cpu_usage(self) -> float:
    return psutil.cpu_percent(interval=0.1)

def get_memory_usage(self) -> Dict[str, float]:
    # –¢—ñ–ª—å–∫–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è, –Ω–µ–º–∞—î auto throttle
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- CPU/RAM –ª–æ–≥—É—é—Ç—å—Å—è
- –ù–ï –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è
- –ù–µ–º–∞—î auto throttle
- –ù–µ–º–∞—î auto batch shrink
- –ù–µ–º–∞—î pause/resume –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ—Å—É—Ä—Å—ñ–≤

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ ‚â† –∫–æ–Ω—Ç—Ä–æ–ª—å.

---

### 6. **Checkpointing: –Ω–µ–º–∞—î –ø–æ–Ω—è—Ç—Ç—è "best model"** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `runtime/checkpointing.py` —Ç–∞ `trainer.py`:**
- Checkpoint –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –ø–æ —á–∞—Å—É (`checkpoint_interval`)
- –ù–µ–º–∞—î `best_loss.ckpt`
- –ù–µ–º–∞—î `best_entropy.ckpt`
- –ù–µ–º–∞—î `best_eval_score.ckpt`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ó–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è "–ø–æ —á–∞—Å—É", –Ω–µ "–ø–æ —è–∫–æ—Å—Ç—ñ"
- –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ –±—É—Ç–∏ –≤—Ç—Ä–∞—á–µ–Ω–∞
- Rollback –Ω–µ–º–æ–∂–ª–∏–≤–∏–π –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –ª–æ–≥—ñ–≤

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, —Ü–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –¥–æ–≤–≥–æ–≥–æ training.

---

### 7. **Inference ‚â† Training (–∞–ª–µ –∫–æ–¥ —á–∞—Å—Ç–∫–æ–≤–æ —Ü—å–æ–≥–æ –Ω–µ –≤–∏–∑–Ω–∞—î)** ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –ù–µ–º–∞—î –æ–∫—Ä–µ–º–æ–≥–æ `InferConfig` –∞–±–æ `TrainConfig`
- –°–ø—ñ–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º—ñ–∂ inference —Ç–∞ training
- –ù–µ–º–∞—î –∂–æ—Ä—Å—Ç–∫–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Ä–µ–∂–∏–º—ñ–≤

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Training-only –ª–æ–≥—ñ–∫–∞ –º–æ–∂–µ –ø–æ—Ç—Ä–∞–ø–∏—Ç–∏ –≤ inference
- –°–∫–ª–∞–¥–Ω–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ inference –æ–∫—Ä–µ–º–æ

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, –ø–æ—Ç—Ä—ñ–±–Ω–µ –∂–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è.

---

### 8. **Memory ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω–æ, —ó—ó –Ω–µ–º–∞—î** ‚úÖ –ü–Ü–î–¢–í–ï–†–î–ñ–ï–ù–û

**–§–∞–∫—Ç–∏:**
- –í TRM —è–∫ —Ç–∞–∫–æ–º—É memory –≤—ñ–¥—Å—É—Ç–Ω—è
- –Ñ recursion, –∞–ª–µ –Ω–µ–º–∞—î persistent working memory
- RAG memory ‚â† reasoning memory

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ, –º–æ—è –æ–±–µ—Ä–µ–∂–Ω—ñ—Å—Ç—å –±—É–ª–∞ –∫–æ—Ä–µ–∫—Ç–Ω–æ—é. Memory policy –∑–∞—Å—Ç–æ—Å–æ–≤–Ω–∞ –ª–∏—à–µ —è–∫—â–æ memory –¥–µ–∫–ª–∞—Ä—É—î—Ç—å—Å—è —è–∫ —á–∞—Å—Ç–∏–Ω–∞ TRM.

---

## üí° –ú–û–Ø –î–û–î–ê–¢–ö–û–í–ê –î–£–ú–ö–ê

**–¶–µ –Ω–µ –ø–æ–≥–∞–Ω–∏–π –∫–æ–¥.**
**–¶–µ –∫–æ–¥, —è–∫–∏–π –ø–µ—Ä–µ—Ä—ñ—Å —Å–≤–æ—é –ø–æ—á–∞—Ç–∫–æ–≤—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É.**

**–î–ª—è R&D:** –ö–æ–¥ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ö–æ—Ä–æ—à–∏–π –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤.

**–î–ª—è production:** –ü–æ—Ç—Ä—ñ–±–Ω—ñ –≤—Å—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è, –æ—Å–æ–±–ª–∏–≤–æ:
- ‚ö†Ô∏è **Thinking cost –≤ loss** (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è real reasoning)
- **Best model checkpointing** (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –¥–æ–≤–≥–∏—Ö train)
- **Contamination control** (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤)

