# üöÄ –î–≤–æ—Ñ–∞–∑–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∫–∞—Å—Ç–æ–º–Ω–æ—ó Transformer –º–æ–¥–µ–ª—ñ (TRM)

–¶–µ–π –ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª—ñ–∑—É—î **–¥–≤–æ—Ñ–∞–∑–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è** –∫–∞—Å—Ç–æ–º–Ω–æ—ó Transformer –º–æ–¥–µ–ª—ñ –∑ –Ω—É–ª—è, –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –¥–ª—è CPU-only –æ–±–ª–∞–¥–Ω–∞–Ω–Ω—è.

## üéØ –û–≥–ª—è–¥ –¥–≤–æ—Ñ–∞–∑–Ω–æ–≥–æ –ø—ñ–¥—Ö–æ–¥—É

### üìö –§–ê–ó–ê 1 - Language Pretraining
- **–ú–µ—Ç–∞**: –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å —Ä–æ–∑—É–º—ñ—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
- **–î–∞—Ç–∞—Å–µ—Ç**: Simple Wikipedia (plain text, ~20M —Ç–æ–∫–µ–Ω—ñ–≤)
- **Objective**: Causal Language Modeling
- **–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è**: Random weights (–ë–ï–ó pretrained –º–æ–¥–µ–ª–µ–π)
- **–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å**: –ú–∞–∫—Å–∏–º—É–º 3 epochs

### üéØ –§–ê–ó–ê 2 - Instruction Tuning  
- **–ú–µ—Ç–∞**: –ù–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –ø–æ–≤–æ–¥–∏—Ç–∏—Å—è —è–∫ AI-–∞—Å–∏—Å—Ç–µ–Ω—Ç
- **–î–∞—Ç–∞—Å–µ—Ç–∏**: Alpaca, SQuAD 1.1, SQuAD v2, DailyDialog
- **Objective**: Instruction Following
- **–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è**: Weights –∑ –§–ê–ó–ò 1
- **–¢—Ä–∏–≤–∞–ª—ñ—Å—Ç—å**: 1-2 epochs –∑ early stopping

## üîß –¢–µ—Ö–Ω—ñ—á–Ω—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

### –û–±–ª–∞–¥–Ω–∞–Ω–Ω—è
- **CPU**: Ryzen 5 3600 (6c/12t)
- **RAM**: 20 GB DDR4  
- **Swap**: 120 GB
- **GPU**: RX560 (–ù–ï –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è)

### –ú–æ–¥–µ–ª—å
- **–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞**: Small Transformer
- **–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**: ~15-25M (–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó)
- **Sequence Length**: 256 —Ç–æ–∫–µ–Ω—ñ–≤ (CPU-safe)
- **Tokenizer**: GPT-2 (–ù–ï –ø–µ—Ä–µ–Ω–∞–≤—á—É—î—Ç—å—Å—è)

## üöÄ –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç

### 1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞

```bash
# –ê–∫—Ç–∏–≤—É–≤–∞—Ç–∏ –≤—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ
source venv/bin/activate

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
pip install -r requirements.txt
```

### 2. –§–ê–ó–ê 1 - Language Pretraining

```bash
# –ü—ñ–¥–≥–æ—Ç—É–≤–∞—Ç–∏ plain text corpus
python scripts/prepare_phase1_dataset.py

# –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –§–ê–ó–£ 1
./start_phase1_pretraining.sh
```

**–û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –§–ê–ó–ò 1:**
- –§–∞–π–ª: `checkpoints/phase1/best_model.pt`
- –ú–æ–¥–µ–ª—å —Ä–æ–∑—É–º—ñ—î –±–∞–∑–æ–≤—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
- –ú–æ–∂–µ –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –∑–≤'—è–∑–Ω–∏–π —Ç–µ–∫—Å—Ç

### 3. –§–ê–ó–ê 2 - Instruction Tuning

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –§–ê–ó–£ 2 (—Ç—ñ–ª—å–∫–∏ –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –§–ê–ó–ò 1!)
./start_phase2_instruction_tuning.sh
```

**–û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –§–ê–ó–ò 2:**
- –§–∞–π–ª: `checkpoints/phase2/best_instruction_model.pt`
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è
- –†–æ–∑—É–º—ñ—î —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó —Ç–∞ –≤–∏–∫–æ–Ω—É—î –∑–∞–≤–¥–∞–Ω–Ω—è

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª—ñ–≤

```
‚îú‚îÄ‚îÄ üöÄ –°–∫—Ä–∏–ø—Ç–∏ –∑–∞–ø—É—Å–∫—É
‚îÇ   ‚îú‚îÄ‚îÄ start_phase1_pretraining.sh      # –ó–∞–ø—É—Å–∫ –§–ê–ó–ò 1
‚îÇ   ‚îî‚îÄ‚îÄ start_phase2_instruction_tuning.sh # –ó–∞–ø—É—Å–∫ –§–ê–ó–ò 2
‚îÇ
‚îú‚îÄ‚îÄ üìã –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
‚îÇ   ‚îú‚îÄ‚îÄ config/phase1_pretraining.yaml   # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –§–ê–ó–ò 1
‚îÇ   ‚îî‚îÄ‚îÄ config/phase2_instruction_tuning.yaml # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –§–ê–ó–ò 2
‚îÇ
‚îú‚îÄ‚îÄ üîß –°–∫—Ä–∏–ø—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
‚îÇ   ‚îú‚îÄ‚îÄ scripts/prepare_phase1_dataset.py    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ plain text
‚îÇ   ‚îú‚îÄ‚îÄ scripts/train_phase1_pretraining.py  # –ù–∞–≤—á–∞–Ω–Ω—è –§–ê–ó–ò 1
‚îÇ   ‚îî‚îÄ‚îÄ scripts/train_phase2_instruction_tuning.py # –ù–∞–≤—á–∞–Ω–Ω—è –§–ê–ó–ò 2
‚îÇ
‚îú‚îÄ‚îÄ üìö –î–∞—Ç–∞—Å–µ—Ç–∏
‚îÇ   ‚îú‚îÄ‚îÄ datasets/pretrain_text.txt       # Plain text –¥–ª—è –§–ê–ó–ò 1
‚îÇ   ‚îú‚îÄ‚îÄ datasets/alpaca.json            # Instruction dataset
‚îÇ   ‚îú‚îÄ‚îÄ datasets/squad.json             # Q&A dataset
‚îÇ   ‚îî‚îÄ‚îÄ datasets/dailydialog_minimal.json # Dialog dataset
‚îÇ
‚îî‚îÄ‚îÄ üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
    ‚îú‚îÄ‚îÄ checkpoints/phase1/             # –ú–æ–¥–µ–ª—ñ –§–ê–ó–ò 1
    ‚îú‚îÄ‚îÄ checkpoints/phase2/             # –ú–æ–¥–µ–ª—ñ –§–ê–ó–ò 2
    ‚îú‚îÄ‚îÄ logs/phase1/                    # –õ–æ–≥–∏ –§–ê–ó–ò 1
    ‚îî‚îÄ‚îÄ logs/phase2/                    # –õ–æ–≥–∏ –§–ê–ó–ò 2
```

## üìä –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –Ω–∞–≤—á–∞–Ω–Ω—è

### –§–ê–ó–ê 1 - Language Pretraining
```bash
# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
tail -f logs/phase1/phase1_pretraining.log

# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤
htop
```

### –§–ê–ó–ê 2 - Instruction Tuning
```bash
# –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å
tail -f logs/phase2/phase2_instruction_tuning.log

# –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑—É–ø–∏–Ω–∏—Ç—å—Å—è –ø—Ä–∏ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—ñ —è–∫–æ—Å—Ç—ñ
```

## ‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–Ω—ñ –≤–∏–º–æ–≥–∏

### ‚úÖ –î–æ–∑–≤–æ–ª–µ–Ω–æ
- **–§–ê–ó–ê 1**: –¢—ñ–ª—å–∫–∏ plain text (Simple Wikipedia)
- **–§–ê–ó–ê 2**: –¢—ñ–ª—å–∫–∏ instruction datasets (Alpaca, SQuAD, DailyDialog)
- Random initialization –≤ –§–ê–ó–Ü 1
- GPT-2 tokenizer (–±–µ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è)
- CPU-only –Ω–∞–≤—á–∞–Ω–Ω—è
- –ú–∞–∫—Å–∏–º—É–º 3 epochs –¥–ª—è –§–ê–ó–ò 1
- –ú–∞–∫—Å–∏–º—É–º 2 epochs –¥–ª—è –§–ê–ó–ò 2

### ‚ùå –ó–∞–±–æ—Ä–æ–Ω–µ–Ω–æ
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è instruction datasets –≤ –§–ê–ó–Ü 1
- –ó–º—ñ—à—É–≤–∞–Ω–Ω—è RAG –¥–∞–Ω–∏—Ö –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è
- Pretrained weights –≤ –§–ê–ó–Ü 1
- –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è tokenizer
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è GPU –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
- –ë—ñ–ª—å—à–µ 3 epochs –¥–ª—è –§–ê–ó–ò 1

## üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

### –ü—ñ—Å–ª—è –§–ê–ó–ò 1
```bash
# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è language modeling
python -c "
from transformers import GPT2Tokenizer
import torch
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å —Ç–∞ –∑–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç
"
```

### –ü—ñ—Å–ª—è –§–ê–ó–ò 2
```bash
# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è instruction following
python scripts/test_model.py \
    --model checkpoints/phase2/best_instruction_model.pt \
    --prompt "Explain what is artificial intelligence"
```

## üö® –£—Å—É–Ω–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞: "–ú–æ–¥–µ–ª—å –∑ –§–ê–ó–ò 1 –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞"
**–†—ñ—à–µ–Ω–Ω—è**: –°–ø–æ—á–∞—Ç–∫—É –∑–∞–≤–µ—Ä—à—ñ—Ç—å –§–ê–ó–£ 1
```bash
./start_phase1_pretraining.sh
```

### –ü—Ä–æ–±–ª–µ–º–∞: "Plain text corpus –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
**–†—ñ—à–µ–Ω–Ω—è**: –ü—ñ–¥–≥–æ—Ç—É–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –§–ê–ó–ò 1
```bash
python scripts/prepare_phase1_dataset.py
```

### –ü—Ä–æ–±–ª–µ–º–∞: "Instruction datasets –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ"
**–†—ñ—à–µ–Ω–Ω—è**: –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤ —É `datasets/`
```bash
ls -la datasets/*.json
```

### –ü—Ä–æ–±–ª–µ–º–∞: –í–∏—Å–æ–∫–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ
**–†—ñ—à–µ–Ω–Ω—è**: –ó–º–µ–Ω—à—ñ—Ç—å batch_size —É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
```yaml
training:
  batch_size: 1  # –ó–∞–º—ñ—Å—Ç—å 2-4
```

## üìà –û—á—ñ–∫—É–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏

### –§–ê–ó–ê 1 (Language Pretraining)
- **–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è**: 4-8 –≥–æ–¥–∏–Ω –Ω–∞ CPU
- **Loss**: –ó–Ω–∏–∂–µ–Ω–Ω—è –∑ ~8-10 –¥–æ ~3-4
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä—É—î –∑–≤'—è–∑–Ω–∏–π –∞–Ω–≥–ª—ñ–π—Å—å–∫–∏–π —Ç–µ–∫—Å—Ç

### –§–ê–ó–ê 2 (Instruction Tuning)  
- **–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è**: 1-3 –≥–æ–¥–∏–Ω–∏ –Ω–∞ CPU
- **Loss**: –ó–Ω–∏–∂–µ–Ω–Ω—è –∑ ~3-4 –¥–æ ~2-3
- **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –ú–æ–¥–µ–ª—å –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è —Ç–∞ –≤–∏–∫–æ–Ω—É—î —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—ó

## üéâ –§—ñ–Ω–∞–ª—å–Ω–∞ –º–æ–¥–µ–ª—å

–ü—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è –æ–±–æ—Ö —Ñ–∞–∑ –≤–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ:

- **–§–∞–π–ª**: `checkpoints/phase2/best_instruction_model.pt`
- **–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ**: 
  - –†–æ–∑—É–º—ñ–Ω–Ω—è –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤–∏
  - –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤'—è–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
  - –í—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è
  - –í–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–æ—Å—Ç–∏—Ö —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ–π
- **–†–æ–∑–º—ñ—Ä**: ~15-25M –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (CPU-friendly)
- **–ì–æ—Ç–æ–≤–Ω—ñ—Å—Ç—å –¥–æ RAG**: –ú–æ–¥–µ–ª—å –º–æ–∂–Ω–∞ —Ä–æ–∑—à–∏—Ä–∏—Ç–∏ RAG —Å–∏—Å—Ç–µ–º–æ—é

## üìû –ü—ñ–¥—Ç—Ä–∏–º–∫–∞

–Ø–∫—â–æ –≤–∏–Ω–∏–∫–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏:

1. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ª–æ–≥–∏: `logs/phase1/` —Ç–∞ `logs/phase2/`
2. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ: `htop`
3. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤: `ls -la checkpoints/`
4. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å –∑ –º–µ–Ω—à–∏–º batch_size

---

**–£—Å–ø—ñ—à–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è! üöÄ**


