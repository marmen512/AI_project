#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∏–π —Ç–µ—Å—Ç –º–æ–¥–µ–ª—ñ –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç—É
"""
import sys
import torch
from pathlib import Path

# –î–æ–¥–∞—Ç–∏ —à–ª—è—Ö –¥–æ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from tiny_recursive_model import TinyRecursiveModel, TransformerBackbone
from tiny_recursive_model.utils import load_tokenizer
from train.constants import DEFAULT_TOKENIZER_NAME

def test_model_from_checkpoint():
    """–¢–µ—Å—Ç –º–æ–¥–µ–ª—ñ –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç—É"""
    print("=" * 60)
    print("üß™ –¢–ï–°–¢ –ú–û–î–ï–õ–Ü –ó –ß–ï–ö–ü–û–Ü–ù–¢–£")
    print("=" * 60)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
    print("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä–∞...")
    tokenizer, _, _ = load_tokenizer(DEFAULT_TOKENIZER_NAME)
    vocab_size = len(tokenizer)
    print(f"‚úÖ –¢–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ (vocab_size: {vocab_size})")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ (–∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó)
    dim = 768
    depth = 12
    seq_len = 1024
    
    print(f"\nüîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    print(f"   dim: {dim}")
    print(f"   depth: {depth}")
    print(f"   seq_len: {seq_len}")
    print(f"   vocab_size: {vocab_size}")
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ backbone
    backbone = TransformerBackbone(
        dim=dim,
        depth=depth,
        seq_len=seq_len,
        pretrained=True,
        model_name='gpt2'
    )
    
    # –°—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å
    model = TinyRecursiveModel(
        dim=dim,
        num_tokens=vocab_size,
        network=backbone,
        max_recursion_depth=20,
        adaptive_recursion=True
    )
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —á–µ–∫–ø–æ—ñ–Ω—Ç
    checkpoint_path = "checkpoints/best_loss.ckpt"
    print(f"\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—É: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á–µ–∫–ø–æ—ñ–Ω—Ç—É
        if 'model_state_dict' in checkpoint:
            model_state = checkpoint['model_state_dict']
            print(f"   –ó–Ω–∞–π–¥–µ–Ω–æ model_state_dict –∑ {len(model_state)} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        else:
            model_state = checkpoint
            print(f"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–µ—Å—å —á–µ–∫–ø–æ—ñ–Ω—Ç —è–∫ model_state_dict")
        
        model.load_state_dict(model_state)
        model.eval()
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑ —á–µ–∫–ø–æ—ñ–Ω—Ç—É")
        
        # –ü—Ä–æ—Å—Ç–∏–π —Ç–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        print(f"\nüß™ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó...")
        test_inputs = [
            "–ü—Ä–∏–≤—ñ—Ç! –Ø–∫ —Å–ø—Ä–∞–≤–∏?",
            "–©–æ —Ç–∞–∫–µ —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç?",
            "–ù–∞–ø–∏—à–∏ –ø—Ä–æ—Å—Ç–∏–π –∫–æ–¥ –Ω–∞ Python:",
            "–ü–æ—è—Å–Ω–∏ —â–æ —Ç–∞–∫–µ —Ä–µ–∫—É—Ä—Å—ñ—è:"
        ]
        
        for i, test_input in enumerate(test_inputs, 1):
            print(f"\n--- –¢–µ—Å—Ç {i} ---")
            print(f"–í—Ö—ñ–¥: {test_input}")
            
            try:
                # –¢–æ–∫–µ–Ω—ñ–∑—É–≤–∞—Ç–∏ –≤—Ö—ñ–¥
                inputs = tokenizer.encode(test_input, return_tensors='pt')
                
                # –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ predict –º–µ—Ç–æ–¥ TRM
                with torch.no_grad():
                    predictions, steps = model.predict(
                        inputs,
                        halt_prob_thres=0.5,
                        max_deep_refinement_steps=12
                    )
                
                # –î–µ–∫–æ–¥—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                generated_text = tokenizer.decode(predictions, skip_special_tokens=True)
                response = generated_text.strip()
                
                print(f"–í—ñ–¥–ø–æ–≤—ñ–¥—å: {response}")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ
                if len(response) > 0:
                    print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —É—Å–ø—ñ—à–Ω–∞")
                else:
                    print(f"‚ö†Ô∏è –ü–æ—Ä–æ–∂–Ω—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å")
                    
            except Exception as e:
                print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó: {e}")
        
        print(f"\n‚úÖ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ–∫–ø–æ—ñ–Ω—Ç—É: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_model_from_checkpoint()
