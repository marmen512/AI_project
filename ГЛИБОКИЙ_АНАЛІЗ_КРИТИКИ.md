# –ì–ª–∏–±–æ–∫–∏–π –∞–Ω–∞–ª—ñ–∑ –∫—Ä–∏—Ç–∏–∫–∏ –ø—Ä–æ–µ–∫—Ç—É "2" (–û–ù–û–í–õ–ï–ù–ò–ô)

## üîç –©–û –ë–£–õ–û –ü–†–û–ü–£–©–ï–ù–û –í –ü–ï–†–®–û–ú–£ –ê–ù–ê–õ–Ü–ó–Ü

–ê–≤—Ç–æ—Ä –∫—Ä–∏—Ç–∏–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∫–∞–∑–∞–≤ –Ω–∞ –≥–ª–∏–±—à—ñ –ø—Ä–æ–±–ª–µ–º–∏, —è–∫—ñ —è –Ω–µ –∑–∞—á–µ–ø–∏–≤.

---

## 1Ô∏è‚É£ TRM ‚â† reasoning loop (–≥–ª–∏–±—à–∞ –ø—Ä–æ–±–ª–µ–º–∞, –Ω—ñ–∂ fixed depth)

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û - –¶–ï –ö–†–ò–¢–ò–ß–ù–û

**–§–∞–∫—Ç–∏ –∑ –∫–æ–¥—É:**
```python
# tiny_recursive_model/trainer.py
loss, (main_loss, halt_loss), outputs, latents, pred, halt = self.model(...)
# –ù–µ–º–∞—î: L_total = L_task + Œª * recursion_steps
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Recursion = repeated refinement
- halt_prob = heuristic exit
- **–ù–µ–º–∞—î –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ–≥–æ —Å–∏–≥–Ω–∞–ª—É "—è –∑–∞–≤–µ—Ä—à–∏–≤ –º–∏—Å–ª–µ–Ω–Ω—è"**
- **–ù–µ–º–∞—î thinking cost –≤ loss**

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º—ñ–∑—É—î —è–∫—ñ—Å—Ç—å output –ø—ñ—Å–ª—è N –ø–æ–≤—Ç–æ—Ä—ñ–≤
- –ù–ï –º—ñ–Ω—ñ–º—ñ–∑—É—î thinking cost
- –ù–µ–º–∞—î pressure to stop early
- –ù–µ–º–∞—î penalty –∑–∞ –∑–∞–π–≤—É recursion

**–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è:**
```python
L_total = L_task + Œª * recursion_steps
```

**–í–∏—Å–Ω–æ–≤–æ–∫:** –ë–µ–∑ —Ü—å–æ–≥–æ –Ω–∞–≤—ñ—Ç—å adaptive gate –±—É–¥–µ –æ–±–º–∞–Ω—é–≤–∞—Ç–∏. –¶–µ –Ω–∞–π–∫—Ä–∏—Ç–∏—á–Ω—ñ—à–∞ –ø—Ä–æ–±–ª–µ–º–∞.

---

## 2Ô∏è‚É£ Curriculum scheduler —Ñ–æ—Ä–º–∞–ª—å–Ω–æ —î, –∞–ª–µ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ —Å–ª–∞–±–∫–∏–π

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `train/curriculum/curriculum_scheduler.py`:**
```python
@dataclass
class CurriculumStage:
    name: str
    seq_len: int      # –¢—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    dim: int
    batch: int
    epochs: int
    # –ù–µ–º–∞—î: task_difficulty, data_types, reasoning_length
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Curriculum –∫–µ—Ä—É—î –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (seq_len, dim, batch)
- –ù–ï –∫–µ—Ä—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é –∑–∞–¥–∞—á—ñ
- –ù–µ–º–∞—î progression –ø–æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–∏—Ö
- –ù–µ–º–∞—î progression –ø–æ reasoning length
- –ù–µ–º–∞—î progression –ø–æ task difficulty

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ú–æ–¥–µ–ª—å –º–æ–∂–µ –≤—á–∏—Ç–∏ —Å–∫–ª–∞–¥–Ω—ñ –∑–∞–¥–∞—á—ñ —Ä–∞–Ω—ñ—à–µ –ø—Ä–æ—Å—Ç–∏—Ö
- –ù–µ–º–∞—î "learning trajectory"

**–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è:**
Curriculum –º–∞—î –∫–µ—Ä—É–≤–∞—Ç–∏:
- max recursion
- max sequence
- —Ç–∏–ø–∞–º–∏ —Å–µ–º–ø–ª—ñ–≤ (easy ‚Üí hard)

---

## 3Ô∏è‚É£ Dataset: –≤—ñ–¥—Å—É—Ç–Ω—ñ–π –∫–æ–Ω—Ç—Ä–æ–ª—å contamination

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ `contamination`, `fingerprint`, `manifest`, `split` –≤ `train/datasets/`
- –ù–µ–º–∞—î dataset fingerprinting
- –ù–µ–º–∞—î hash-based –∫–æ–Ω—Ç—Ä–æ–ª—é

**–ü—Ä–æ–±–ª–µ–º–∞:**
- pretrain / instruction / eval –º–æ–∂—É—Ç—å –∑–º—ñ—à—É–≤–∞—Ç–∏—Å—å
- –ù–µ–º–∞—î dataset split manifest
- –†–∏–∑–∏–∫ eval leakage
- –†–∏–∑–∏–∫ instruction bias –≤ pretrain
- –ù–µ–∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω–∏–π overfit

**–©–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ:**
- dataset split manifest
- hash-based –∫–æ–Ω—Ç—Ä–æ–ª—å
- –∂–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Ä–µ–∂–∏–º—ñ–≤

---

## 4Ô∏è‚É£ Trainer –Ω–µ –ø—Ä–æ—Å—Ç–æ "–ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π" ‚Äî –≤—ñ–Ω –Ω–µ —Ç–µ—Å—Ç–æ–≤–∞–Ω–∏–π

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- `tests/test_trm.py` - —Ç—ñ–ª—å–∫–∏ –±–∞–∑–æ–≤—ñ —Ç–µ—Å—Ç–∏ –º–æ–¥–µ–ª—ñ
- –ù–µ–º–∞—î unit-—Ç–µ—Å—Ç—ñ–≤ –¥–ª—è Trainer
- –ù–µ–º–∞—î mockable –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- Trainer = orchestration + logic –≤ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—ñ

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ë—É–¥—å-—è–∫–∞ –∑–º—ñ–Ω–∞ = —Ä–∏–∑–∏–∫ —Ä–µ–≥—Ä–µ—Å—ñ—ó
- –°–∫–ª–∞–¥–Ω–æ –≥–∞—Ä–∞–Ω—Ç—É–≤–∞—Ç–∏ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –ø—Ä–∏ –¥–æ–≤–≥–æ–º—É train

**–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∞ –ø—Ä–∞–≤–¥–∞:**
–¶–µ –Ω–µ "anti-pattern", —Ü–µ research shortcut, —è–∫–∏–π —Ç—Ä–µ–±–∞ –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ –ø–µ—Ä–µ–¥ production-grade train.

---

## 5Ô∏è‚É£ ResourceMonitor —î, –∞–ª–µ –Ω–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `train/resource_monitor.py`:**
```python
def get_cpu_usage(self) -> float:
    return psutil.cpu_percent(interval=0.1)  # –¢—ñ–ª—å–∫–∏ –ª–æ–≥—É–≤–∞–Ω–Ω—è

def get_memory_usage(self) -> Dict[str, float]:
    # –¢—ñ–ª—å–∫–∏ –ø–æ–≤–µ—Ä—Ç–∞—î –¥–∞–Ω—ñ, –Ω–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è
```

**–ü—Ä–æ–±–ª–µ–º–∞:**
- CPU/RAM –ª–æ–≥—É—é—Ç—å—Å—è
- –ù–ï –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è
- –ù–µ–º–∞—î auto throttle
- –ù–µ–º–∞—î auto batch shrink
- –ù–µ–º–∞—î pause/resume –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ—Å—É—Ä—Å—ñ–≤

**–¶–µ –æ–∑–Ω–∞—á–∞—î:**
–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ ‚â† –∫–æ–Ω—Ç—Ä–æ–ª—å

---

## 6Ô∏è‚É£ Checkpointing: –Ω–µ–º–∞—î –ø–æ–Ω—è—Ç—Ç—è "best model"

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏ –∑ `runtime/checkpointing.py` —Ç–∞ `trainer.py`:**
- Checkpoint –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –ø–æ —á–∞—Å—É (`checkpoint_interval`)
- –ù–µ–º–∞—î `best_loss.ckpt`
- –ù–µ–º–∞—î `best_entropy.ckpt`
- –ù–µ–º–∞—î `best_eval_score.ckpt`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ó–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è "–ø–æ —á–∞—Å—É", –Ω–µ "–ø–æ —è–∫–æ—Å—Ç—ñ"
- –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ –±—É—Ç–∏ –≤—Ç—Ä–∞—á–µ–Ω–∞
- Rollback –Ω–µ–º–æ–∂–ª–∏–≤–∏–π –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –ª–æ–≥—ñ–≤

**–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è:**
- `best_loss.ckpt`
- `best_entropy.ckpt`
- `best_eval_score.ckpt`

---

## 7Ô∏è‚É£ Inference ‚â† Training (–∞–ª–µ –∫–æ–¥ —á–∞—Å—Ç–∫–æ–≤–æ —Ü—å–æ–≥–æ –Ω–µ –≤–∏–∑–Ω–∞—î)

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û

**–§–∞–∫—Ç–∏:**
- –ù–µ–º–∞—î –æ–∫—Ä–µ–º–æ–≥–æ `InferConfig` –∞–±–æ `TrainConfig`
- –°–ø—ñ–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º—ñ–∂ inference —Ç–∞ training
- –ù–µ–º–∞—î –∂–æ—Ä—Å—Ç–∫–æ–≥–æ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Ä–µ–∂–∏–º—ñ–≤

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Training-only –ª–æ–≥—ñ–∫–∞ –º–æ–∂–µ –ø–æ—Ç—Ä–∞–ø–∏—Ç–∏ –≤ inference
- –°–∫–ª–∞–¥–Ω–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ inference –æ–∫—Ä–µ–º–æ

**–†—ñ—à–µ–Ω–Ω—è:**
- –ñ–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è TrainConfig vs InferConfig
- –û–∫—Ä–µ–º—ñ entrypoints –±–µ–∑ shared state

---

## üß† –ü–†–û MEMORY ‚Äî –ü–Ü–î–¢–í–ï–†–î–ñ–ï–ù–û

### ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û (–º–æ—è –æ–±–µ—Ä–µ–∂–Ω—ñ—Å—Ç—å –±—É–ª–∞ –∫–æ—Ä–µ–∫—Ç–Ω–æ—é)

**–§–∞–∫—Ç–∏:**
- –í TRM —è–∫ —Ç–∞–∫–æ–º—É memory –≤—ñ–¥—Å—É—Ç–Ω—è
- –Ñ recursion, –∞–ª–µ –Ω–µ–º–∞—î persistent working memory
- RAG memory ‚â† reasoning memory

**–í–∏—Å–Ω–æ–≤–æ–∫:**
–ö—Ä–∏—Ç–∏–∫–∞ –ø—Ä–æ "memory policy" –∑–∞—Å—Ç–æ—Å–æ–≤–Ω–∞ –ª–∏—à–µ —è–∫—â–æ memory –¥–µ–∫–ª–∞—Ä—É—î—Ç—å—Å—è —è–∫ —á–∞—Å—Ç–∏–Ω–∞ TRM.
–£ –ø–æ—Ç–æ—á–Ω–æ–º—É –∫–æ–¥—ñ ‚Äî —ó—ó –ø—Ä–æ—Å—Ç–æ –Ω–µ–º–∞—î, —ñ —Ü–µ –æ–∫—Ä–µ–º–∏–π –ø—É–Ω–∫—Ç –Ω–µ–¥–æ–ª—ñ–∫—É, –∞ –Ω–µ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—ó.

---

## üèÅ –§–Ü–ù–ê–õ–¨–ù–ò–ô –í–ï–†–î–ò–ö–¢ (–£–ó–ê–ì–ê–õ–¨–ù–ï–ù–ò–ô)

### –¢–≤—ñ–π –∞–Ω–∞–ª—ñ–∑ (–ø–µ—Ä—à–∏–π):
- ‚úî –¢–µ—Ö–Ω—ñ—á–Ω–æ –∫–æ—Ä–µ–∫—Ç–Ω–∏–π
- ‚úî –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∏–π –∫–æ–¥–æ–º
- ‚úî –ë–µ–∑ —Ñ–∞–Ω—Ç–∞–∑—ñ–π
- ‚úî –†—ñ–≤–µ–Ω—å senior+/research engineer

### –†–µ–∞–ª—å–Ω–∏–π —Å—Ç–∞–Ω –ø—Ä–æ—î–∫—Ç—É:
- üü° **Research-grade, –Ω–µ production**
- üü° –î–æ–±—Ä–µ –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
- üî¥ –ù–µ–±–µ–∑–ø–µ—á–Ω–æ –¥–ª—è –¥–æ–≤–≥–æ–≥–æ train –±–µ–∑ –¥–æ–æ–ø—Ä–∞—Ü—é–≤–∞–Ω—å

### –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ:

**–¶–µ –Ω–µ –ø–æ–≥–∞–Ω–∏–π –∫–æ–¥.**  
**–¶–µ –∫–æ–¥, —è–∫–∏–π –ø–µ—Ä–µ—Ä—ñ—Å —Å–≤–æ—é –ø–æ—á–∞—Ç–∫–æ–≤—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É.**

### –ö—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç):

1. ‚ö†Ô∏è **Thinking cost –≤ loss** - –±–µ–∑ —Ü—å–æ–≥–æ TRM ‚â† reasoning
2. Best model checkpointing - –≤—Ç—Ä–∞—Ç–∞ –Ω–∞–π–∫—Ä–∞—â–æ—ó –º–æ–¥–µ–ª—ñ
3. Dataset contamination - eval leakage
4. ResourceMonitor –±–µ–∑ –≤–ø–ª–∏–≤—É - –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ ‚â† –∫–æ–Ω—Ç—Ä–æ–ª—å
5. Trainer –Ω–µ —Ç–µ—Å—Ç–æ–≤–∞–Ω–∏–π - —Ä–∏–∑–∏–∫ —Ä–µ–≥—Ä–µ—Å—ñ—ó
6. Curriculum —Å–ª–∞–±–∫–∏–π - —Ç—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
7. Inference/Training –Ω–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω—ñ

### –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏:

- Recursion = fixed depth (–Ω–µ adaptive)
- Trainer –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π (–Ω–µ–º–∞—î callbacks)
- –ù–µ–º–∞—î state object
- Document boundaries –Ω–µ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è
- Entropy –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
- –ù–µ–º–∞—î deterministic mode (–¥–ª—è inference)

---

## üìã –¢–û–ü-10 –ü–û–ö–†–ê–©–ï–ù–¨ (–û–ù–û–í–õ–ï–ù–ò–ô –ü–†–Ü–û–†–ò–¢–ï–¢)

1. ‚ö†Ô∏è **Thinking cost –≤ loss** - `L_total = L_task + Œª * recursion_steps` ‚ö†Ô∏è **–ù–ê–ô–ö–†–ò–¢–ò–ß–ù–Ü–®–ï**
2. Best model checkpointing - `best_loss.ckpt`, `best_eval_score.ckpt`
3. Dataset contamination control - manifest, hash-based –∫–æ–Ω—Ç—Ä–æ–ª—å
4. ResourceMonitor –∑ auto throttle - –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ ‚Üí –∫–æ–Ω—Ç—Ä–æ–ª—å
5. Adaptive recursion gate - –º–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î depth
6. Callback-based trainer - —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è orchestration —Ç–∞ logic
7. Document-aware dataset - doc_id, segment_id, boundaries
8. TrainState + resume - —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π state object
9. Curriculum –∫–µ—Ä—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é - –Ω–µ —Ç—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
10. InferConfig vs TrainConfig - –∂–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è

---

**–î–æ–¥–∞—Ç–∫–æ–≤—ñ –∑–∞—É–≤–∞–∂–µ–Ω–Ω—è –∞–≤—Ç–æ—Ä–∞ –∫—Ä–∏—Ç–∏–∫–∏ –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ —ñ –∫—Ä–∏—Ç–∏—á–Ω–æ –≤–∞–∂–ª–∏–≤—ñ!** ‚úÖ
