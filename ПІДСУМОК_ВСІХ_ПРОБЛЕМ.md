# –ü—ñ–¥—Å—É–º–æ–∫ –≤—Å—ñ—Ö –≤–∏—è–≤–ª–µ–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º –ø—Ä–æ–µ–∫—Ç—É "2"

## ‚úÖ –ö–†–ò–¢–ò–ö–ê –ü–†–ê–í–ò–õ–¨–ù–ê –ù–ê ~95%

–ê–≤—Ç–æ—Ä –∫—Ä–∏—Ç–∏–∫–∏ –≤–∏—è–≤–∏–≤ **–≤—Å—ñ –∫—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏** —ñ –Ω–∞–≤—ñ—Ç—å –≥–ª–∏–±—à—ñ, —è–∫—ñ —è —Å–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–ø—É—Å—Ç–∏–≤.

---

## üî¥ –ö–†–ò–¢–ò–ß–ù–Ü –ü–†–û–ë–õ–ï–ú–ò (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 1)

### 1. ‚ö†Ô∏è **TRM ‚â† reasoning loop - –ù–µ–º–∞—î thinking cost –≤ loss**

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –ó–∞—Ä–∞–∑:
loss, (main_loss, halt_loss), outputs, latents, pred, halt = self.model(...)
# –ù–µ–º–∞—î: L_total = L_task + Œª * recursion_steps
```

**–ù–∞—Å–ª—ñ–¥–æ–∫:**
- –ú–æ–¥–µ–ª—å –æ–ø—Ç–∏–º—ñ–∑—É—î —è–∫—ñ—Å—Ç—å output –ø—ñ—Å–ª—è N –ø–æ–≤—Ç–æ—Ä—ñ–≤
- –ù–ï –º—ñ–Ω—ñ–º—ñ–∑—É—î thinking cost
- –ù–µ–º–∞—î pressure to stop early
- Recursion = repeated refinement, –Ω–µ reasoning

**–†—ñ—à–µ–Ω–Ω—è:**
```python
L_total = L_task + Œª * recursion_steps
```

---

### 2. **Checkpointing: –Ω–µ–º–∞—î "best model"**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ó–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è –ø–æ —á–∞—Å—É (`checkpoint_interval`)
- –ù–µ–º–∞—î `best_loss.ckpt`, `best_eval_score.ckpt`
- –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ –±—É—Ç–∏ –≤—Ç—Ä–∞—á–µ–Ω–∞

**–†—ñ—à–µ–Ω–Ω—è:**
- `best_loss.ckpt`
- `best_entropy.ckpt`
- `best_eval_score.ckpt`

---

### 3. **Dataset contamination - –Ω–µ–º–∞—î –∫–æ–Ω—Ç—Ä–æ–ª—é**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ù–µ–º–∞—î dataset fingerprinting
- –ù–µ–º–∞—î hash-based –∫–æ–Ω—Ç—Ä–æ–ª—é
- pretrain/instruction/eval –º–æ–∂—É—Ç—å –∑–º—ñ—à—É–≤–∞—Ç–∏—Å—å
- –†–∏–∑–∏–∫ eval leakage

**–†—ñ—à–µ–Ω–Ω—è:**
- dataset split manifest
- hash-based –∫–æ–Ω—Ç—Ä–æ–ª—å
- –∂–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è —Ä–µ–∂–∏–º—ñ–≤

---

### 4. **ResourceMonitor –Ω–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ä—ñ—à–µ–Ω–Ω—è**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- CPU/RAM —Ç—ñ–ª—å–∫–∏ –ª–æ–≥—É—é—Ç—å—Å—è
- –ù–µ–º–∞—î auto throttle
- –ù–µ–º–∞—î auto batch shrink
- –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ ‚â† –∫–æ–Ω—Ç—Ä–æ–ª—å

**–†—ñ—à–µ–Ω–Ω—è:**
- Auto throttle –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ—Å—É—Ä—Å—ñ–≤
- Auto batch shrink –ø—Ä–∏ –Ω–µ—Å—Ç–∞—á—ñ –ø–∞–º'—è—Ç—ñ
- Pause/resume –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–µ—Å—É—Ä—Å—ñ–≤

---

### 5. **Trainer –Ω–µ —Ç–µ—Å—Ç–æ–≤–∞–Ω–∏–π**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ù–µ–º–∞—î unit-—Ç–µ—Å—Ç—ñ–≤ –¥–ª—è Trainer
- –ù–µ–º–∞—î mockable –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
- –ë—É–¥—å-—è–∫–∞ –∑–º—ñ–Ω–∞ = —Ä–∏–∑–∏–∫ —Ä–µ–≥—Ä–µ—Å—ñ—ó

**–†—ñ—à–µ–Ω–Ω—è:**
- Unit-—Ç–µ—Å—Ç–∏ –¥–ª—è Trainer
- Mockable –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
- Callback-based –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è

---

## üü° –ê–†–•–Ü–¢–ï–ö–¢–£–†–ù–Ü –ü–†–û–ë–õ–ï–ú–ò (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 2)

### 6. **Recursion = fixed depth (–Ω–µ adaptive)**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- `max_deep_refinement_steps` –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –∑–∑–æ–≤–Ω—ñ
- –ù–µ–º–∞—î adaptive recursion gate
- –ú–æ–¥–µ–ª—å –Ω–µ –≤–∏—Ä—ñ—à—É—î —Å–∫—ñ–ª—å–∫–∏ "–¥—É–º–∞—Ç–∏"

**–†—ñ—à–µ–Ω–Ω—è:**
- Adaptive recursion gate: `gate = torch.sigmoid(self.depth_head(h))`

---

### 7. **Trainer –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –í—Å—è –ª–æ–≥—ñ–∫–∞ –≤ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—ñ
- –ö–µ—Ä—É—î curriculum, checkpointing, validation
- –ù–µ–º–∞—î callback-based –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏

**–†—ñ—à–µ–Ω–Ω—è:**
```
train/
 ‚îú‚îÄ‚îÄ loop.py
 ‚îú‚îÄ‚îÄ callbacks/
 ‚îÇ   ‚îú‚îÄ‚îÄ curriculum.py
 ‚îÇ   ‚îú‚îÄ‚îÄ validation.py
 ‚îÇ   ‚îú‚îÄ‚îÄ checkpoint.py
 ‚îÇ   ‚îî‚îÄ‚îÄ logging.py
```

---

### 8. **Curriculum scheduler —Å–ª–∞–±–∫–∏–π**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ö–µ—Ä—É—î —Ç—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (seq_len, dim, batch)
- –ù–ï –∫–µ—Ä—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é –∑–∞–¥–∞—á—ñ
- –ù–µ–º–∞—î progression –ø–æ —Ç–∏–ø–∞—Ö –¥–∞–Ω–∏—Ö

**–†—ñ—à–µ–Ω–Ω—è:**
- Curriculum –∫–µ—Ä—É—î: max recursion, max sequence, —Ç–∏–ø–∞–º–∏ —Å–µ–º–ø–ª—ñ–≤ (easy ‚Üí hard)

---

### 9. **–ù–µ–º–∞—î state object**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –í—Å–µ –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏
- –°–∫–ª–∞–¥–Ω–æ –≤—ñ–¥–Ω–æ–≤–ª—é–≤–∞—Ç–∏ —Å—Ç–∞–Ω
- –°–∫–ª–∞–¥–Ω–æ –¥–µ–±–∞–∂–∏—Ç–∏

**–†—ñ—à–µ–Ω–Ω—è:**
```python
class TrainState:
    epoch, step, loss, depth, memory_usage
```

---

### 10. **Document boundaries –Ω–µ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –õ—ñ–Ω—ñ–π–Ω–∞ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –∫–æ—Ä–ø—É—Å—É
- –ù–µ–º–∞—î doc_id, segment_id
- –ú–æ–∂–ª–∏–≤–∞ –∑–º—ñ—à—É–≤–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤

**–†—ñ—à–µ–Ω–Ω—è:**
```python
{
  "input_ids": ...,
  "labels": ...,
  "doc_id": ...,
  "segment_id": ...
}
```

---

### 11. **Entropy –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- Entropy –ø—Ä–æ—Å—Ç–æ –ª–æ–≥—É—î—Ç—å—Å—è
- –ù–µ–º–∞—î –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è entropy vs depth
- –ù–µ–º–∞—î entropy-driven early stop

**–†—ñ—à–µ–Ω–Ω—è:**
- –õ–æ–≥—É–≤–∞—Ç–∏ entropy vs depth, entropy vs epoch, entropy delta
- Entropy-driven early stop

---

### 12. **–ù–µ–º–∞—î deterministic mode (–¥–ª—è inference)**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ù–µ–º–∞—î `torch.manual_seed()` –≤ inference
- –°–∫–ª–∞–¥–Ω–æ debug

**–†—ñ—à–µ–Ω–Ω—è:**
```python
torch.manual_seed(42)
torch.use_deterministic_algorithms(True)
```

---

### 13. **–ù–µ–º–∞—î —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è Inference/Training**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –°–ø—ñ–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
- –ù–µ–º–∞—î `InferConfig` vs `TrainConfig`
- Training-only –ª–æ–≥—ñ–∫–∞ –º–æ–∂–µ –ø–æ—Ç—Ä–∞–ø–∏—Ç–∏ –≤ inference

**–†—ñ—à–µ–Ω–Ω—è:**
- –ñ–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è TrainConfig vs InferConfig
- –û–∫—Ä–µ–º—ñ entrypoints –±–µ–∑ shared state

---

### 14. **–ù–µ–º–∞—î core —à–∞—Ä—É**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –ù–µ–º–∞—î `core/interfaces.py`, `core/types.py`, `core/constants.py`
- –ë—ñ–∑–Ω–µ—Å-–ª–æ–≥—ñ–∫–∞ —Ä–æ–∑–∫–∏–¥–∞–Ω–∞
- –°–∫–ª–∞–¥–Ω–æ —Ç–µ—Å—Ç—É–≤–∞—Ç–∏ –º–æ–¥—É–ª—ñ —ñ–∑–æ–ª—å–æ–≤–∞–Ω–æ

**–†—ñ—à–µ–Ω–Ω—è:**
```
core/
 ‚îú‚îÄ‚îÄ interfaces.py
 ‚îú‚îÄ‚îÄ types.py
 ‚îî‚îÄ‚îÄ constants.py
```

---

### 15. **Qualitative eval - –Ω–µ–º–∞—î –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è**

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –Ñ evaluators, –∞–ª–µ –Ω–µ–º–∞—î –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤
- –ù–µ–º–∞—î `eval_samples/epoch_10.txt`

**–†—ñ—à–µ–Ω–Ω—è:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π sample log –ø–æ –µ–ø–æ—Ö–∞—Ö

---

## üü¢ –ú–ï–ù–® –ö–†–ò–¢–ò–ß–ù–Ü (–ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç 3)

### 16. **Timeout –¥–ª—è inference recursion**

- –Ñ timeout –¥–ª—è training, –∞–ª–µ –Ω–µ –¥–ª—è inference recursion

---

## üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê

- **–ö—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º:** 5
- **–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º:** 10
- **–ú–µ–Ω—à –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö:** 1
- **–í—Å—å–æ–≥–æ:** 16 –ø—Ä–æ–±–ª–µ–º

---

## üéØ –¢–û–ü-10 –ü–û–ö–†–ê–©–ï–ù–¨ (–ü–†–Ü–û–†–ò–¢–ï–¢)

1. ‚ö†Ô∏è **Thinking cost –≤ loss** - `L_total = L_task + Œª * recursion_steps` ‚ö†Ô∏è **–ù–ê–ô–ö–†–ò–¢–ò–ß–ù–Ü–®–ï**
2. **Best model checkpointing** - `best_loss.ckpt`, `best_eval_score.ckpt`
3. **Dataset contamination control** - manifest, hash-based –∫–æ–Ω—Ç—Ä–æ–ª—å
4. **ResourceMonitor –∑ auto throttle** - –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ ‚Üí –∫–æ–Ω—Ç—Ä–æ–ª—å
5. **Adaptive recursion gate** - –º–æ–¥–µ–ª—å –≤–∏—Ä—ñ—à—É—î depth
6. **Callback-based trainer** - —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è orchestration —Ç–∞ logic
7. **Document-aware dataset** - doc_id, segment_id, boundaries
8. **TrainState + resume** - —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π state object
9. **Curriculum –∫–µ—Ä—É—î —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—é** - –Ω–µ —Ç—ñ–ª—å–∫–∏ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏
10. **InferConfig vs TrainConfig** - –∂–æ—Ä—Å—Ç–∫–µ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è

---

## üèÅ –í–ò–°–ù–û–í–û–ö

**–ö—Ä–∏—Ç–∏–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–∞ –Ω–∞ ~95%** - –∞–≤—Ç–æ—Ä –≤–∏—è–≤–∏–≤ –≤—Å—ñ –∫—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏ —Ç–∞ –≥–ª–∏–±—à—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –Ω–µ–¥–æ–ª—ñ–∫–∏.

**–¶–µ –Ω–µ –ø–æ–≥–∞–Ω–∏–π –∫–æ–¥.**  
**–¶–µ –∫–æ–¥, —è–∫–∏–π –ø–µ—Ä–µ—Ä—ñ—Å —Å–≤–æ—é –ø–æ—á–∞—Ç–∫–æ–≤—É –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É.**

**–†–µ–∞–ª—å–Ω–∏–π —Å—Ç–∞–Ω:**
- üü° Research-grade, –Ω–µ production
- üü° –î–æ–±—Ä–µ –¥–ª—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
- üî¥ –ù–µ–±–µ–∑–ø–µ—á–Ω–æ –¥–ª—è –¥–æ–≤–≥–æ–≥–æ train –±–µ–∑ –¥–æ–æ–ø—Ä–∞—Ü—é–≤–∞–Ω—å

